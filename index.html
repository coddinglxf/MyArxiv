<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/dvlab-research/MiniGemini</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Modularity Transferable? A Case Study through the Lens of Knowledge
  Distillation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Klimaszewski, Piotr Andruszkiewicz, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Modular Deep Learning showcases its potential in various Natural
Language Processing applications. Parameter-efficient fine-tuning (PEFT)
modularity has been shown to work for various use cases, from domain adaptation
to multilingual setups. However, all this work covers the case where the
modular components are trained and deployed within one single Pre-trained
Language Model (PLM). This model-specific setup is a substantial limitation on
the very modularity that modular architectures are trying to achieve. We ask
whether current modular approaches are transferable between models and whether
we can transfer the modules from more robust and larger PLMs to smaller ones.
In this work, we aim to fill this gap via a lens of Knowledge Distillation,
commonly used for model compression, and present an extremely straightforward
approach to transferring pre-trained, task-specific PEFT modules between
same-family PLMs. Moreover, we propose a method that allows the transfer of
modules between incompatible PLMs without any change in the inference
complexity. The experiments on Named Entity Recognition, Natural Language
Inference, and Paraphrase Identification tasks over multiple languages and PEFT
methods showcase the initial potential of transferable modularity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projective Methods for Mitigating Gender Bias in <span class="highlight-title">Pre-train</span>ed Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hillary Dawkins, Isar Nejadgholi, Daniel Gillis, Judi McCuaig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigation of gender bias in NLP has a long history tied to debiasing static
word embeddings. More recently, attention has shifted to debiasing pre-trained
language models. We study to what extent the simplest projective debiasing
methods, developed for word embeddings, can help when applied to BERT's
internal representations. Projective methods are fast to implement, use a small
number of saved parameters, and make no updates to the existing model
parameters. We evaluate the efficacy of the methods in reducing both intrinsic
bias, as measured by BERT's next sentence prediction task, and in mitigating
observed bias in a downstream setting when fine-tuned. To this end, we also
provide a critical analysis of a popular gender-bias assessment test for
quantifying intrinsic bias, resulting in an enhanced test set and new bias
measures. We find that projective methods can be effective at both intrinsic
bias and downstream bias mitigation, but that the two outcomes are not
necessarily correlated. This finding serves as a warning that intrinsic bias
test sets, based either on language modeling tasks or next sentence prediction,
should not be the only benchmark in developing a debiased language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a World-English Language Model for On-Device Virtual Assistants <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rricha Jalota, Lyan Verwimp, Markus Nussbaum-Thom, Amr Mousa, Arturo Argueta, Youssef Oualil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are
generally language-, region-, and in some cases, device-dependent, which
increases the effort to scale and maintain them. Combining NNLMs for one or
more of the categories is one way to improve scalability. In this work, we
combine regional variants of English to build a ``World English'' NNLM for
on-device VAs. In particular, we investigate the application of adapter
bottlenecks to model dialect-specific characteristics in our existing
production NNLMs {and enhance the multi-dialect baselines}. We find that
adapter modules are more effective in modeling dialects than specializing
entire sub-networks. Based on this insight and leveraging the design of our
production models, we introduce a new architecture for World English NNLM that
meets the accuracy, latency, and memory constraints of our single-dialect
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CheckEval: Robust Evaluation Framework using Large Language Model via
  Checklist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CheckEval, a novel evaluation framework using Large Language
Models, addressing the challenges of ambiguity and inconsistency in current
evaluation methods. CheckEval addresses these challenges by dividing evaluation
criteria into detailed sub-aspects and constructing a checklist of Boolean
questions for each, simplifying the evaluation. This approach not only renders
the process more interpretable but also significantly enhances the robustness
and reliability of results by focusing on specific evaluation dimensions.
Validated through a focused case study using the SummEval benchmark, CheckEval
indicates a strong correlation with human judgments. Furthermore, it
demonstrates a highly consistent Inter-Annotator Agreement. These findings
highlight the effectiveness of CheckEval for objective, flexible, and precise
evaluations. By offering a customizable and interactive framework, CheckEval
sets a new standard for the use of LLMs in evaluation, responding to the
evolving needs of the field and establishing a clear method for future
LLM-based evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HEAL at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Neural Protoform Reconstruction via Reflex Prediction <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Lu, Jingzhi Wang, David R. Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protolanguage reconstruction is central to historical linguistics. The
comparative method, one of the most influential theoretical and methodological
frameworks in the history of the language sciences, allows linguists to infer
protoforms (reconstructed ancestral words) from their reflexes (related modern
words) based on the assumption of regular sound change. Not surprisingly,
numerous computational linguists have attempted to operationalize comparative
reconstruction through various computational models, the most successful of
which have been supervised encoder-decoder models, which treat the problem of
predicting protoforms given sets of reflexes as a sequence-to-sequence problem.
We argue that this framework ignores one of the most important aspects of the
comparative method: not only should protoforms be inferable from cognate sets
(sets of related reflexes) but the reflexes should also be inferable from the
protoforms. Leveraging another line of research -- reflex prediction -- we
propose a system in which candidate protoforms from a reconstruction model are
reranked by a reflex prediction model. We show that this more complete
implementation of the comparative method allows us to surpass state-of-the-art
protoform reconstruction methods on three of four Chinese and Romance datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CYCLE: Learning to Self-Refine the Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangruibo Ding, Marcus J. Min, Gail Kaiser, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained code language models have achieved promising performance in code
generation and improved the programming efficiency of human developers.
However, their self-refinement capability is typically overlooked by the
existing evaluations of code LMs, which focus only on the accuracy of the
one-time prediction. For the cases when code LMs fail to implement the correct
program, developers actually find it hard to debug and fix the faulty
prediction since it is not written by the developers themselves. Unfortunately,
our study reveals that code LMs cannot efficiently self-refine their faulty
generations as well.
  In this paper, we propose CYCLE framework, learning to self-refine the faulty
generation according to the available feedback, such as the execution results
reported by the test suites. We evaluate CYCLE on three popular code generation
benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE
successfully maintains, sometimes improves, the quality of one-time code
generation, while significantly improving the self-refinement capability of
code LMs. We implement four variants of CYCLE with varied numbers of parameters
across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently
boosts the code generation performance, by up to 63.5%, across benchmarks and
varied model sizes. We also notice that CYCLE outperforms code LMs that have
3$\times$ more parameters in self-refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready for OOPSLA'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Invalsi Benchmark: measuring Language Models Mathematical and
  Language understanding in Italian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Esuli, Giovanni Puccetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Italian is by all metrics a high resource language, currently, there
are isn't a Language Model pre-trained exclusively in this language. This
results in a lower number of available benchmarks to evaluate the performance
of language models in Italian.
  This work presents two new benchmarks to evaluate the models performance on
mathematical understanding and language understanding in Italian. These
benchmarks are based on real tests that are undertaken by students of age
between 11 and 18 within the Italian school system and have therefore been
validated by several experts in didactics and pedagogy.
  To validate this dataset we evaluate the performance of 9 language models
that are the best performing when writing in Italian, including our own
fine-tuned models. We show that this is a challenging benchmark where current
language models are bound by 60\% accuracy.
  We believe that the release of this dataset paves the way for improving
future models mathematical and language understanding in Italian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws For Dense Retrieval <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up neural models has yielded significant advancements in a wide array
of tasks, particularly in language generation. Previous studies have found that
the performance of neural models frequently adheres to predictable scaling
laws, correlated with factors such as training set size and model size. This
insight is invaluable, especially as large-scale experiments grow increasingly
resource-intensive. Yet, such scaling law has not been fully explored in dense
retrieval due to the discrete nature of retrieval metrics and complex
relationships between training data and model sizes in retrieval tasks. In this
study, we investigate whether the performance of dense retrieval models follows
the scaling law as other neural models. We propose to use contrastive
log-likelihood as the evaluation metric and conduct extensive experiments with
dense retrieval models implemented with different numbers of parameters and
trained with different amounts of annotated data. Results indicate that, under
our settings, the performance of dense retrieval models follows a precise
power-law scaling related to the model size and the number of annotations.
Additionally, we examine scaling with prevalent data augmentation methods to
assess the impact of annotation quality, and apply the scaling law to find the
best resource allocation strategy under a budget constraint. We believe that
these insights will significantly contribute to understanding the scaling
effect of dense retrieval models and offer meaningful guidance for future
research endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Samsung/NL-ITI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Checking Beyond Training Set <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Content Recommendation: Knowledge Graph-Based Semantic
  Contrastive Learning for Diversity and Cold-Start Users <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Kim, Scott Rome, Kevin Foley, Mayur Nankani, Rimon Melamed, Javier Morales, Abhay Yadav, Maria Peifer, Sardar Hamidian, H. Howie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenges related to data sparsity, cold-start problems, and
diversity in recommendation systems is both crucial and demanding. Many current
solutions leverage knowledge graphs to tackle these issues by combining both
item-based and user-item collaborative signals. A common trend in these
approaches focuses on improving ranking performance at the cost of escalating
model complexity, reducing diversity, and complicating the task. It is
essential to provide recommendations that are both personalized and diverse,
rather than solely relying on achieving high rank-based performance, such as
Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task
learning approach, training on user-item and item-item interactions. We apply
item-based contrastive learning on descriptive text, sampling positive and
negative pairs based on item metadata. Our approach allows the model to better
understand the relationships between entities within the knowledge graph by
utilizing semantic information from text. It leads to more accurate, relevant,
and diverse user recommendations and a benefit that extends even to cold-start
users who have few interactions with items. We perform extensive experiments on
two widely used datasets to validate the effectiveness of our approach. Our
findings demonstrate that jointly training user-item interactions and
item-based signals using synopsis text is highly effective. Furthermore, our
results provide evidence that item-based contrastive learning enhances the
quality of entity embeddings, as indicated by metrics such as uniformity and
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-Pro: Learning to Evolve via Policy-Level Reflection and
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'
  API Invocation Capabilities <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglin Mu, Yang Xu, Yunlong Feng, Xiaofeng Han, Yitong Li, Yutai Hou, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of Large Language Models (LLMs), AI assistants' ability to
utilize tools, especially through API calls, has advanced notably. This
progress has necessitated more accurate evaluation methods. Many existing
studies adopt static evaluation, where they assess AI assistants' API call
based on pre-defined dialogue histories. However, such evaluation method can be
misleading, as an AI assistant might fail in generating API calls from
preceding human interaction in real cases. Instead of the resource-intensive
method of direct human-machine interactions, we propose Automated Dynamic
Evaluation (AutoDE) to assess an assistant's API call capability without human
involvement. In our framework, we endeavor to closely mirror genuine human
conversation patterns in human-machine interactions, using a LLM-based user
agent, equipped with a user script to ensure human alignment. Experimental
results highlight that AutoDE uncovers errors overlooked by static evaluations,
aligning more closely with human assessment. Testing four AI assistants using
our crafted benchmark, our method further mirrored human evaluation compared to
conventional static evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real Acoustic Fields: An Audio-Visual Room Acoustics <span class="highlight-title">Dataset</span> and
  Benchmark <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, Alexander Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new dataset called Real Acoustic Fields (RAF) that captures real
acoustic room data from multiple modalities. The dataset includes high-quality
and densely captured room impulse response data paired with multi-view images,
and precise 6DoF pose tracking data for sound emitters and listeners in the
rooms. We used this dataset to evaluate existing methods for novel-view
acoustic synthesis and impulse response generation which previously relied on
synthetic data. In our evaluation, we thoroughly assessed existing audio and
audio-visual models against multiple criteria and proposed settings to enhance
their performance on real-world data. We also conducted experiments to
investigate the impact of incorporating visual data (i.e., images and depth)
into neural acoustic field models. Additionally, we demonstrated the
effectiveness of a simple sim2real approach, where a model is pre-trained with
simulated data and fine-tuned with sparse real-world data, resulting in
significant improvements in the few-shot learning approach. RAF is the first
dataset to provide densely captured room acoustic data, making it an ideal
resource for researchers working on audio and audio-visual neural acoustic
field modeling techniques. Demos and datasets are available on our project
page: https://facebookresearch.github.io/real-acoustic-fields/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project site:
  https://facebookresearch.github.io/real-acoustic-fields/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view
  Human Performance Capture and Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxing Sun, Rishabh Dabral, Pascal Fua, Christian Theobalt, Marc Habermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithful human performance capture and free-view rendering from sparse RGB
observations is a long-standing problem in Vision and Graphics. The main
challenges are the lack of observations and the inherent ambiguities of the
setting, e.g. occlusions and depth ambiguity. As a result, radiance fields,
which have shown great promise in capturing high-frequency appearance and
geometry details in dense setups, perform poorly when na\"ively supervising
them on sparse camera views, as the field simply overfits to the sparse-view
inputs. To address this, we propose MetaCap, a method for efficient and
high-quality geometry recovery and novel view synthesis given very sparse or
even a single view of the human. Our key idea is to meta-learn the radiance
field weights solely from potentially sparse multi-view videos, which can serve
as a prior when fine-tuning them on sparse imagery depicting the human. This
prior provides a good network weight initialization, thereby effectively
addressing ambiguities in sparse-view capture. Due to the articulated structure
of the human body and motion-induced surface deformations, learning such a
prior is non-trivial. Therefore, we propose to meta-learn the field weights in
a pose-canonicalized space, which reduces the spatial feature range and makes
feature learning more effective. Consequently, one can fine-tune our field
parameters to quickly generalize to unseen poses, novel illumination conditions
as well as novel and sparse (even monocular) camera views. For evaluating our
method under different scenarios, we collect a new dataset, WildDynaCap, which
contains subjects captured in, both, a dense camera dome and in-the-wild sparse
camera rigs, and demonstrate superior results compared to recent
state-of-the-art methods on both public and WildDynaCap dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vcai.mpi-inf.mpg.de/projects/MetaCap/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Object Detectors with COCO: A New Path Forward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shweta Singh, Aayan Yadav, Jitesh Jain, Humphrey Shi, Justin Johnson, Karan Desai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Common Objects in Context (COCO) dataset has been instrumental in
benchmarking object detectors over the past decade. Like every dataset, COCO
contains subtle errors and imperfections stemming from its annotation
procedure. With the advent of high-performing models, we ask whether these
errors of COCO are hindering its utility in reliably benchmarking further
progress. In search for an answer, we inspect thousands of masks from COCO
(2017 version) and uncover different types of errors such as imprecise mask
boundaries, non-exhaustively annotated instances, and mislabeled masks. Due to
the prevalence of COCO, we choose to correct these errors to maintain
continuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner
set of annotations with visibly better mask quality than COCO-2017. We evaluate
fifty object detectors and find that models that predict visually sharper masks
score higher on COCO-ReM, affirming that they were being incorrectly penalized
due to errors in COCO-2017. Moreover, our models trained using COCO-ReM
converge faster and score higher than their larger variants trained using
COCO-2017, highlighting the importance of data quality in improving object
detectors. With these findings, we advocate using COCO-ReM for future object
detection research. Our dataset is available at https://cocorem.xyz
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Dataset website: https://cocorem.xyz and code:
  https://github.com/kdexd/coco-rem</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object
  Removal and Insertion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have revolutionized image editing but often generate images
that violate physical laws, particularly the effects of objects on the scene,
e.g., occlusions, shadows, and reflections. By analyzing the limitations of
self-supervised approaches, we propose a practical solution centered on a
\q{counterfactual} dataset. Our method involves capturing a scene before and
after removing a single object, while minimizing other changes. By fine-tuning
a diffusion model on this dataset, we are able to not only remove objects but
also their effects on the scene. However, we find that applying this approach
for photorealistic object insertion requires an impractically large dataset. To
tackle this challenge, we propose bootstrap supervision; leveraging our object
removal model trained on a small counterfactual dataset, we synthetically
expand this dataset considerably. Our approach significantly outperforms prior
methods in photorealistic object removal and insertion, particularly at
modeling the effects of objects on the scene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Garment3DGen: 3D Garment Stylization and Texture Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Sarafianos, Tuur Stuyck, Xiaoyu Xiang, Yilei Li, Jovan Popovic, Rakesh Ranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Garment3DGen a new method to synthesize 3D garment assets from a
base mesh given a single input image as guidance. Our proposed approach allows
users to generate 3D textured clothes based on both real and synthetic images,
such as those generated by text prompts. The generated assets can be directly
draped and simulated on human bodies. First, we leverage the recent progress of
image to 3D diffusion methods to generate 3D garment geometries. However, since
these geometries cannot be utilized directly for downstream tasks, we propose
to use them as pseudo ground-truth and set up a mesh deformation optimization
procedure that deforms a base template mesh to match the generated 3D target.
Second, we introduce carefully designed losses that allow the input base mesh
to freely deform towards the desired target, yet preserve mesh quality and
topology such that they can be simulated. Finally, a texture estimation module
generates high-fidelity texture maps that are globally and locally consistent
and faithfully capture the input guidance, allowing us to render the generated
3D assets. With Garment3DGen users can generate the textured 3D garment of
their choice without the need of artist intervention. One can provide a textual
prompt describing the garment they desire to generate a simulation-ready 3D
asset. We present a plethora of quantitative and qualitative comparisons on
various assets both real and generated and provide use-cases of how one can
generate simulation-ready 3D garments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://nsarafianos.github.io/garment3dgen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/dvlab-research/MiniGemini</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duolando: Follower <span class="highlight-title">GPT</span> with Off-Policy Reinforcement Learning for Dance
  Accompaniment <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Siyao, Tianpei Gu, Zhitao Yang, Zhengyu Lin, Ziwei Liu, Henghui Ding, Lei Yang, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel task within the field of 3D dance generation, termed
dance accompaniment, which necessitates the generation of responsive movements
from a dance partner, the "follower", synchronized with the lead dancer's
movements and the underlying musical rhythm. Unlike existing solo or group
dance generation tasks, a duet dance scenario entails a heightened degree of
interaction between the two participants, requiring delicate coordination in
both pose and position. To support this task, we first build a large-scale and
diverse duet interactive dance dataset, DD100, by recording about 117 minutes
of professional dancers' performances. To address the challenges inherent in
this task, we propose a GPT-based model, Duolando, which autoregressively
predicts the subsequent tokenized motion conditioned on the coordinated
information of the music, the leader's and the follower's movements. To further
enhance the GPT's capabilities of generating stable results on unseen
conditions (music and leader motions), we devise an off-policy reinforcement
learning strategy that allows the model to explore viable trajectories from
out-of-distribution samplings, guided by human-defined rewards. Based on the
collected dataset and proposed method, we establish a benchmark with several
carefully designed metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gamba: Marry Gaussian Splatting with Mamba for single view 3D
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Pose Estimation via the Aggregation of Diffusion Features <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianfu Wang, Guosheng Hu, Hongguang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the pose of objects from images is a crucial task of 3D scene
understanding, and recent approaches have shown promising results on very large
benchmarks. However, these methods experience a significant performance drop
when dealing with unseen objects. We believe that it results from the limited
generalizability of image features. To address this problem, we have an
in-depth analysis on the features of diffusion models, e.g. Stable Diffusion,
which hold substantial potential for modeling unseen objects. Based on this
analysis, we then innovatively introduce these diffusion features for object
pose estimation. To achieve this, we propose three distinct architectures that
can effectively capture and aggregate diffusion features of different
granularity, greatly improving the generalizability of object pose estimation.
Our approach outperforms the state-of-the-art methods by a considerable margin
on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our
method achieves higher accuracy than the previous best arts on unseen objects:
98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the
strong generalizability of our method. Our code is released at
https://github.com/Tianfu18/diff-feats-pose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable
  Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Luo, Jing Liu, James Davis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SplatFace, a novel Gaussian splatting framework designed for 3D
human face reconstruction without reliance on accurate pre-determined geometry.
Our method is designed to simultaneously deliver both high-quality novel view
rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D
Morphable Model (3DMM) to provide a surface geometric structure, making it
possible to reconstruct faces with a limited set of input images. We introduce
a joint optimization strategy that refines both the Gaussians and the morphable
surface through a synergistic non-rigid alignment process. A novel distance
metric, splat-to-surface, is proposed to improve alignment by considering both
the Gaussian position and covariance. The surface information is also utilized
to incorporate a world-space densification process, resulting in superior
reconstruction quality. Our experimental analysis demonstrates that the
proposed method is competitive with both other Gaussian splatting techniques in
novel view synthesis and other 3D reconstruction methods in producing 3D face
meshes with high geometric precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weidong Xie, Lun Luo, Nanfei Ye, Yi Ren, Shaoyi Du, Minhang Wang, Jintao Xu, Rui Ai, Weihao Gu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is an important task for robots and autonomous cars to
localize themselves and close loops in pre-built maps. While single-modal
sensor-based methods have shown satisfactory performance, cross-modal place
recognition that retrieving images from a point-cloud database remains a
challenging problem. Current cross-modal methods transform images into 3D
points using depth estimation for modality conversion, which are usually
computationally intensive and need expensive labeled data for depth
supervision. In this work, we introduce a fast and lightweight framework to
encode images and point clouds into place-distinctive descriptors. We propose
an effective Field of View (FoV) transformation module to convert point clouds
into an analogous modality as images. This module eliminates the necessity for
depth estimation and helps subsequent modules achieve real-time performance. We
further design a non-negative factorization-based encoder to extract mutually
consistent semantic features between point clouds and images. This encoder
yields more distinctive global descriptors for retrieval. Experimental results
on the KITTI dataset show that our proposed methods achieve state-of-the-art
performance while running in real time. Additional evaluation on the HAOMO
dataset covering a 17 km trajectory further shows the practical generalization
capabilities. We have released the implementation of our methods as open source
at: https://github.com/haomo-ai/ModaLink.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A vascular synthetic model for improved aneurysm segmentation and
  detection via Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafic Nader, Florent Autrusseau, Vincent L'Allinec, Romain Bourcier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We hereby present a full synthetic model, able to mimic the various
constituents of the cerebral vascular tree: the cerebral arteries, the
bifurcations and the intracranial aneurysms. By building this model, our goal
was to provide a substantial dataset of brain arteries which could be used by a
3D Convolutional Neural Network (CNN) to either segment or detect/recognize
various vascular diseases (such as artery dissection/thrombosis) or even some
portions of the cerebral vasculature, such as the bifurcations or aneurysms. In
this study, we will particularly focus on Intra-Cranial Aneurysm (ICA)
detection and segmentation. The cerebral aneurysms most often occur on a
particular structure of the vascular tree named the Circle of Willis. Various
studies have been conducted to detect and monitor the ICAs and those based on
Deep Learning (DL) achieve the best performances. Specifically, in this work,
we propose a full synthetic 3D model able to mimic the brain vasculature as
acquired by Magnetic Resonance Angiography (MRA), and more particularly the
Time Of Flight (TOF) principle. Among the various MRI modalities, the MRA-TOF
allows to have a relatively good rendering of the blood vessels and is
non-invasive (no contrast liquid injection). Our model has been designed to
simultaneously mimic the arteries geometry, the ICA shape and the background
noise. The geometry of the vascular tree is modeled thanks to an interpolation
with 3D Spline functions, and the statistical properties of the background MRI
noise is collected from MRA acquisitions and reproduced within the model. In
this work, we thoroughly describe the synthetic vasculature model, we build up
a neural network designed for ICA segmentation and detection, and finally, we
carry out an in-depth evaluation of the performance gap gained thanks to the
synthetic model data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality Prediction Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Image Ambient Lighting Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florin-Alexandru Vasluianu, Tim Seizinger, Zongwei Wu, Rakesh Ranjan, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lighting normalization is a crucial but underexplored restoration task with
broad applications. However, existing works often simplify this task within the
context of shadow removal, limiting the light sources to one and
oversimplifying the scene, thus excluding complex self-shadows and restricting
surface classes to smooth ones. Although promising, such simplifications hinder
generalizability to more realistic settings encountered in daily use. In this
paper, we propose a new challenging task termed Ambient Lighting Normalization
(ALN), which enables the study of interactions between shadows, unifying image
restoration and shadow removal in a broader context. To address the lack of
appropriate datasets for ALN, we introduce the large-scale high-resolution
dataset Ambient6K, comprising samples obtained from multiple light sources and
including self-shadows resulting from complex geometries, which is the first of
its kind. For benchmarking, we select various mainstream methods and rigorously
evaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong
baseline that maximizes Image-Frequency joint entropy to selectively restore
local areas under different lighting conditions, without relying on shadow
localization priors. Experiments show that IFBlend achieves SOTA scores on
Ambient6K and exhibits competitive performance on conventional shadow removal
benchmarks compared to shadow-specific models with mask priors. The dataset,
benchmark, and code are available at https://github.com/fvasluianu97/IFBlend.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Textual <span class="highlight-title">Prompt</span> to AI-Generated Image Quality Assessment <span class="chip">ICME2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Qu, Haohui Li, Wei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike
traditional image quality assessment (IQA) on natural scenarios, AGIs quality
assessment (AGIQA) takes the correspondence of image and its textual prompt
into consideration. This is coupled in the ground truth score, which confuses
the unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs
Quality Assessment via Image and Prompt), a multimodal framework for AGIQA via
corresponding image and prompt incorporation. Specifically, we propose a novel
incremental pretraining task named Image2Prompt for better understanding of
AGIs and their corresponding textual prompts. An effective and efficient
image-prompt fusion module, along with a novel special [QA] token, are also
applied. Both are plug-and-play and beneficial for the cooperation of image and
its corresponding prompt. Experiments demonstrate that our IP-IQA achieves the
state-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, accepted by ICME2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable
  Transient-Free 3D reconstruction from Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Billouard, Dawa Derksen, Emmanuelle Sarrazin, Bruno Vallet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Vision <span class="highlight-title">Transformer</span> Compression with Few Samples <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxiao Zhang, Yifan Zhou, Guo-Hua Wang, Jianxin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot model compression aims to compress a large model into a more compact
one with only a tiny training set (even without labels). Block-level pruning
has recently emerged as a leading technique in achieving high accuracy and low
latency in few-shot CNN compression. But, few-shot compression for Vision
Transformers (ViT) remains largely unexplored, which presents a new challenge.
In particular, the issue of sparse compression exists in traditional CNN
few-shot methods, which can only produce very few compressed models of
different model sizes. This paper proposes a novel framework for few-shot ViT
compression named DC-ViT. Instead of dropping the entire block, DC-ViT
selectively eliminates the attention module while retaining and reusing
portions of the MLP module. DC-ViT enables dense compression, which outputs
numerous compressed models that densely populate the range of model complexity.
DC-ViT outperforms state-of-the-art few-shot compression methods by a
significant margin of 10 percentage points, along with lower latency in the
compression of ViT and its variants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Note: Jianxin Wu is a contributing author for
  the arXiv version of this paper but is not listed as an author in the CVPR
  version due to his role as Program Chair</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annolid: Annotate, Segment, and Track Anything You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Thomas A. Cleland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annolid is a deep learning-based software package designed for the
segmentation, labeling, and tracking of research targets within video files,
focusing primarily on animal behavior analysis. Based on state-of-the-art
instance segmentation methods, Annolid now harnesses the Cutie video object
segmentation model to achieve resilient, markerless tracking of multiple
animals from single annotated frames, even in environments in which they may be
partially or entirely concealed by environmental features or by one another.
Our integration of Segment Anything and Grounding-DINO strategies additionally
enables the automatic masking and segmentation of recognizable animals and
objects by text command, removing the need for manual annotation. Annolid's
comprehensive approach to object segmentation flexibly accommodates a broad
spectrum of behavior analysis applications, enabling the classification of
diverse behavioral states such as freezing, digging, pup huddling, and social
interactions in addition to the tracking of animals and their body parts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Robust and Explainable Models in Computer Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Amirian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in machine and deep learning (ML and DL) research have
provided excellent tools for leveraging enormous amounts of data and optimizing
huge models with millions of parameters to obtain accurate networks for image
processing. These developments open up tremendous opportunities for using
artificial intelligence (AI) in the automation and human assisted AI industry.
However, as more and more models are deployed and used in practice, many
challenges have emerged. This thesis presents various approaches that address
robustness and explainability challenges for using ML and DL in practice.
  Robustness and reliability are the critical components of any model before
certification and deployment in practice. Deep convolutional neural networks
(CNNs) exhibit vulnerability to transformations of their inputs, such as
rotation and scaling, or intentional manipulations as described in the
adversarial attack literature. In addition, building trust in AI-based models
requires a better understanding of current models and developing methods that
are more explainable and interpretable a priori.
  This thesis presents developments in computer vision models' robustness and
explainability. Furthermore, this thesis offers an example of using vision
models' feature response visualization (models' interpretations) to improve
robustness despite interpretability and robustness being seemingly unrelated in
the related research. Besides methodological developments for robust and
explainable vision models, a key message of this thesis is introducing model
interpretation techniques as a tool for understanding vision models and
improving their design and robustness. In addition to the theoretical
developments, this thesis demonstrates several applications of ML and DL in
different contexts, such as medical imaging and affective computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>150 pages, 37 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructBrush: Learning Attention-based Instruction Optimization for
  Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Zhao, Qingnan Fan, Fei Kou, Shuai Qin, Hong Gu, Wei Wu, Pengcheng Xu, Mingrui Zhu, Nannan Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, instruction-based image editing methods have garnered
significant attention in image editing. However, despite encompassing a wide
range of editing priors, these methods are helpless when handling editing tasks
that are challenging to accurately describe through language. We propose
InstructBrush, an inversion method for instruction-based image editing methods
to bridge this gap. It extracts editing effects from exemplar image pairs as
editing instructions, which are further applied for image editing. Two key
techniques are introduced into InstructBrush, Attention-based Instruction
Optimization and Transformation-oriented Instruction Initialization, to address
the limitations of the previous method in terms of inversion effects and
instruction generalization. To explore the ability of instruction inversion
methods to guide image editing in open scenarios, we establish a
TransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set
of scenes and editing types. The creation of this benchmark paves the way for
further exploration of instruction inversion. Quantitatively and qualitatively,
our approach achieves superior performance in editing and is more semantically
consistent with the target editing effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://royzhao926.github.io/InstructBrush/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Data Annotation Challenges in Multiple Sensors: A Solution
  for Scania Collected <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajinkya Khoche, Aron Asefaw, Alejandro Gonzalez, Bogdan Timus, Sina Sharif Mansouri, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data annotation in autonomous vehicles is a critical step in the development
of Deep Neural Network (DNN) based models or the performance evaluation of the
perception system. This often takes the form of adding 3D bounding boxes on
time-sequential and registered series of point-sets captured from active
sensors like Light Detection and Ranging (LiDAR) and Radio Detection and
Ranging (RADAR). When annotating multiple active sensors, there is a need to
motion compensate and translate the points to a consistent coordinate frame and
timestamp respectively. However, highly dynamic objects pose a unique
challenge, as they can appear at different timestamps in each sensor's data.
Without knowing the speed of the objects, their position appears to be
different in different sensor outputs. Thus, even after motion compensation,
highly dynamic objects are not matched from multiple sensors in the same frame,
and human annotators struggle to add unique bounding boxes that capture all
objects. This article focuses on addressing this challenge, primarily within
the context of Scania collected datasets. The proposed solution takes a track
of an annotated object as input and uses the Moving Horizon Estimation (MHE) to
robustly estimate its speed. The estimated speed profile is utilized to correct
the position of the annotated box and add boxes to object clusters missed by
the original annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to European Control Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s-based architectures for stroke segmentation: A <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke remains a significant global health concern, necessitating precise and
efficient diagnostic tools for timely intervention and improved patient
outcomes. The emergence of deep learning methodologies has transformed the
landscape of medical image analysis. Recently, Transformers, initially designed
for natural language processing, have exhibited remarkable capabilities in
various computer vision applications, including medical image analysis. This
comprehensive review aims to provide an in-depth exploration of the
cutting-edge Transformer-based architectures applied in the context of stroke
segmentation. It commences with an exploration of stroke pathology, imaging
modalities, and the challenges associated with accurate diagnosis and
segmentation. Subsequently, the review delves into the fundamental ideas of
Transformers, offering detailed insights into their architectural intricacies
and the underlying mechanisms that empower them to effectively capture complex
spatial information within medical images. The existing literature is
systematically categorized and analyzed, discussing various approaches that
leverage Transformers for stroke segmentation. A critical assessment is
provided, highlighting the strengths and limitations of these methods,
including considerations of performance and computational efficiency.
Additionally, this review explores potential avenues for future research and
development
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong-Tung Nguyen, Duc-Anh Nguyen, Anh Tran, Cuong Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our work addresses limitations seen in previous approaches for object-centric
editing problems, such as unrealistic results due to shape discrepancies and
limited control in object replacement or insertion. To this end, we introduce
FlexEdit, a flexible and controllable editing framework for objects where we
iteratively adjust latents at each denoising step using our FlexEdit block.
Initially, we optimize latents at test time to align with specified object
constraints. Then, our framework employs an adaptive mask, automatically
extracted during denoising, to protect the background while seamlessly blending
new content into the target image. We demonstrate the versatility of FlexEdit
in various object editing tasks and curate an evaluation test suite with
samples from both real and synthetic images, along with novel evaluation
metrics designed for object-centric editing. We conduct extensive experiments
on different editing scenarios, demonstrating the superiority of our editing
framework over recent advanced text-guided image editing methods. Our project
page is published at https://flex-edit.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project page: https://flex-edit.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in
  Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets.In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote
  Sensing Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Shao, Zhaoyang Zhang, Chao Tao, Yunsheng Zhang, Chengli Peng, Haifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tokenizer, as one of the fundamental components of large models, has long
been overlooked or even misunderstood in visual tasks. One key factor of the
great comprehension power of the large language model is that natural language
tokenizers utilize meaningful words or subwords as the basic elements of
language. In contrast, mainstream visual tokenizers, represented by patch-based
methods such as Patch Embed, rely on meaningless rectangular patches as basic
elements of vision, which cannot serve as effectively as words or subwords in
language. Starting from the essence of the tokenizer, we defined semantically
independent regions (SIRs) for vision. We designed a simple HOmogeneous visual
tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception
Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,
the OPM splits the image into 4*4 pixel seeds and then utilizes the attention
mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds
within the same SIR. To achieve adaptability, the OVM defines a variable number
of learnable vectors as cross-attention queries, allowing for the adjustment of
token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19
classification dataset, and GID5 segmentation dataset for sparse and dense
tasks. The results demonstrate that the visual tokens obtained by HOOK
correspond to individual objects, which demonstrates homogeneity. HOOK
outperformed Patch Embed by 6\% and 10\% in the two tasks and achieved
state-of-the-art performance compared to the baselines used for comparison.
Compared to Patch Embed, which requires more than one hundred tokens for one
image, HOOK requires only 6 and 8 tokens for sparse and dense tasks,
respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The
code is available at https://github.com/GeoX-Lab/Hook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Users prefer Jpegli over same-sized libjpeg-turbo or MozJPEG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Bruse, Luca Versari, Zoltan Szabadka, Jyrki Alakuijala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We performed pairwise comparisons by human raters of JPEG images from
MozJPEG, libjpeg-turbo and our new Jpegli encoder. When compressing images at a
quality similar to libjpeg-turbo quality 95, the Jpegli images were 54% likely
to be preferred over both libjpeg-turbo and MozJPEG images, but used only 2.8
bits per pixel compared to libjpeg-turbo and MozJPEG that used 3.8 and 3.5 bits
per pixel respectively. The raw ratings and source images are publicly
available for further analysis and study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency
  Attacks in Computer Vision <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Müller, Erwin Quiring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resource efficiency plays an important role for machine learning nowadays.
The energy and decision latency are two critical aspects to ensure a
sustainable and practical application. Unfortunately, the energy consumption
and decision latency are not robust against adversaries. Researchers have
recently demonstrated that attackers can compute and submit so-called sponge
examples at inference time to increase the energy consumption and decision
latency of neural networks. In computer vision, the proposed strategy crafts
inputs with less activation sparsity which could otherwise be used to
accelerate the computation. In this paper, we analyze the mechanism how these
energy-latency attacks reduce activation sparsity. In particular, we find that
input uniformity is a key enabler. A uniform image, that is, an image with
mostly flat, uniformly colored surfaces, triggers more activations due to a
specific interplay of convolution, batch normalization, and ReLU activation.
Based on these insights, we propose two new simple, yet effective strategies
for crafting sponge examples: sampling images from a probability distribution
and identifying dense, yet inconspicuous inputs in natural datasets. We
empirically examine our findings in a comprehensive evaluation with multiple
image classification models and show that our attack achieves the same sparsity
effect as prior sponge-example methods, but at a fraction of computation
effort. We also show that our sponge examples transfer between different neural
networks. Finally, we discuss applications of our findings for the good by
improving efficiency by increasing sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the DLSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional
  Synthesis and Sampling of Hand-Object Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D hand mesh robustly from a single image is very challenging,
due to the lack of diversity in existing real-world datasets. While data
synthesis helps relieve the issue, the syn-to-real gap still hinders its usage.
In this work, we present HandBooster, a new approach to uplift the data
diversity and boost the 3D hand-mesh reconstruction performance by training a
conditional generative space on hand-object interactions and purposely sampling
the space to synthesize effective data samples. First, we construct versatile
content-aware conditions to guide a diffusion model to produce realistic images
with diverse hand appearances, poses, views, and backgrounds; favorably,
accurate 3D annotations are obtained for free. Then, we design a novel
condition creator based on our similarity-aware distribution sampling
strategies to deliberately find novel and realistic interaction poses that are
distinctive from the training set. Equipped with our method, several baselines
can be significantly improved beyond the SOTA on the HO3D and DexYCB
benchmarks. Our code will be released on
https://github.com/hxwork/HandBooster_Pytorch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images
  with Deep Learning -- A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Amirian, Daniel Barco, Ivo Herzig, Frank-Peter Schilling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based approaches have been used to improve image quality in
cone-beam computed tomography (CBCT), a medical imaging technique often used in
applications such as image-guided radiation therapy, implant dentistry or
orthopaedics. In particular, while deep learning methods have been applied to
reduce various types of CBCT image artifacts arising from motion, metal
objects, or low-dose acquisition, a comprehensive review summarizing the
successes and shortcomings of these approaches, with a primary focus on the
type of artifacts rather than the architecture of neural networks, is lacking
in the literature. In this review, the data generation and simulation
pipelines, and artifact reduction techniques are specifically investigated for
each type of artifact. We provide an overview of deep learning techniques that
have successfully been shown to reduce artifacts in 3D, as well as in
time-resolved (4D) CBCT through the use of projection- and/or volume-domain
optimizations, or by introducing neural networks directly within the CBCT
reconstruction algorithms. Research gaps are identified to suggest avenues for
future exploration. One of the key findings of this work is an observed trend
towards the use of generative models including GANs and score-based or
diffusion models, accompanied with the need for more diverse and open training
datasets and simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures, 1 Table, published in IEEE Access Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CosalPure: Learning Concept from Group Images for Robust Co-Saliency
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-salient object detection (CoSOD) aims to identify the common and salient
(usually in the foreground) regions across a given group of images. Although
achieving significant progress, state-of-the-art CoSODs could be easily
affected by some adversarial perturbations, leading to substantial accuracy
reduction. The adversarial perturbations can mislead CoSODs but do not change
the high-level semantic information (e.g., concept) of the co-salient objects.
In this paper, we propose a novel robustness enhancement framework by first
learning the concept of the co-salient objects based on the input group images
and then leveraging this concept to purify adversarial perturbations, which are
subsequently fed to CoSODs for robustness enhancement. Specifically, we propose
CosalPure containing two modules, i.e., group-image concept learning and
concept-guided diffusion purification. For the first module, we adopt a
pre-trained text-to-image diffusion model to learn the concept of co-salient
objects within group images where the learned concept is robust to adversarial
examples. For the second module, we map the adversarial image to the latent
space and then perform diffusion generation by embedding the learned concept
into the noise prediction function as an extra condition. Our method can
effectively alleviate the influence of the SOTA adversarial attack containing
different adversarial patterns, including exposure and noise. The extensive
results demonstrate that our method could enhance the robustness of CoSODs
significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Calibration for Disentangled Text-to-Image Personalization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbing Zhang, Mengping Yang, Qin Zhou, Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent thrilling progress in large-scale text-to-image (T2I) models has
unlocked unprecedented synthesis quality of AI-generated content (AIGC)
including image generation, 3D and video composition. Further, personalized
techniques enable appealing customized production of a novel concept given only
several images as reference. However, an intriguing problem persists: Is it
possible to capture multiple, novel concepts from one single reference image?
In this paper, we identify that existing approaches fail to preserve visual
consistency with the reference image and eliminate cross-influence from
concepts. To alleviate this, we propose an attention calibration mechanism to
improve the concept-level understanding of the T2I model. Specifically, we
first introduce new learnable modifiers bound with classes to capture
attributes of multiple concepts. Then, the classes are separated and
strengthened following the activation of the cross-attention operation,
ensuring comprehensive and self-contained concepts. Additionally, we suppress
the attention activation of different classes to mitigate mutual influence
among concepts. Together, our proposed method, dubbed DisenDiff, can learn
disentangled multiple concepts from one single image and produce novel
customized images with learned concepts. We demonstrate that our method
outperforms the current state of the art in both qualitative and quantitative
evaluations. More importantly, our proposed techniques are compatible with LoRA
and inpainting pipelines, enabling more interactive experiences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OrCo: Towards Better Generalization via Orthogonality and Contrast for
  Few-Shot Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noor Ahmed, Anna Kukleva, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which
the problem space expands with limited data. FSCIL methods inherently face the
challenge of catastrophic forgetting as data arrives incrementally, making
models susceptible to overwriting previously acquired knowledge. Moreover,
given the scarcity of labeled samples available at any given time, models may
be prone to overfitting and find it challenging to strike a balance between
extensive pretraining and the limited incremental data. To address these
challenges, we propose the OrCo framework built on two core principles:
features' orthogonality in the representation space, and contrastive learning.
In particular, we improve the generalization of the embedding space by
employing a combination of supervised and self-supervised contrastive losses
during the pretraining phase. Additionally, we introduce OrCo loss to address
challenges arising from data limitations during incremental sessions. Through
feature space perturbations and orthogonality between classes, the OrCo loss
maximizes margins and reserves space for the following incremental data. This,
in turn, ensures the accommodation of incoming classes in the feature space
without compromising previously acquired knowledge. Our experimental results
showcase state-of-the-art performance across three benchmark datasets,
including mini-ImageNet, CIFAR100, and CUB datasets. Code is available at
https://github.com/noorahmedds/OrCo
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency
  Aware and Realistic Brightness Constraint <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Cong, Jie Gui, Jing Zhang, Junming Hou, Hao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research based on deep learning has extensively explored the problem
of daytime image dehazing. However, few studies have considered the
characteristics of nighttime hazy scenes. There are two distinctions between
nighttime and daytime haze. First, there may be multiple active colored light
sources with lower illumination intensity in nighttime scenes, which may cause
haze, glow and noise with localized, coupled and frequency inconsistent
characteristics. Second, due to the domain discrepancy between simulated and
real-world data, unrealistic brightness may occur when applying a dehazing
model trained on simulated data to real-world data. To address the above two
issues, we propose a semi-supervised model for real-world nighttime dehazing.
First, the spatial attention and frequency spectrum filtering are implemented
as a spatial-frequency domain information interaction module to handle the
first issue. Second, a pseudo-label-based retraining strategy and a local
window-based brightness loss for semi-supervised training process is designed
to suppress haze and glow while achieving realistic brightness. Experiments on
public benchmarks validate the effectiveness of the proposed method and its
superiority over state-of-the-art methods. The source code and Supplementary
Materials are placed in the https://github.com/Xiaofeng-life/SFSNiD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siang Chen, Wei Tang, Pengwei Xie, Wenming Yang, Guijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast and robust object grasping in clutter is a crucial component of
robotics. Most current works resort to the whole observed point cloud for 6-Dof
grasp generation, ignoring the guidance information excavated from global
semantics, thus limiting high-quality grasp generation and real-time
performance. In this work, we show that the widely used heatmaps are
underestimated in the efficiency of 6-Dof grasp generation. Therefore, we
propose an effective local grasp generator combined with grasp heatmaps as
guidance, which infers in a global-to-local semantic-to-point way.
Specifically, Gaussian encoding and the grid-based strategy are applied to
predict grasp heatmaps as guidance to aggregate local points into graspable
regions and provide global semantic information. Further, a novel non-uniform
anchor sampling mechanism is designed to improve grasp accuracy and diversity.
Benefiting from the high-efficiency encoding in the image space and focusing on
points in local graspable regions, our framework can perform high-quality grasp
detection in real-time and achieve state-of-the-art results. In addition, real
robot experiments demonstrate the effectiveness of our method with a success
rate of 94% and a clutter completion rate of 100%. Our code is available at
https://github.com/THU-VCLab/HGGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extensive results on GraspNet-1B dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection
  of Pathological Pulmonary CT scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aissam Djahnine, Alexandre Popoff, Emilien Jupin-Delevaux, Vincent Cottin, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised pathology detection can be implemented by training a model on
healthy data only and measuring the deviation from the training set upon
inference, for example with CNN-based feature extraction and one-class
classifiers, or reconstruction-score-based methods such as AEs, GANs and
Diffusion models. Normalizing Flows (NF) have the ability to directly learn the
probability distribution of training examples through an invertible
architecture. We leverage this property in a novel 3D NF-based model named
CT-3DFlow, specifically tailored for patient-level pulmonary pathology
detection in chest CT data. Our model is trained unsupervised on healthy 3D
pulmonary CT patches, and detects deviations from its log-likelihood
distribution as anomalies. We aggregate patches-level likelihood values from a
patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.
Out-of-distribution detection performance is evaluated using expert annotations
on a separate chest CT test dataset, outperforming other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ParCo: Part-Coordinating Text-to-Motion Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiran Zou, Shangyuan Yuan, Shian Du, Yu Wang, Chang Liu, Yi Xu, Jie Chen, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a challenging task: text-to-motion synthesis, aiming to generate
motions that align with textual descriptions and exhibit coordinated movements.
Currently, the part-based methods introduce part partition into the motion
synthesis process to achieve finer-grained generation. However, these methods
encounter challenges such as the lack of coordination between different part
motions and difficulties for networks to understand part concepts. Moreover,
introducing finer-grained part concepts poses computational complexity
challenges. In this paper, we propose Part-Coordinating Text-to-Motion
Synthesis (ParCo), endowed with enhanced capabilities for understanding part
motions and communication among different part motion generators, ensuring a
coordinated and fined-grained motion synthesis. Specifically, we discretize
whole-body motion into multiple part motions to establish the prior concept of
different parts. Afterward, we employ multiple lightweight generators designed
to synthesize different part motions and coordinate them through our part
coordination module. Our approach demonstrates superior performance on common
benchmarks with economic computations, including HumanML3D and KIT-ML,
providing substantial evidence of its effectiveness. Code is available at
https://github.com/qrzou/ParCo .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEMIT: H&E to Multiplex-immunohistochemistry Image Translation with
  Dual-Branch Pix2pix Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Bian, Beth Philips, Tim Cootes, Martin Fergie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational analysis of multiplexed immunofluorescence histology data is
emerging as an important method for understanding the tumour micro-environment
in cancer. This work presents HEMIT, a dataset designed for translating
Hematoxylin and Eosin (H&E) sections to multiplex-immunohistochemistry (mIHC)
images, featuring DAPI, CD3, and panCK markers. Distinctively, HEMIT's mIHC
images are multi-component and cellular-level aligned with H&E, enriching
supervised stain translation tasks. To our knowledge, HEMIT is the first
publicly available cellular-level aligned dataset that enables H&E to
multi-target mIHC image translation. This dataset provides the computer vision
community with a valuable resource to develop novel computational methods which
have the potential to gain new insights from H&E slide archives.
  We also propose a new dual-branch generator architecture, using residual
Convolutional Neural Networks (CNNs) and Swin Transformers which achieves
better translation outcomes than other popular algorithms. When evaluated on
HEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the
highest overall score on key metrics including the Structural Similarity Index
Measure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio
(PSNR). Additionally, downstream analysis has been used to further validate the
quality of the generated mIHC images. These results set a new benchmark in the
field of stain translation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct mineral content prediction from drill core images via transfer
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romana Boiger, Sergey V. Churakov, Ignacio Ballester Llagaria, Georg Kosakowski, Raphael Wüst, Nikolaos I. Prasianakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep subsurface exploration is important for mining, oil and gas industries,
as well as in the assessment of geological units for the disposal of chemical
or nuclear waste, or the viability of geothermal energy systems. Typically,
detailed examinations of subsurface formations or units are performed on
cuttings or core materials extracted during drilling campaigns, as well as on
geophysical borehole data, which provide detailed information about the
petrophysical properties of the rocks. Depending on the volume of rock samples
and the analytical program, the laboratory analysis and diagnostics can be very
time-consuming. This study investigates the potential of utilizing machine
learning, specifically convolutional neural networks (CNN), to assess the
lithology and mineral content solely from analysis of drill core images, aiming
to support and expedite the subsurface geological exploration. The paper
outlines a comprehensive methodology, encompassing data preprocessing, machine
learning methods, and transfer learning techniques. The outcome reveals a
remarkable 96.7% accuracy in the classification of drill core segments into
distinct formation classes. Furthermore, a CNN model was trained for the
evaluation of mineral content using a learning data set from multidimensional
log analysis data (silicate, total clay, carbonate). When benchmarked against
laboratory XRD measurements on samples from the cores, both the advanced
multidimensional log analysis model and the neural network approach developed
here provide equally good performance. This work demonstrates that deep
learning and particularly transfer learning can support extracting
petrophysical properties, including mineral content and formation
classification, from drill core images, thus offering a road map for enhancing
model performance and data set quality in image-based analysis of drill cores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VersaT2I: Improving Text-to-Image Models with Versatile Reward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianshu Guo, Wenhao Chai, Jie Deng, Hsiang-Wei Huang, Tian Ye, Yichen Xu, Jiawei Zhang, Jenq-Neng Hwang, Gaoang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-image (T2I) models have benefited from large-scale and
high-quality data, demonstrating impressive performance. However, these T2I
models still struggle to produce images that are aesthetically pleasing,
geometrically accurate, faithful to text, and of good low-level quality. We
present VersaT2I, a versatile training framework that can boost the performance
with multiple rewards of any T2I model. We decompose the quality of the image
into several aspects such as aesthetics, text-image alignment, geometry,
low-level quality, etc. Then, for every quality aspect, we select high-quality
images in this aspect generated by the model as the training set to finetune
the T2I model using the Low-Rank Adaptation (LoRA). Furthermore, we introduce a
gating function to combine multiple quality aspects, which can avoid conflicts
between different quality aspects. Our method is easy to extend and does not
require any manual annotation, reinforcement learning, or model architecture
changes. Extensive experiments demonstrate that VersaT2I outperforms the
baseline methods across various quality criteria.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoub Karine, Thibault Napoléon, Maher Jridi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new knowledge distillation method tailored for image
semantic segmentation, termed Intra- and Inter-Class Knowledge Distillation
(I2CKD). The focus of this method is on capturing and transferring knowledge
between the intermediate layers of teacher (cumbersome model) and student
(compact model). For knowledge extraction, we exploit class prototypes derived
from feature maps. To facilitate knowledge transfer, we employ a triplet loss
in order to minimize intra-class variances and maximize inter-class variances
between teacher and student prototypes. Consequently, I2CKD enables the student
to better mimic the feature representation of the teacher for each class,
thereby enhancing the segmentation performance of the compact network.
Extensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal
VOC and CamVid, using various teacher-student network pairs demonstrate the
effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling uncertainty for Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Savant, Diego Valsesia, Enrico Magli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Stochastic Gaussian Splatting (SGS): the first framework for
uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the
novel-view synthesis field by achieving impressive reconstruction quality at a
fraction of the computational cost of Neural Radiance Fields (NeRF). However,
contrary to the latter, it still lacks the ability to provide information about
the confidence associated with their outputs. To address this limitation, in
this paper, we introduce a Variational Inference-based approach that seamlessly
integrates uncertainty prediction into the common rendering pipeline of GS.
Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new
term in the loss function, enabling optimization of uncertainty estimation
alongside image reconstruction. Experimental results on the LLFF dataset
demonstrate that our method outperforms existing approaches in terms of both
image rendering quality and uncertainty estimation accuracy. Overall, our
framework equips practitioners with valuable insights into the reliability of
synthesized views, facilitating safer decision-making in real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionFace: Towards a Comprehensive <span class="highlight-title">Dataset</span> for Diffusion-Based Face
  Forgery Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongxi Chen, Ke Sun, Ziyin Zhou, Xianming Lin, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progress in deep learning has given rise to hyper-realistic facial
forgery methods, leading to concerns related to misinformation and security
risks. Existing face forgery datasets have limitations in generating
high-quality facial images and addressing the challenges posed by evolving
generative techniques. To combat this, we present DiffusionFace, the first
diffusion-based face forgery dataset, covering various forgery categories,
including unconditional and Text Guide facial image generation, Img2Img,
Inpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace
dataset stands out with its extensive collection of 11 diffusion models and the
high-quality of the generated images, providing essential metadata and a
real-world internet-sourced forgery facial image dataset for evaluation.
Additionally, we provide an in-depth analysis of the data and introduce
practical evaluation protocols to rigorously assess discriminative models'
effectiveness in detecting counterfeit facial images, aiming to enhance
security in facial image authentication processes. The dataset is available for
download at \url{https://github.com/Rapisurazurite/DiffFace}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain
  Adaptive Segmentation of 3D Point Clouds <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan Guo, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to
annotating new domains. Self-training is a competitive approach for this task,
but its performance is limited by different sensor sampling patterns (i.e.,
variations in point density) and incomplete training strategies. In this work,
we propose a density-guided translator (DGT), which translates point density
between domains, and integrates it into a two-stage self-training pipeline
named DGT-ST. First, in contrast to existing works that simultaneously conduct
data generation and feature/output alignment within unstable adversarial
training, we employ the non-learnable DGT to bridge the domain gap at the input
level. Second, to provide a well-initialized model for self-training, we
propose a category-level adversarial network in stage one that utilizes the
prototype to prevent negative transfer. Finally, by leveraging the designs
above, a domain-mixed self-training method with source-aware consistency loss
is proposed in stage two to narrow the domain gap further. Experiments on two
synthetic-to-real segmentation tasks (SynLiDAR $\rightarrow$ semanticKITTI and
SynLiDAR $\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms
state-of-the-art methods, achieving 9.4$\%$ and 4.3$\%$ mIoU improvements,
respectively. Code is available at \url{https://github.com/yuan-zm/DGT-ST}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Segmentation and Classification of Red Blood Cells Using a
  Large Multi-Scanner <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elmanna, Ahmed Elsafty, Yomna Ahmed, Muhammad Rushdi, Ahmed Morsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital pathology has recently been revolutionized by advancements in
artificial intelligence, deep learning, and high-performance computing. With
its advanced tools, digital pathology can help improve and speed up the
diagnostic process, reduce human errors, and streamline the reporting step. In
this paper, we report a new large red blood cell (RBC) image dataset and
propose a two-stage deep learning framework for RBC image segmentation and
classification. The dataset is a highly diverse dataset of more than 100K RBCs
containing eight different classes. The dataset, which is considerably larger
than any publicly available hematopathology dataset, was labeled independently
by two hematopathologists who also manually created masks for RBC cell
segmentation. Subsequently, in the proposed framework, first, a U-Net model was
trained to achieve automatic RBC image segmentation. Second, an EfficientNetB0
model was trained to classify RBC images into one of the eight classes using a
transfer learning approach with a 5X2 cross-validation scheme. An IoU of 98.03%
and an average classification accuracy of 96.5% were attained on the test set.
Moreover, we have performed experimental comparisons against several prominent
CNN models. These comparisons show the superiority of the proposed model with a
good balance between performance and computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffStyler: Diffusion-based Localized Image Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image style transfer aims to imbue digital imagery with the distinctive
attributes of style targets, such as colors, brushstrokes, shapes, whilst
concurrently preserving the semantic integrity of the content. Despite the
advancements in arbitrary style transfer methods, a prevalent challenge remains
the delicate equilibrium between content semantics and style attributes. Recent
developments in large-scale text-to-image diffusion models have heralded
unprecedented synthesis capabilities, albeit at the expense of relying on
extensive and often imprecise textual descriptions to delineate artistic
styles. Addressing these limitations, this paper introduces DiffStyler, a novel
approach that facilitates efficient and precise arbitrary image style transfer.
DiffStyler lies the utilization of a text-to-image Stable Diffusion model-based
LoRA to encapsulate the essence of style targets. This approach, coupled with
strategic cross-LoRA feature and attention injection, guides the style transfer
process. The foundation of our methodology is rooted in the observation that
LoRA maintains the spatial feature consistency of UNet, a discovery that
further inspired the development of a mask-wise style transfer technique. This
technique employs masks extracted through a pre-trained FastSAM model,
utilizing mask prompts to facilitate feature fusion during the denoising
process, thereby enabling localized style transfer that preserves the original
image's unaffected regions. Moreover, our approach accommodates multiple style
targets through the use of corresponding masks. Through extensive
experimentation, we demonstrate that DiffStyler surpasses previous methods in
achieving a more harmonious balance between content preservation and style
integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Vision-and-Language Navigation With Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valay Bundele, Mahesh Bhupati, Biplab Banerjee, Aditya Grover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of vision-and-language navigation (VLN) has typically relied on
expert trajectories, which may not always be available in real-world situations
due to the significant effort required to collect them. On the other hand,
existing approaches to training VLN agents that go beyond available expert data
involve data augmentations or online exploration which can be tedious and
risky. In contrast, it is easy to access large repositories of suboptimal
offline trajectories. Inspired by research in offline reinforcement learning
(ORL), we introduce a new problem setup of VLN-ORL which studies VLN using
suboptimal demonstration data. We introduce a simple and effective
reward-conditioned approach that can account for dataset suboptimality for
training VLN agents, as well as benchmarks to evaluate progress and promote
research in this area. We empirically study various noise models for
characterizing dataset suboptimality among other unique challenges in VLN-ORL
and instantiate it for the VLN$\circlearrowright$BERT and MTVM architectures in
the R2R and RxR environments. Our experiments demonstrate that the proposed
reward-conditioned approach leads to significant performance improvements, even
in complex and intricate environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (04/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Young-Jae Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are five types of trajectory prediction tasks: deterministic,
stochastic, domain adaptation, momentary observation, and few-shot. These
associated tasks are defined by various factors, such as the length of input
paths, data split and pre-processing methods. Interestingly, even though they
commonly take sequential coordinates of observations as input and infer future
paths in the same coordinates as output, designing specialized architectures
for each task is still necessary. For the other task, generality issues can
lead to sub-optimal performances. In this paper, we propose SingularTrajectory,
a diffusion-based universal trajectory prediction framework to reduce the
performance gap across the five tasks. The core of SingularTrajectory is to
unify a variety of human dynamics representations on the associated tasks. To
do this, we first build a Singular space to project all types of motion
patterns from each task into one embedding space. We next propose an adaptive
anchor working in the Singular space. Unlike traditional fixed anchor methods
that sometimes yield unacceptable paths, our adaptive anchor enables correct
anchors, which are put into a wrong location, based on a traversability map.
Finally, we adopt a diffusion-based predictor to further enhance the prototype
paths using a cascaded denoising process. Our unified framework ensures the
generality across various benchmark settings such as input modality, and
trajectory lengths. Extensive experiments on five public benchmarks demonstrate
that SingularTrajectory substantially outperforms existing models, highlighting
its effectiveness in estimating general dynamics of human movements. Code is
publicly available at https://github.com/inhwanbae/SingularTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\mathrm{F^2Depth}$: <span class="highlight-title">Self-supervised</span> Indoor Monocular Depth Estimation
  via Optical Flow Consistency and Feature Map Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotong Guo, Huijie Zhao, Shuwei Shao, Xudong Li, Baochang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised monocular depth estimation methods have been increasingly
given much attention due to the benefit of not requiring large, labelled
datasets. Such self-supervised methods require high-quality salient features
and consequently suffer from severe performance drop for indoor scenes, where
low-textured regions dominant in the scenes are almost indiscriminative. To
address the issue, we propose a self-supervised indoor monocular depth
estimation framework called $\mathrm{F^2Depth}$. A self-supervised optical flow
estimation network is introduced to supervise depth learning. To improve
optical flow estimation performance in low-textured areas, only some patches of
points with more discriminative features are adopted for finetuning based on
our well-designed patch-based photometric loss. The finetuned optical flow
estimation network generates high-accuracy optical flow as a supervisory signal
for depth estimation. Correspondingly, an optical flow consistency loss is
designed. Multi-scale feature maps produced by finetuned optical flow
estimation network perform warping to compute feature map synthesis loss as
another supervisory signal for depth learning. Experimental results on the NYU
Depth V2 dataset demonstrate the effectiveness of the framework and our
proposed losses. To evaluate the generalization ability of our
$\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of
approximately 1500 points selected from 99 images in 18 scenes. Zero-shot
generalization experiments on 7-Scenes dataset and Campus Indoor achieve
$\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show
that our model can generalize well to monocular images captured in unknown
indoor scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backpropagation-free Network for 3D Test-time Adaptation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanshuo Wang, Ali Cheraghian, Zeeshan Hayder, Jie Hong, Sameera Ramasinghe, Shafin Rahman, David Ahmedt-Aristizabal, Xuesong Li, Lars Petersson, Mehrtash Harandi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world systems often encounter new data over time, which leads to
experiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods
tend to apply computationally heavy and memory-intensive backpropagation-based
approaches to handle this. Here, we propose a novel method that uses a
backpropagation-free approach for TTA for the specific case of 3D data. Our
model uses a two-stream architecture to maintain knowledge about the source
domain as well as complementary target-domain-specific information. The
backpropagation-free property of our model helps address the well-known
forgetting problem and mitigates the error accumulation issue. The proposed
method also eliminates the need for the usually noisy process of
pseudo-labeling and reliance on costly self-supervised training. Moreover, our
method leverages subspace learning, effectively reducing the distribution
variance between the two domains. Furthermore, the source-domain-specific and
the target-domain-specific streams are aligned using a novel entropy-based
adaptive fusion strategy. Extensive experiments on popular benchmarks
demonstrate the effectiveness of our method. The code will be available at
https://github.com/abie-e/BFTT3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECNet: Effective Controllable Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Li, Keqiang Sun, Zhixin Lai, Xiaoshi Wu, Feng Qiu, Haoran Xie, Kazunori Miyata, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conditional text-to-image diffusion models have garnered significant
attention in recent years. However, the precision of these models is often
compromised mainly for two reasons, ambiguous condition input and inadequate
condition guidance over single denoising loss. To address the challenges, we
introduce two innovative solutions. Firstly, we propose a Spatial Guidance
Injector (SGI) which enhances conditional detail by encoding text inputs with
precise annotation information. This method directly tackles the issue of
ambiguous control inputs by providing clear, annotated guidance to the model.
Secondly, to overcome the issue of limited conditional supervision, we
introduce Diffusion Consistency Loss (DCL), which applies supervision on the
denoised latent code at any given time step. This encourages consistency
between the latent code at each time step and the input signal, thereby
enhancing the robustness and accuracy of the output. The combination of SGI and
DCL results in our Effective Controllable Network (ECNet), which offers a more
accurate controllable end-to-end text-to-image generation framework with a more
precise conditioning input and stronger controllable supervision. We validate
our approach through extensive experiments on generation under various
conditions, such as human body skeletons, facial landmarks, and sketches of
general objects. The results consistently demonstrate that our method
significantly enhances the controllability and robustness of the generated
images, outperforming existing state-of-the-art controllable text-to-image
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is
  Critical for Semi-supervised Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wu, Junbiao Pang, Baochang Zhang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) is a practical challenge in computer vision.
Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of
The Art (SOTA) performances in SSL. These approaches employ a
threshold-to-pseudo-label (T2L) process to generate PLs by truncating the
confidence scores of unlabeled data predicted by the self-training method.
However, self-trained models typically yield biased and high-variance
predictions, especially in the scenarios when a little labeled data are
supplied. To address this issue, we propose a lightweight channel-based
ensemble method to effectively consolidate multiple inferior PLs into the
theoretically guaranteed unbiased and low-variance one. Importantly, our
approach can be readily extended to any SSL framework, such as FixMatch or
FreeMatch. Experimental results demonstrate that our method significantly
outperforms state-of-the-art techniques on CIFAR10/100 in terms of
effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Velibor Bojkovic, Bin Gu, Kun Suo, Kai Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient
computing compared with Artificial Neural Networks (ANNs), closely mirroring
biological neural processes. However, this potential comes with inherent
challenges in directly training SNNs through spatio-temporal backpropagation --
stemming from the temporal dynamics of spiking neurons and their discrete
signal processing -- which necessitates alternative ways of training, most
notably through ANN-SNN conversion. In this work, we introduce a lightweight
Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing
conversion accuracy without the computational overhead. We ground our method on
provided theoretical findings that through proper temporal bias calibration the
expected error of ANN-SNN conversion can be reduced to be zero after each time
step. We further propose a heuristic algorithm for finding the temporal bias
only in the forward pass, thus eliminating the computational burden of
backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet
datasets, achieving a notable increase in accuracy on all datasets. Codes are
released at a GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BAM: Box Abstraction Monitors for Real-time OoD Detection in Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changshun Wu, Weicheng He, Chih-Hong Cheng, Xiaowei Huang, Saddek Bensalem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OoD) detection techniques for deep neural networks
(DNNs) become crucial thanks to their filtering of abnormal inputs, especially
when DNNs are used in safety-critical applications and interact with an open
and dynamic environment. Nevertheless, integrating OoD detection into
state-of-the-art (SOTA) object detection DNNs poses significant challenges,
partly due to the complexity introduced by the SOTA OoD construction methods,
which require the modification of DNN architecture and the introduction of
complex loss functions. This paper proposes a simple, yet surprisingly
effective, method that requires neither retraining nor architectural change in
object detection DNN, called Box Abstraction-based Monitors (BAM). The novelty
of BAM stems from using a finite union of convex box abstractions to capture
the learned features of objects for in-distribution (ID) data, and an important
observation that features from OoD data are more likely to fall outside of
these boxes. The union of convex regions within the feature space allows the
formation of non-convex and interpretable decision boundaries, overcoming the
limitations of VOS-like detectors without sacrificing real-time performance.
Experiments integrating BAM into Faster R-CNN-based object detection DNNs
demonstrate a considerably improved performance against SOTA OoD detection
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ship in Sight: Diffusion Models for Ship-Image Super Resolution <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, remarkable advancements have been achieved in the field of
image generation, primarily driven by the escalating demand for high-quality
outcomes across various image generation subtasks, such as inpainting,
denoising, and super resolution. A major effort is devoted to exploring the
application of super-resolution techniques to enhance the quality of
low-resolution images. In this context, our method explores in depth the
problem of ship image super resolution, which is crucial for coastal and port
surveillance. We investigate the opportunity given by the growing interest in
text-to-image diffusion models, taking advantage of the prior knowledge that
such foundation models have already learned. In particular, we present a
diffusion-model-based architecture that leverages text conditioning during
training while being class-aware, to best preserve the crucial details of the
ships during the generation of the super-resoluted image. Since the specificity
of this task and the scarcity availability of off-the-shelf data, we also
introduce a large labeled ship dataset scraped from online ship images, mostly
from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method
achieves more robust results than other deep learning models previously
employed for super resolution, as proven by the multiple experiments performed.
Moreover, we investigate how this model can benefit downstream tasks, such as
classification and object detection, thus emphasizing practical implementation
in a real-world scenario. Experimental results show flexibility, reliability,
and impressive performance of the proposed framework over state-of-the-art
methods for different tasks. The code is available at:
https://github.com/LuigiSigillo/ShipinSight .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 International Joint Conference on Neural Networks
  (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViTAR: Vision <span class="highlight-title">Transformer</span> with Any Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  his paper tackles a significant challenge faced by Vision Transformers
(ViTs): their constrained scalability across different image resolutions.
Typically, ViTs experience a performance decline when processing resolutions
different from those seen during training. Our work introduces two key
innovations to address this issue. Firstly, we propose a novel module for
dynamic resolution adjustment, designed with a single Transformer block,
specifically to achieve highly efficient incremental token integration.
Secondly, we introduce fuzzy positional encoding in the Vision Transformer to
provide consistent positional awareness across multiple resolutions, thereby
preventing overfitting to any single training resolution. Our resulting model,
ViTAR (Vision Transformer with Any Resolution), demonstrates impressive
adaptability, achieving 83.3\% top-1 accuracy at a 1120x1120 resolution and
80.4\% accuracy at a 4032x4032 resolution, all while reducing computational
costs. ViTAR also shows strong performance in downstream tasks such as instance
and semantic segmentation and can easily combined with self-supervised learning
techniques like Masked AutoEncoder. Our work provides a cost-effective solution
for enhancing the resolution scalability of ViTs, paving the way for more
versatile and efficient high-resolution image processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific
  Boundaries for Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ba Hung Ngo, Nhat-Tuong Do-Tran, Tuan-Ngoc Nguyen, Hae-Gon Jeon, Tae Jong Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most domain adaptation (DA) methods are based on either a convolutional
neural networks (CNNs) or a vision transformers (ViTs). They align the
distribution differences between domains as encoders without considering their
unique characteristics. For instance, ViT excels in accuracy due to its
superior ability to capture global representations, while CNN has an advantage
in capturing local representations. This fact has led us to design a hybrid
method to fully take advantage of both ViT and CNN, called Explicitly
Class-specific Boundaries (ECB). ECB learns CNN on ViT to combine their
distinct strengths. In particular, we leverage ViT's properties to explicitly
find class-specific decision boundaries by maximizing the discrepancy between
the outputs of the two classifiers to detect target samples far from the source
support. In contrast, the CNN encoder clusters target features based on the
previously defined class-specific boundaries by minimizing the discrepancy
between the probabilities of the two classifiers. Finally, ViT and CNN mutually
exchange knowledge to improve the quality of pseudo labels and reduce the
knowledge discrepancies of these models. Compared to conventional DA methods,
our ECB achieves superior performance, which verifies its effectiveness in this
hybrid model. The project website can be found
https://dotrannhattuong.github.io/ECB/website/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonoHair: High-Fidelity Hair Modeling from a Monocular Video <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyu Wu, Lingchen Yang, Zhiyi Kuang, Yao Feng, Xutao Han, Yuefan Shen, Hongbo Fu, Kun Zhou, Youyi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Undoubtedly, high-fidelity 3D hair is crucial for achieving realism, artistic
expression, and immersion in computer graphics. While existing 3D hair modeling
methods have achieved impressive performance, the challenge of achieving
high-quality hair reconstruction persists: they either require strict capture
conditions, making practical applications difficult, or heavily rely on learned
prior data, obscuring fine-grained details in images. To address these
challenges, we propose MonoHair,a generic framework to achieve high-fidelity
hair reconstruction from a monocular video, without specific requirements for
environments. Our approach bifurcates the hair modeling process into two main
stages: precise exterior reconstruction and interior structure inference. The
exterior is meticulously crafted using our Patch-based Multi-View Optimization
(PMVO). This method strategically collects and integrates hair information from
multiple views, independent of prior data, to produce a high-fidelity exterior
3D line map. This map not only captures intricate details but also facilitates
the inference of the hair's inner structure. For the interior, we employ a
data-driven, multi-view 3D hair reconstruction method. This method utilizes 2D
structural renderings derived from the reconstructed exterior, mirroring the
synthetic 2D inputs used during training. This alignment effectively bridges
the domain gap between our training data and real-world data, thereby enhancing
the accuracy and reliability of our interior structure inference. Lastly, we
generate a strand model and resolve the directional ambiguity by our hair
growth algorithm. Our experiments demonstrate that our method exhibits
robustness across diverse hairstyles and achieves state-of-the-art performance.
For more results, please refer to our project page
https://keyuwu-cs.github.io/MonoHair/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying and Mitigating Unimodal Biases in Multimodal Large Language
  Models: A Causal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have facilitated the
development of Multimodal LLMs (MLLMs). Despite their impressive capabilities,
MLLMs often suffer from an over-reliance on unimodal biases (e.g., language
bias and vision bias), leading to incorrect answers in complex multimodal
tasks. To investigate this issue, we propose a causal framework to interpret
the biases in Visual Question Answering (VQA) problems. Within our framework,
we devise a causal graph to elucidate the predictions of MLLMs on VQA problems,
and assess the causal effect of biases through an in-depth causal analysis.
Motivated by the causal graph, we introduce a novel MORE dataset, consisting of
12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,
necessitating multi-hop reasoning and the surmounting of unimodal biases.
Furthermore, we propose two strategies to mitigate unimodal biases and enhance
MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)
framework for limited-access MLLMs and the refinement of open-source MLLMs
through fine-tuning. Extensive quantitative and qualitative experiments offer
valuable insights for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Inclusion Matching for Animation Paint Bucket Colorization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuekun Dai, Shangchen Zhou, Qinyue Li, Chongyi Li, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colorizing line art is a pivotal task in the production of hand-drawn cel
animation. This typically involves digital painters using a paint bucket tool
to manually color each segment enclosed by lines, based on RGB values
predetermined by a color designer. This frame-by-frame process is both arduous
and time-intensive. Current automated methods mainly focus on segment matching.
This technique migrates colors from a reference to the target frame by aligning
features within line-enclosed segments across frames. However, issues like
occlusion and wrinkles in animations often disrupt these direct
correspondences, leading to mismatches. In this work, we introduce a new
learning-based inclusion matching pipeline, which directs the network to
comprehend the inclusion relationships between segments rather than relying
solely on direct visual correspondences. Our method features a two-stage
pipeline that integrates a coarse color warping module with an inclusion
matching module, enabling more nuanced and accurate colorization. To facilitate
the training of our network, we also develope a unique dataset, referred to as
PaintBucket-Character. This dataset includes rendered line arts alongside their
colorized counterparts, featuring various 3D characters. Extensive experiments
demonstrate the effectiveness and superiority of our method over existing
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to CVPR 2024. Project Page:
  https://ykdai.github.io/projects/InclusionMatching</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for
  Tumor Segmentation in PET/CT Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinpeng Lu, Jingyun Chen, Linghan Cai, Songhan Jiang, Yongbing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positron emission tomography (PET) combined with computed tomography (CT)
imaging is routinely used in cancer diagnosis and prognosis by providing
complementary information. Automatically segmenting tumors in PET/CT images can
significantly improve examination efficiency. Traditional multi-modal
segmentation solutions mainly rely on concatenation operations for modality
fusion, which fail to effectively model the non-linear dependencies between PET
and CT modalities. Recent studies have investigated various approaches to
optimize the fusion of modality-specific features for enhancing joint
representations. However, modality-specific encoders used in these methods
operate independently, inadequately leveraging the synergistic relationships
inherent in PET and CT modalities, for example, the complementarity between
semantics and structure. To address these issues, we propose a Hierarchical
Adaptive Interaction and Weighting Network termed H2ASeg to explore the
intrinsic cross-modal correlations and transfer potential complementary
information. Specifically, we design a Modality-Cooperative Spatial Attention
(MCSA) module that performs intra- and inter-modal interactions globally and
locally. Additionally, a Target-Aware Modality Weighting (TAMW) module is
developed to highlight tumor-related features within multi-modal features,
thereby refining tumor segmentation. By embedding these modules across
different layers, H2ASeg can hierarchically model cross-modal correlations,
enabling a nuanced understanding of both semantic and structural tumor
features. Extensive experiments demonstrate the superiority of H2ASeg,
outperforming state-of-the-art methods on AutoPet-II and Hecktor2022
benchmarks. The code is released at https://github.com/G14nTDo4/H2ASeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DODA: Diffusion for Object-detection Domain Adaptation in Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Xiang, Pieter M. Blok, James Burridge, Haozhou Wang, Wei Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diverse and high-quality content generated by recent generative models
demonstrates the great potential of using synthetic data to train downstream
models. However, in vision, especially in objection detection, related areas
are not fully explored, the synthetic images are merely used to balance the
long tails of existing datasets, and the accuracy of the generated labels is
low, the full potential of generative models has not been exploited. In this
paper, we propose DODA, a data synthesizer that can generate high-quality
object detection data for new domains in agriculture. Specifically, we improve
the controllability of layout-to-image through encoding layout as an image,
thereby improving the quality of labels, and use a visual encoder to provide
visual clues for the diffusion model to decouple visual features from the
diffusion model, and empowering the model the ability to generate data in new
domains. On the Global Wheat Head Detection (GWHD) Dataset, which is the
largest dataset in agriculture and contains diverse domains, using the data
synthesized by DODA improves the performance of the object detector by
12.74-17.76 AP$_{50}$ in the domain that was significantly shifted from the
training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking-Assisted Object Detection with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin, Jia-Fong Yeh, Hung-Ting Su, Winston Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based object detection has recently garnered attention in the computer
vision community due to the exceptional properties of event cameras, such as
high dynamic range and no motion blur. However, feature asynchronism and
sparsity cause invisible objects due to no relative motion to the camera,
posing a significant challenge in the task. Prior works have studied various
memory mechanisms to preserve as many features as possible at the current time,
guided by temporal clues. While these implicit-learned memories retain some
short-term information, they still struggle to preserve long-term features
effectively. In this paper, we consider those invisible objects as
pseudo-occluded objects and aim to reveal their features. Firstly, we introduce
visibility attribute of objects and contribute an auto-labeling algorithm to
append additional visibility labels on an existing event camera dataset.
Secondly, we exploit tracking strategies for pseudo-occluded objects to
maintain their permanence and retain their bounding boxes, even when features
have not been available for a very long time. These strategies can be treated
as an explicit-learned memory guided by the tracking objective to record the
displacements of objects across frames. Lastly, we propose a spatio-temporal
feature aggregation module to enrich the latent features and a consistency loss
to increase the robustness of the overall pipeline. We conduct comprehensive
experiments to verify our method's effectiveness where still objects are
retained but real occluded objects are discarded. The results demonstrate that
(1) the additional visibility labels can assist in supervised training, and (2)
our method outperforms state-of-the-art approaches with a significant
improvement of 7.9% absolute mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Anita De Santi, Jörg Schlötterer, Michael Scheschenja, Joel Wessendorf, Meike Nauta, Vincenzo Positano, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information from neuroimaging examinations (CT, MRI) is increasingly used to
support diagnoses of dementia, e.g., Alzheimer's disease. While current
clinical practice is mainly based on visual inspection and feature engineering,
Deep Learning approaches can be used to automate the analysis and to discover
new image-biomarkers. Part-prototype neural networks (PP-NN) are an alternative
to standard blackbox models, and have shown promising results in general
computer vision. PP-NN's base their reasoning on prototypical image regions
that are learned fully unsupervised, and combined with a simple-to-understand
decision layer. We present PIPNet3D, a PP-NN for volumetric images. We apply
PIPNet3D to the clinical case study of Alzheimer's Disease diagnosis from
structural Magnetic Resonance Imaging (sMRI). We assess the quality of
prototypes under a systematic evaluation framework, propose new metrics to
evaluate brain prototypes and perform an evaluation with domain experts. Our
results show that PIPNet3D is an interpretable, compact model for Alzheimer's
diagnosis with its reasoning well aligned to medical domain knowledge. Notably,
PIPNet3D achieves the same accuracy as its blackbox counterpart; and removing
the remaining clinically irrelevant prototypes from its decision process does
not decrease predictive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of the Principal Component Analysis onto High-Performance
  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and
  Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Martel, R. Lazcano, J. Lopez, D. Madroñal, R. Salvador, S. Lopez, E. Juarez, R. Guerra, C. Sanz, R. Sarmiento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction represents a critical preprocessing step in order to
increase the efficiency and the performance of many hyperspectral imaging
algorithms. However, dimensionality reduction algorithms, such as the Principal
Component Analysis (PCA), suffer from their computationally demanding nature,
becoming advisable for their implementation onto high-performance computer
architectures for applications under strict latency constraints. This work
presents the implementation of the PCA algorithm onto two different
high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and
a Kalray manycore, uncovering a highly valuable set of tips and tricks in order
to take full advantage of the inherent parallelism of these high-performance
computing platforms, and hence, reducing the time that is required to process a
given hyperspectral image. Moreover, the achieved results obtained with
different hyperspectral images have been compared with the ones that were
obtained with a field programmable gate array (FPGA)-based implementation of
the PCA algorithm that has been recently published, providing, for the first
time in the literature, a comprehensive analysis in order to highlight the pros
and cons of each option.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via
  Bayesian Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Ye, Rajgopal Kannan, Viktor Prasanna, Carl Busart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks have demonstrated the vulnerability of Machine Learning
(ML) image classifiers in Synthetic Aperture Radar (SAR) Automatic Target
Recognition (ATR) systems. An adversarial attack can deceive the classifier
into making incorrect predictions by perturbing the input SAR images, for
example, with a few scatterers attached to the on-ground objects. Therefore, it
is critical to develop robust SAR ATR systems that can detect potential
adversarial attacks by leveraging the inherent uncertainty in ML classifiers,
thereby effectively alerting human decision-makers. In this paper, we propose a
novel uncertainty-aware SAR ATR for detecting adversarial attacks.
Specifically, we leverage the capability of Bayesian Neural Networks (BNNs) in
performing image classification with quantified epistemic uncertainty to
measure the confidence for each input SAR image. By evaluating the uncertainty,
our method alerts when the input SAR image is likely to be adversarially
generated. Simultaneously, we also generate visual explanations that reveal the
specific regions in the SAR image where the adversarial scatterers are likely
to to be present, thus aiding human decision-making with hints of evidence of
adversarial attacks. Experiments on the MSTAR dataset demonstrate that our
approach can identify over 80% adversarial SAR images with fewer than 20% false
alarms, and our visual explanations can identify up to over 90% of scatterers
in an adversarial SAR image.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-scale Unified Network for Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) have advanced significantly in visual
representation learning and recognition. However, they face notable challenges
in performance and computational efficiency when dealing with real-world,
multi-scale image inputs. Conventional methods rescale all input images into a
fixed size, wherein a larger fixed size favors performance but rescaling small
size images to a larger size incurs digitization noise and increased
computation cost. In this work, we carry out a comprehensive, layer-wise
investigation of CNN models in response to scale variation, based on Centered
Kernel Alignment (CKA) analysis. The observations reveal lower layers are more
sensitive to input image scale variations than high-level layers. Inspired by
this insight, we propose Multi-scale Unified Network (MUSN) consisting of
multi-scale subnets, a unified network, and scale-invariant constraint. Our
method divides the shallow layers into multi-scale subnets to enable feature
extraction from multi-scale inputs, and the low-level features are unified in
deep layers for extracting high-level semantic features. A scale-invariant
constraint is posed to maintain feature consistency across different scales.
Extensive experiments on ImageNet and other scale-diverse datasets, demonstrate
that MSUN achieves significant improvements in both model performance and
computational efficiency. Particularly, MSUN yields an accuracy increase up to
44.53% and diminishes FLOPs by 7.01-16.13% in multi-scale scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Test-Time Adaptation of Vision-Language Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation with pre-trained vision-language models has attracted
increasing attention for tackling distribution shifts during the test time.
Though prior studies have achieved very promising performance, they involve
intensive computation which is severely unaligned with test-time adaptation. We
design TDA, a training-free dynamic adapter that enables effective and
efficient test-time adaptation with vision-language models. TDA works with a
lightweight key-value cache that maintains a dynamic queue with few-shot pseudo
labels as values and the corresponding test-sample features as keys. Leveraging
the key-value cache, TDA allows adapting to test data gradually via progressive
pseudo label refinement which is super-efficient without incurring any
backpropagation. In addition, we introduce negative pseudo labeling that
alleviates the adverse impact of pseudo label noises by assigning pseudo labels
to certain negative classes when the model is uncertain about its pseudo label
predictions. Extensive experiments over two benchmarks demonstrate TDA's
superior effectiveness and efficiency as compared with the state-of-the-art.
The code has been released in \url{https://kdiaaa.github.io/tda/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. The code has been released in
  \url{https://kdiaaa.github.io/tda/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Non-Exemplar Semi-Supervised Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks perform remarkably well in close-world scenarios.
However, novel classes emerged continually in real applications, making it
necessary to learn incrementally. Class-incremental learning (CIL) aims to
gradually recognize new classes while maintaining the discriminability of old
ones. Existing CIL methods have two limitations: a heavy reliance on preserving
old data for forgetting mitigation and the need for vast labeled data for
knowledge adaptation. To overcome these issues, we propose a non-exemplar
semi-supervised CIL framework with contrastive learning and semi-supervised
incremental prototype classifier (Semi-IPC). On the one hand, contrastive
learning helps the model learn rich representations, easing the trade-off
between learning representations of new classes and forgetting that of old
classes. On the other hand, Semi-IPC learns a prototype for each class with
unsupervised regularization, enabling the model to incrementally learn from
partially labeled new data while maintaining the knowledge of old classes.
Experiments on benchmark datasets demonstrate the strong performance of our
method: without storing any old samples and only using less than 1% of labels,
Semi-IPC outperforms advanced exemplar-based methods. We hope our work offers
new insights for future CIL research. The code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGDM: Static-Guided Dynamic Module Make Stronger Visual Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Xing, Zhenchao Cui, Jing Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spatial attention mechanism has been widely used to improve object
detection performance. However, its operation is currently limited to static
convolutions lacking content-adaptive features. This paper innovatively
approaches from the perspective of dynamic convolution. We propose Razor
Dynamic Convolution (RDConv) to address thetwo flaws in dynamic weight
convolution, making it hard to implement in spatial mechanism: 1) it is
computation-heavy; 2) when generating weights, spatial information is
disregarded. Firstly, by using Razor Operation to generate certain features, we
vastly reduce the parameters of the entire dynamic convolution operation.
Secondly, we added a spatial branch inside RDConv to generate convolutional
kernel parameters with richer spatial information. Embedding dynamic
convolution will also bring the problem of sensitivity to high-frequency noise.
We propose the Static-Guided Dynamic Module (SGDM) to address this limitation.
By using SGDM, we utilize a set of asymmetric static convolution kernel
parameters to guide the construction of dynamic convolution. We introduce the
mechanism of shared weights in static convolution to solve the problem of
dynamic convolution being sensitive to high-frequency noise. Extensive
experiments illustrate that multiple different object detection backbones
equipped with SGDM achieve a highly competitive boost in performance(e.g., +4%
mAP with YOLOv5n on VOC and +1.7% mAP with YOLOv8n on COCO) with negligible
parameter increase(i.e., +0.33M on YOLOv5n and +0.19M on YOLOv8n).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIR-HLoc: Adaptive Image Retrieval for Efficient Visual Localisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changkun Liu, Huajian Huang, Zhengyang Ma, Tristan Braud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art (SOTA) hierarchical localisation pipelines (HLoc) rely on
image retrieval (IR) techniques to establish 2D-3D correspondences by selecting
the $k$ most similar images from a reference image database for a given query
image. Although higher values of $k$ enhance localisation robustness, the
computational cost for feature matching increases linearly with $k$. In this
paper, we observe that queries that are the most similar to images in the
database result in a higher proportion of feature matches and, thus, more
accurate positioning. Thus, a small number of images is sufficient for queries
very similar to images in the reference database. We then propose a novel
approach, AIR-HLoc, which divides query images into different localisation
difficulty levels based on their similarity to the reference image database. We
consider an image with high similarity to the reference image as an easy query
and an image with low similarity as a hard query. Easy queries show a limited
improvement in accuracy when increasing $k$. Conversely, higher values of $k$
significantly improve accuracy for hard queries. Given the limited improvement
in accuracy when increasing $k$ for easy queries and the significant
improvement for hard queries, we adapt the value of $k$ to the query's
difficulty level. Therefore, AIR-HLoc optimizes processing time by adaptively
assigning different values of $k$ based on the similarity between the query and
reference images without losing accuracy. Our extensive experiments on the
Cambridge Landmarks, 7Scenes, and Aachen Day-Night-v1.1 datasets demonstrate
our algorithm's efficacy, reducing 30\%, 26\%, and 11\% in computational
overhead while maintaining SOTA accuracy compared to HLoc with fixed image
retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and
  Bi-Directional Structure Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuming Liu, Dong Zhuo, Zhiheng Feng, Siting Zhu, Chensheng Peng, Zhe Liu, Hesheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information inside visual and LiDAR data is well complementary derived from
the fine-grained texture of images and massive geometric information in point
clouds. However, it remains challenging to explore effective visual-LiDAR
fusion, mainly due to the intrinsic data structure inconsistency between two
modalities: Images are regular and dense, but LiDAR points are unordered and
sparse. To address the problem, we propose a local-to-global fusion network
with bi-directional structure alignment. To obtain locally fused features, we
project points onto image plane as cluster centers and cluster image pixels
around each center. Image pixels are pre-organized as pseudo points for
image-to-point structure alignment. Then, we convert points to pseudo images by
cylindrical projection (point-to-image structure alignment) and perform
adaptive global feature fusion between point features with local fused
features. Our method achieves state-of-the-art performance on KITTI odometry
and FlyingThings3D scene flow datasets compared to both single-modal and
multi-modal methods. Codes will be released later.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the Potential of SAM for Medical Adaptation via Hierarchical
  Decoding <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiheng Cheng, Qingyue Wei, Hongru Zhu, Yan Wang, Liangqiong Qu, Wei Shao, Yuyin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) has garnered significant attention for its
versatile segmentation abilities and intuitive prompt-based interface. However,
its application in medical imaging presents challenges, requiring either
substantial training costs and extensive medical datasets for full model
fine-tuning or high-quality prompts for optimal performance. This paper
introduces H-SAM: a prompt-free adaptation of SAM tailored for efficient
fine-tuning of medical images via a two-stage hierarchical decoding procedure.
In the initial stage, H-SAM employs SAM's original decoder to generate a prior
probabilistic mask, guiding a more intricate decoding process in the second
stage. Specifically, we propose two key designs: 1) A class-balanced,
mask-guided self-attention mechanism addressing the unbalanced label
distribution, enhancing image embedding; 2) A learnable mask cross-attention
mechanism spatially modulating the interplay among different image regions
based on the prior mask. Moreover, the inclusion of a hierarchical pixel
decoder in H-SAM enhances its proficiency in capturing fine-grained and
localized details. This approach enables SAM to effectively integrate learned
medical priors, facilitating enhanced adaptation for medical image segmentation
with limited samples. Our H-SAM demonstrates a 4.78% improvement in average
Dice compared to existing prompt-free SAM variants for multi-organ segmentation
using only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM
even outperforms state-of-the-art semi-supervised models relying on extensive
unlabeled training data across various medical datasets. Our code is available
at https://github.com/Cccccczh404/H-SAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Deraining via <span class="highlight-title">Self-supervised</span> Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He-Hao Liao, Yan-Tsung Peng, Wen-Tao Chu, Ping-Chun Hsieh, Chung-Chi Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of images captured outdoors is often affected by the weather. One
factor that interferes with sight is rain, which can obstruct the view of
observers and computer vision applications that rely on those images. The work
aims to recover rain images by removing rain streaks via Self-supervised
Reinforcement Learning (RL) for image deraining (SRL-Derain). We locate rain
streak pixels from the input rain image via dictionary learning and use
pixel-wise RL agents to take multiple inpainting actions to remove rain
progressively. To our knowledge, this work is the first attempt where
self-supervised RL is applied to image deraining. Experimental results on
several benchmark image-deraining datasets show that the proposed SRL-Derain
performs favorably against state-of-the-art few-shot and self-supervised
deraining and denoising methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branch-Tuning: Balancing Stability and Plasticity for Continual
  <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has emerged as an effective paradigm for
deriving general representations from vast amounts of unlabeled data. However,
as real-world applications continually integrate new content, the high
computational and resource demands of SSL necessitate continual learning rather
than complete retraining. This poses a challenge in striking a balance between
stability and plasticity when adapting to new information. In this paper, we
employ Centered Kernel Alignment for quantitatively analyzing model stability
and plasticity, revealing the critical roles of batch normalization layers for
stability and convolutional layers for plasticity. Motivated by this, we
propose Branch-tuning, an efficient and straightforward method that achieves a
balance between stability and plasticity in continual SSL. Branch-tuning
consists of branch expansion and compression, and can be easily applied to
various SSL methods without the need of modifying the original methods,
retaining old data or models. We validate our method through incremental
experiments on various benchmark datasets, demonstrating its effectiveness and
practical value in real-world scenarios. We hope our work offers new insights
for future continual self-supervised learning research. The code will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Interactive Regional Understanding in Vision-Large Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungbeom Lee, Sanghyuk Chun, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Vision-Language Pre-training (VLP) models have demonstrated
significant advancements. Nevertheless, these models heavily rely on image-text
pairs that capture only coarse and global information of an image, leading to a
limitation in their regional understanding ability. In this work, we introduce
\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,
allowing them to understand user-indicated image regions. To achieve this, we
design a simple yet innovative architecture, requiring no modifications to the
model architecture or objective function. Additionally, we leverage a dataset
that contains a novel source of information, namely Localized Narratives, which
has been overlooked in previous VLP research. Our experiments demonstrate that
our single generalist model not only achieves an interactive dialogue system
but also exhibits superior performance on various zero-shot region
understanding tasks, without compromising its ability for global image
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shifting to Machine Supervision: Annotation-Efficient Semi and
  <span class="highlight-title">Self-Supervised</span> Learning for Automatic Medical Image Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10319v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10319v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in clinical treatment are increasingly constrained by the
limitations of supervised learning techniques, which depend heavily on large
volumes of annotated data. The annotation process is not only costly but also
demands substantial time from clinical specialists. Addressing this issue, we
introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)
pipeline, a novel approach that leverages advancements in self-supervised and
semi-supervised learning. These techniques engage in auxiliary tasks that do
not require labeling, thus simplifying the scaling of machine supervision
compared to fully-supervised methods. Our study benchmarks these techniques on
three distinct medical imaging datasets to evaluate their effectiveness in
classification and segmentation tasks. Notably, we observed that self
supervised learning significantly surpassed the performance of supervised
methods in the classification of all evaluated datasets. Remarkably, the
semi-supervised approach demonstrated superior outcomes in segmentation,
outperforming fully-supervised methods while using 50% fewer labels across all
datasets. In line with our commitment to contributing to the scientific
community, we have made the S4MI code openly accessible, allowing for broader
application and further development of these methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Seventeen pages (incl. references), five figures, and one table.
  (Under Review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Du, Miaojing Shi, Jiankang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting objects in low-light scenarios presents a persistent challenge, as
detectors trained on well-lit data exhibit significant performance degradation
on low-light data due to low visibility. Previous methods mitigate this issue
by exploring image enhancement or object detection techniques with real
low-light image datasets. However, the progress is impeded by the inherent
difficulties about collecting and annotating low-light images. To address this
challenge, we propose to boost low-light object detection with zero-shot
day-night domain adaptation, which aims to generalize a detector from well-lit
scenarios to low-light ones without requiring real low-light data. Revisiting
Retinex theory in the low-level vision, we first design a reflectance
representation learning module to learn Retinex-based illumination invariance
in images with a carefully designed illumination invariance reinforcement
strategy. Next, an interchange-redecomposition-coherence procedure is
introduced to improve over the vanilla Retinex image decomposition process by
performing two sequential image decompositions and introducing a
redecomposition cohering loss. Extensive experiments on ExDark, DARK FACE, and
CODaN datasets show strong low-light generalizability of our method. Our code
is available at https://github.com/ZPDu/DAI-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable machine learning for time-to-event prediction in medicine
  and healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hubert Baniecki, Bartlomiej Sobieski, Patryk Szatkowski, Przemyslaw Bombinski, Przemyslaw Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-to-event prediction, e.g. cancer survival analysis or hospital length of
stay, is a highly prominent machine learning task in medical and healthcare
applications. However, only a few interpretable machine learning methods comply
with its challenges. To facilitate a comprehensive explanatory analysis of
survival models, we formally introduce time-dependent feature effects and
global feature importance explanations. We show how post-hoc interpretation
methods allow for finding biases in AI systems predicting length of stay using
a novel multi-modal dataset created from 1235 X-ray images with textual
radiology reports annotated by human experts. Moreover, we evaluate cancer
survival models beyond predictive performance to include the importance of
multi-omics feature groups based on a large-scale benchmark comprising 11
datasets from The Cancer Genome Atlas (TCGA). Model developers can use the
proposed methods to debug and improve machine learning algorithms, while
physicians can discover disease biomarkers and assess their significance. We
hope the contributed open data and code resources facilitate future work in the
emerging research direction of explainable survival analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extended version of an AIME 2023 paper submitted to Artificial
  Intelligence in Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplified Diffusion Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/checkcrab/SDSB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> co-salient object detection via feature correspondence
  at multiple scales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souradeep Chakraborty, Dimitris Samaras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper introduces a novel two-stage self-supervised approach for detecting
co-occurring salient objects (CoSOD) in image groups without requiring
segmentation annotations. Unlike existing unsupervised methods that rely solely
on patch-level information (e.g. clustering patch descriptors) or on
computation heavy off-the-shelf components for CoSOD, our lightweight model
leverages feature correspondences at both patch and region levels,
significantly improving prediction performance. In the first stage, we train a
self-supervised network that detects co-salient regions by computing local
patch-level feature correspondences across images. We obtain the segmentation
predictions using confidence-based adaptive thresholding. In the next stage, we
refine these intermediate segmentations by eliminating the detected regions
(within each image) whose averaged feature representations are dissimilar to
the foreground feature representation averaged across all the cross-attention
maps (from the previous stage). Extensive experiments on three CoSOD benchmark
datasets show that our self-supervised model outperforms the corresponding
state-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model
has a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably,
our self-supervised model also outperforms several recent fully supervised
CoSOD models on the three test datasets (e.g., on the CoCA dataset, our model
has a 4.6% F-measure gain over a recent supervised CoSOD model).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LION: Implicit Vision <span class="highlight-title">Prompt</span> Tuning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haixin Wang, Jianlong Chang, Xiao Luo, Jinan Sun, Zhouchen Lin, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent competitive performance across a range of vision tasks, vision
Transformers still have an issue of heavy computational costs. Recently, vision
prompt learning has provided an economic solution to this problem without
fine-tuning the whole large-scale models. However, the efficiency of existing
models are still far from satisfactory due to insertion of extensive prompts
blocks and trick prompt designs. In this paper, we propose an efficient vision
model named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep
implicit models with stable memory costs for various complex tasks. In
particular, we merely insect two equilibrium implicit layers in two ends of the
pre-trained main backbone with parameters in the backbone frozen. Moreover, we
prune the parameters in these two layers according to lottery hypothesis. The
performance obtained by our LION are promising on a wide range of datasets. In
particular, our LION reduces up to 11.5% of training parameter numbers while
obtaining higher performance compared with the state-of-the-art baseline VPT,
especially under challenging scenes. Furthermore, we find that our proposed
LION had a good generalization performance, making it an easy way to boost
transfer learning in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024; 9 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incorporating simulated spatial context information improves the
  effectiveness of contrastive learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhen Zhu, James Z. Wang, Wonseuk Lee, Brad Wyble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual learning often occurs in a specific context, where an agent acquires
skills through exploration and tracking of its location in a consistent
environment. The historical spatial context of the agent provides a similarity
signal for self-supervised contrastive learning. We present a unique approach,
termed Environmental Spatial Similarity (ESS), that complements existing
contrastive learning methods. Using images from simulated, photorealistic
environments as an experimental setting, we demonstrate that ESS outperforms
traditional instance discrimination approaches. Moreover, sampling additional
data from the same environment substantially improves accuracy and provides new
augmentations. ESS allows remarkable proficiency in room classification and
spatial prediction tasks, especially in unfamiliar environments. This learning
paradigm has the potential to enable rapid visual learning in agents operating
in new environments with unique visual characteristics. Potentially
transformative applications span from robotics to space exploration. Our proof
of concept demonstrates improved efficiency over methods that rely on
extensive, disconnected datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision <span class="highlight-title">Transformer</span>-Based Deep Learning for Histologic Classification of
  Endometrial Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manu Goyal, Laura J. Tafe, James X. Feng, Kristen E. Muller, Liesbeth Hondelink, Jessica L. Bentz, Saeed Hassanpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endometrial cancer, the fourth most common cancer in females in the United
States, with the lifetime risk for developing this disease is approximately
2.8% in women. Precise histologic evaluation and molecular classification of
endometrial cancer is important for effective patient management and
determining the best treatment modalities. This study introduces EndoNet, which
uses convolutional neural networks for extracting histologic features and a
vision transformer for aggregating these features and classifying slides based
on their visual characteristics into high- and low- grade. The model was
trained on 929 digitized hematoxylin and eosin-stained whole-slide images of
endometrial cancer from hysterectomy cases at Dartmouth-Health. It classifies
these slides into low-grade (Endometroid Grades 1 and 2) and high-grade
(endometroid carcinoma FIGO grade 3, uterine serous carcinoma, carcinosarcoma)
categories. EndoNet was evaluated on an internal test set of 110 patients and
an external test set of 100 patients from the public TCGA database. The model
achieved a weighted average F1-score of 0.91 (95% CI: 0.86-0.95) and an AUC of
0.95 (95% CI: 0.89-0.99) on the internal test, and 0.86 (95% CI: 0.80-0.94) for
F1-score and 0.86 (95% CI: 0.75-0.93) for AUC on the external test. Pending
further validation, EndoNet has the potential to support pathologists without
the need of manual annotations in classifying the grades of gynecologic
pathology tumors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 Tables and 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Construction of Time-Space Diagrams for Traffic Analysis Using
  Street-View Video Sequence <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanay Rastogi, Mårten Björkman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-space diagrams are essential tools for analyzing traffic patterns and
optimizing transportation infrastructure and traffic management strategies.
Traditional data collection methods for these diagrams have limitations in
terms of temporal and spatial coverage. Recent advancements in camera
technology have overcome these limitations and provided extensive urban data.
In this study, we propose an innovative approach to constructing time-space
diagrams by utilizing street-view video sequences captured by cameras mounted
on moving vehicles. Using the state-of-the-art YOLOv5, StrongSORT, and
photogrammetry techniques for distance calculation, we can infer vehicle
trajectories from the video data and generate time-space diagrams. To evaluate
the effectiveness of our proposed method, we utilized datasets from the KITTI
computer vision benchmark suite. The evaluation results demonstrate that our
approach can generate trajectories from video data, although there are some
errors that can be mitigated by improving the performance of the detector,
tracker, and distance calculation components. In conclusion, the utilization of
street-view video sequences captured by cameras mounted on moving vehicles,
combined with state-of-the-art computer vision techniques, has immense
potential for constructing comprehensive time-space diagrams. These diagrams
offer valuable insights into traffic patterns and contribute to the design of
transportation infrastructure and traffic management strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is published in 2023 IEEE 26th International Conference on
  Intelligent Transportation Systems (ITSC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using
  Neural Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Herau, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In rapidly-evolving domains such as autonomous driving, the use of multiple
sensors with different modalities is crucial to ensure high operational
precision and stability. To correctly exploit the provided information by each
sensor in a single common frame, it is essential for these sensors to be
accurately calibrated. In this paper, we leverage the ability of Neural
Radiance Fields (NeRF) to represent different sensors modalities in a common
volumetric representation to achieve robust and accurate spatio-temporal sensor
calibration. By designing a partitioning approach based on the visible part of
the scene for each sensor, we formulate the calibration problem using only the
overlapping areas. This strategy results in a more robust and accurate
calibration that is less prone to failure. We demonstrate that our approach
works on outdoor urban scenes by validating it on multiple established driving
datasets. Results show that our method is able to get better accuracy and
robustness compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024. Project page: https://qherau.github.io/SOAC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point, Segment and Count: A Generalized Framework for Object Counting <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12386v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12386v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizhong Huang, Mingliang Dai, Yi Zhang, Junping Zhang, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-agnostic object counting aims to count all objects in an image with
respect to example boxes or class names, \emph{a.k.a} few-shot and zero-shot
counting. In this paper, we propose a generalized framework for both few-shot
and zero-shot object counting based on detection. Our framework combines the
superior advantages of two foundation models without compromising their
zero-shot capability: (\textbf{i}) SAM to segment all possible objects as mask
proposals, and (\textbf{ii}) CLIP to classify proposals to obtain accurate
object counts. However, this strategy meets the obstacles of efficiency
overhead and the small crowded objects that cannot be localized and
distinguished. To address these issues, our framework, termed PseCo, follows
three steps: point, segment, and count. Specifically, we first propose a
class-agnostic object localization to provide accurate but least point prompts
for SAM, which consequently not only reduces computation costs but also avoids
missing small objects. Furthermore, we propose a generalized object
classification that leverages CLIP image/text embeddings as the classifier,
following a hierarchical knowledge distillation to obtain discriminative
classifications among hierarchical mask proposals. Extensive experimental
results on FSC-147, COCO, and LVIS demonstrate that PseCo achieves
state-of-the-art performance in both few-shot/zero-shot object
counting/detection. Code: https://github.com/Hzzone/PseCo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. Camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech
  Gesture Generation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17532v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17532v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating vivid and emotional 3D co-speech gestures is crucial for virtual
avatar animation in human-machine interaction applications. While the existing
methods enable generating the gestures to follow a single emotion label, they
overlook that long gesture sequence modeling with emotion transition is more
practical in real scenes. In addition, the lack of large-scale available
datasets with emotional transition speech and corresponding 3D human gestures
also limits the addressing of this task. To fulfill this goal, we first
incorporate the ChatGPT-4 and an audio inpainting approach to construct the
high-fidelity emotion transition human speeches. Considering obtaining the
realistic 3D pose annotations corresponding to the dynamically inpainted
emotion transition audio is extremely difficult, we propose a novel weakly
supervised training strategy to encourage authority gesture transitions.
Specifically, to enhance the coordination of transition gestures w.r.t
different emotional ones, we model the temporal association representation
between two different emotional gesture sequences as style guidance and infuse
it into the transition generation. We further devise an emotion mixture
mechanism that provides weak supervision based on a learnable mixed emotion
label for transition gestures. Last, we present a keyframe sampler to supply
effective initial posture cues in long sequences, enabling us to generate
diverse gestures. Extensive experiments demonstrate that our method outperforms
the state-of-the-art models constructed by adapting single emotion-conditioned
counterparts on our newly defined emotion transition task and datasets. Our
code and dataset will be released on the project page:
https://xingqunqi-lab.github.io/Emo-Transition-Gesture/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning by Erasing: Conditional Entropy based Transferable
  Out-Of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.11041v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.11041v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Xing, Zhiyong Feng, Yong Su, Changjae Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is essential to handle the distribution
shifts between training and test scenarios. For a new in-distribution (ID)
dataset, existing methods require retraining to capture the dataset-specific
feature representation or data distribution. In this paper, we propose a deep
generative models (DGM) based transferable OOD detection method, which is
unnecessary to retrain on a new ID dataset. We design an image erasing strategy
to equip exclusive conditional entropy distribution for each ID dataset, which
determines the discrepancy of DGM's posteriori ucertainty distribution on
different ID datasets. Owing to the powerful representation capacity of
convolutional neural networks, the proposed model trained on complex dataset
can capture the above discrepancy between ID datasets without retraining and
thus achieve transferable OOD detection. We validate the proposed method on
five datasets and verity that ours achieves comparable performance to the
state-of-the-art group based OOD detection methods that need to be retrained to
deploy on new ID datasets. Our code is available at
https://github.com/oOHCIOo/CETOOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update new experimental results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Structure-Aware Image Filterings for Semi-supervised Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Gu, Zhichao Sun, Tian Chen, Xin Xiao, Yepeng Liu, Yongchao Xu, Laurent Najman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised image segmentation has attracted great attention recently.
The key is how to leverage unlabeled images in the training process. Most
methods maintain consistent predictions of the unlabeled images under
variations (e.g., adding noise/perturbations, or creating alternative versions)
in the image and/or model level. In most image-level variation, medical images
often have prior structure information, which has not been well explored. In
this paper, we propose novel dual structure-aware image filterings (DSAIF) as
the image-level variations for semi-supervised medical image segmentation.
Motivated by connected filtering that simplifies image via filtering in
structure-aware tree-based image representation, we resort to the dual contrast
invariant Max-tree and Min-tree representation. Specifically, we propose a
novel connected filtering that removes topologically equivalent nodes (i.e.
connected components) having no siblings in the Max/Min-tree. This results in
two filtered images preserving topologically critical structure. Applying the
proposed DSAIF to mutually supervised networks decreases the consensus of their
erroneous predictions on unlabeled images. This helps to alleviate the
confirmation bias issue of overfitting to noisy pseudo labels of unlabeled
images, and thus effectively improves the segmentation performance. Extensive
experimental results on three benchmark datasets demonstrate that the proposed
method significantly/consistently outperforms some state-of-the-art methods.
The source codes will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decomposing Disease Descriptions for Enhanced Pathology Detection: A
  Multi-Aspect Vision-Language <span class="highlight-title">Pre-train</span>ing Framework <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07636v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07636v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vu Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu, Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, Johan W. Verjans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical vision language pre-training (VLP) has emerged as a frontier of
research, enabling zero-shot pathological recognition by comparing the query
image with the textual descriptions for each disease. Due to the complex
semantics of biomedical texts, current methods struggle to align medical images
with key pathological findings in unstructured reports. This leads to the
misalignment with the target disease's textual representation. In this paper,
we introduce a novel VLP framework designed to dissect disease descriptions
into their fundamental aspects, leveraging prior knowledge about the visual
manifestations of pathologies. This is achieved by consulting a large language
model and medical experts. Integrating a Transformer module, our approach
aligns an input image with the diverse elements of a disease, generating
aspect-centric image representations. By consolidating the matches from each
aspect, we improve the compatibility between an image and its associated
disease. Additionally, capitalizing on the aspect-oriented representations, we
present a dual-head Transformer tailored to process known and unknown diseases,
optimizing the comprehensive detection efficacy. Conducting experiments on
seven downstream datasets, ours improves the accuracy of recent methods by up
to 8.56% and 17.0% for seen and unseen categories, respectively. Our code is
released at https://github.com/HieuPhan33/MAVL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR2024. Pre-print before final camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shapley Values-Powered Framework for Fair Reward Split in Content
  Produced by GenAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Glinsky, Alexey Sokolsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is evident that, currently, generative models are surpassed in quality by
human professionals. However, with the advancements in Artificial Intelligence,
this gap will narrow, leading to scenarios where individuals who have dedicated
years of their lives to mastering a skill become obsolete due to their high
costs, which are inherently linked to the time they require to complete a task
-- a task that AI could accomplish in minutes or seconds. To avoid future
social upheavals, we must, even now, contemplate how to fairly assess the
contributions of such individuals in training generative models and how to
compensate them for the reduction or complete loss of their incomes. In this
work, we propose a method to structure collaboration between model developers
and data providers. To achieve this, we employ Shapley Values to quantify the
contribution of artist(s) in an image generated by the Stable Diffusion-v1.5
model and to equitably allocate the reward among them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maomao Li, Ge Yuan, Cairong Wang, Zhian Liu, Yong Zhang, Yongwei Nie, Jue Wang, Dong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to face swapping from the perspective of
fine-grained facial editing, dubbed "editing for swapping" (E4S). The
traditional face swapping methods rely on global feature extraction and fail to
preserve the detailed source identity. In contrast, we propose a Regional GAN
Inversion (RGI) method, which allows the explicit disentanglement of shape and
texture. Specifically, our E4S performs face swapping in the latent space of a
pretrained StyleGAN, where a multi-scale mask-guided encoder is applied to
project the texture of each facial component into regional style codes and a
mask-guided injection module manipulating feature maps with the style codes.
Based on this disentanglement, face swapping can be simplified as style and
mask swapping. Besides, due to the large lighting condition gap, transferring
the source skin into the target image may lead to disharmony lighting. We
propose a re-coloring network to make the swapped face maintain the target
lighting condition while preserving the source skin. Further, to deal with the
potential mismatch areas during mask exchange, we design a face inpainting
module to refine the face shape. The extensive comparisons with
state-of-the-art methods demonstrate that our E4S outperforms existing methods
in preserving texture, shape, and lighting. Our implementation is available at
https://github.com/e4s2024/E4S2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://e4s2024.github.io/ ;. arXiv admin note: text
  overlap with arXiv:2211.14068</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViDA: Homeostatic Visual Domain Adapter for Continual Test Time
  Adaptation <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Liu, Senqiao Yang, Peidong Jia, Renrui Zhang, Ming Lu, Yandong Guo, Wei Xue, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since real-world machine systems are running in non-stationary environments,
Continual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained
model to continually changing target domains. Recently, existing methods mainly
focus on model-based adaptation, which aims to leverage a self-training manner
to extract the target domain knowledge. However, pseudo labels can be noisy and
the updated model parameters are unreliable under dynamic data distributions,
leading to error accumulation and catastrophic forgetting in the continual
adaptation process. To tackle these challenges and maintain the model
plasticity, we design a Visual Domain Adapter (ViDA) for CTTA, explicitly
handling both domain-specific and domain-shared knowledge. Specifically, we
first comprehensively explore the different domain representations of the
adapters with trainable high-rank or low-rank embedding spaces. Then we inject
ViDAs into the pre-trained model, which leverages high-rank and low-rank
features to adapt the current domain distribution and maintain the continual
domain-shared knowledge, respectively. To exploit the low-rank and high-rank
ViDAs more effectively, we further propose a Homeostatic Knowledge Allotment
(HKA) strategy, which adaptively combines different knowledge from each ViDA.
Extensive experiments conducted on four widely used benchmarks demonstrate that
our proposed method achieves state-of-the-art performance in both
classification and segmentation CTTA tasks. Note that, our method can be
regarded as a novel transfer paradigm for large-scale models, delivering
promising results in adaptation to continually changing distributions. Project
page: https://sites.google.com/view/iclr2024-vida/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visually Guided Generative Text-Layout <span class="highlight-title">Pre-train</span>ing for Document
  Intelligence <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intraoperative 2D/3D Image Registration via Differentiable X-ray
  Rendering <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Gopalakrishnan, Neel Dey, Polina Golland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical decisions are informed by aligning rapid portable 2D intraoperative
images (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g.,
CT). 2D/3D image registration often fails in practice: conventional
optimization methods are prohibitively slow and susceptible to local minima,
while neural networks trained on small datasets fail on new patients or require
impractical landmark supervision. We present DiffPose, a self-supervised
approach that leverages patient-specific simulation and differentiable
physics-based rendering to achieve accurate 2D/3D registration without relying
on manually labeled data. Preoperatively, a CNN is trained to regress the pose
of a randomly oriented synthetic X-ray rendered from the preoperative CT. The
CNN then initializes rapid intraoperative test-time optimization that uses the
differentiable X-ray renderer to refine the solution. Our work further proposes
several geometrically principled methods for sampling camera poses from
$\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving
registration in the tangent space $\mathfrak{se}(3)$ with geodesic and
multiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy
across surgical datasets at intraoperative speeds, improving upon existing
unsupervised methods by an order of magnitude and even outperforming supervised
baselines. Our code is available at https://github.com/eigenvivek/DiffPose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Fields for Interactive Visualization of Statistical Dependencies
  in 3D Simulation Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02203v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02203v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Farokhmanesh, Kevin Höhlein, Christoph Neuhauser, Tobias Necker, Martin Weissmann, Takemasa Miyoshi, Rüdiger Westermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first neural network that has learned to compactly represent
and can efficiently reconstruct the statistical dependencies between the values
of physical variables at different spatial locations in large 3D simulation
ensembles. Going beyond linear dependencies, we consider mutual information as
a measure of non-linear dependence. We demonstrate learning and reconstruction
with a large weather forecast ensemble comprising 1000 members, each storing
multiple physical variables at a 250 x 352 x 20 simulation grid. By
circumventing compute-intensive statistical estimators at runtime, we
demonstrate significantly reduced memory and computation requirements for
reconstructing the major dependence structures. This enables embedding the
estimator into a GPU-accelerated direct volume renderer and interactively
visualizing all mutual dependencies for a selected domain point.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAR-Net: Multi-scale Direction-aware SAR Network via Global Information
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxiang Cao, Jie Lei, Weiying Xie, Jiaqing Zhang, Daixun Li, Yunsong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has driven significant progress in object detection using
Synthetic Aperture Radar (SAR) imagery. Existing methods, while achieving
promising results, often struggle to effectively integrate local and global
information, particularly direction-aware features. This paper proposes
SAR-Net, a novel framework specifically designed for global fusion of
direction-aware information in SAR object detection. SAR-Net leverages two key
innovations: the Unity Compensation Mechanism (UCM) and the Direction-aware
Attention Module (DAM). UCM facilitates the establishment of complementary
relationships among features across different scales, enabling efficient global
information fusion. Among them, Multi-scale Alignment Module (MAM) and distinct
Multi-level Fusion Module (MFM) enhance feature integration by capturing both
texture detail and semantic information. Then, Multi-feature Embedding Module
(MEM) feeds back global features into the primary branches, further improving
information transmission. Additionally, DAM, through bidirectional attention
polymerization, captures direction-aware information, effectively eliminating
background interference. Extensive experiments demonstrate the effectiveness of
SAR-Net, achieving state-of-the-art results on aircraft (SAR-AIRcraft-1.0) and
ship datasets (SSDD, HRSID), confirming its generalization capability and
robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient <span class="highlight-title">Transformer</span>-Based 3D Human Pose
  Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Object Coherence in Layout-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10522v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10522v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel's generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BEVUDA: Multi-geometric Space Alignments for Domain Adaptive BEV 3D
  Object Detection <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.17126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.17126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Liu, Rongyu Zhang, Xiaoqi Li, Xiaowei Chi, Zehui Chen, Ming Lu, Yandong Guo, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-centric bird-eye-view (BEV) perception has shown promising potential
in autonomous driving. Recent works mainly focus on improving efficiency or
accuracy but neglect the challenges when facing environment changing, resulting
in severe degradation of transfer performance. For BEV perception, we figure
out the significant domain gaps existing in typical real-world cross-domain
scenarios and comprehensively solve the Domain Adaption (DA) problem for
multi-view 3D object detection. Since BEV perception approaches are complicated
and contain several components, the domain shift accumulation on multiple
geometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In
this paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework
to ease the domain shift accumulation, which consists of a Depth-Aware Teacher
(DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines
target lidar and reliable depth prediction to construct depth-aware
information, extracting target domain-specific knowledge in Voxel and BEV
feature spaces. It then transfers the sufficient domain knowledge of multiple
spaces to the student model. In order to jointly alleviate the domain shift,
GAS projects multi-geometric space features to a shared geometric embedding
space and decreases data distribution distance between two domains. To verify
the effectiveness of our method, we conduct BEV 3D object detection experiments
on three cross-domain scenarios and achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D
  Features <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wimmer, Peter Wonka, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the immense growth of dataset sizes and computing resources in recent
years, so-called foundation models have become popular in NLP and vision tasks.
In this work, we propose to explore foundation models for the task of keypoint
detection on 3D shapes. A unique characteristic of keypoint detection is that
it requires semantic and geometric awareness while demanding high localization
accuracy. To address this problem, we propose, first, to back-project features
from large pre-trained 2D vision models onto 3D shapes and employ them for this
task. We show that we obtain robust 3D features that contain rich semantic
information and analyze multiple candidate features stemming from different 2D
foundation models. Second, we employ a keypoint candidate optimization module
which aims to match the average observed distribution of keypoints on the shape
and is guided by the back-projected features. The resulting approach achieves a
new state of the art for few-shot keypoint detection on the KeyPointNet
dataset, almost doubling the performance of the previous best methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024, Project page:
  https://wimmerth.github.io/back-to-3d.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Dynamic 3D Object Generation from a Single-view Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Pan, Zeyu Yang, Xiatian Zhu, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating dynamic 3D object from a single-view video is challenging due to
the lack of 4D labeled data. Extending image-to-3D pipelines by transferring
off-the-shelf image generation models such as score distillation sampling,
existing methods tend to be slow and expensive to scale due to the need for
back-propagating the information-limited supervision signals through a large
pretrained model. To address this, we propose an efficient video-to-4D object
generation framework called Efficient4D. It generates high-quality
spacetime-consistent images under different camera views, and then uses them as
labeled data to directly train a novel 4D Gaussian splatting model with
explicit point cloud geometry, enabling real-time rendering under continuous
camera trajectories. Extensive experiments on synthetic and real videos show
that Efficient4D offers a remarkable 20-fold increase in speed when compared to
prior art alternatives while preserving the quality of novel view synthesis.
For example, Efficient4D takes only 6 mins to model a dynamic object, vs 120
mins by Consistent4D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Feng, Mohammadhossein Bahari, Kaouther Messaoud Ben Amor, Éloi Zablocki, Matthieu Cord, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle trajectory prediction has increasingly relied on data-driven
solutions, but their ability to scale to different data domains and the impact
of larger dataset sizes on their generalization remain under-explored. While
these questions can be studied by employing multiple datasets, it is
challenging due to several discrepancies, e.g., in data formats, map
resolution, and semantic annotation types. To address these challenges, we
introduce UniTraj, a comprehensive framework that unifies various datasets,
models, and evaluation criteria, presenting new opportunities for the vehicle
trajectory prediction field. In particular, using UniTraj, we conduct extensive
experiments and find that model performance significantly drops when
transferred to other datasets. However, enlarging data size and diversity can
substantially improve performance, leading to a new state-of-the-art result for
the nuScenes dataset. We provide insights into dataset characteristics to
explain these findings. The code can be found here:
https://github.com/vita-epfl/UniTraj
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary
  semantic segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monika Wysoczańska, Oriane Siméoni, Michaël Ramamonjisoa, Andrei Bursuc, Tomasz Trzciński, Patrick Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popular CLIP model displays impressive zero-shot capabilities thanks to
its seamless interaction with arbitrary text prompts. However, its lack of
spatial awareness makes it unsuitable for dense computer vision tasks, e.g.,
semantic segmentation, without an additional fine-tuning step that often uses
annotations and can potentially suppress its original open-vocabulary
properties. Meanwhile, self-supervised representation methods have demonstrated
good localization properties without human-made annotations nor explicit
supervision. In this work, we take the best of both worlds and propose an
open-vocabulary semantic segmentation method, which does not require any
annotations. We propose to locally improve dense MaskCLIP features, which are
computed with a simple modification of CLIP's last pooling layer, by
integrating localization priors extracted from self-supervised features. By
doing so, we greatly improve the performance of MaskCLIP and produce smooth
outputs. Moreover, we show that the used self-supervised feature properties can
directly be learnt from CLIP features. Our method CLIP-DINOiser needs only a
single forward pass of CLIP and two light convolutional layers at inference, no
extra supervision nor extra memory and reaches state-of-the-art results on
challenging and fine-grained benchmarks such as COCO, Pascal Context,
Cityscapes and ADE20k. The code to reproduce our results is available at
https://github.com/wysoczanska/clip_dinoiser.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual
  Test-Time Adaptation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Liu, Ran Xu, Senqiao Yang, Renrui Zhang, Qizhe Zhang, Zehui Chen, Yandong Guo, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Test-Time Adaptation (CTTA) is proposed to migrate a source
pre-trained model to continually changing target distributions, addressing
real-world dynamism. Existing CTTA methods mainly rely on entropy minimization
or teacher-student pseudo-labeling schemes for knowledge extraction in
unlabeled target domains. However, dynamic data distributions cause
miscalibrated predictions and noisy pseudo-labels in existing self-supervised
learning methods, hindering the effective mitigation of error accumulation and
catastrophic forgetting problems during the continual adaptation process. To
tackle these issues, we propose a continual self-supervised method, Adaptive
Distribution Masked Autoencoders (ADMA), which enhances the extraction of
target domain knowledge while mitigating the accumulation of distribution
shifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism
to adaptively sample masked positions, followed by establishing consistency
constraints between the masked target samples and the original target samples.
Additionally, for masked tokens, we utilize an efficient decoder to reconstruct
a hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients),
leveraging its invariant properties to boost task-relevant representations.
Through conducting extensive experiments on four widely recognized benchmarks,
our proposed method attains state-of-the-art performance in both classification
and segmentation CTTA tasks. Our project page:
https://sites.google.com/view/continual-mae/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel
  Segmentation via Two-Phase Training Angiography-to-Venography Translation <span class="chip">BMVC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla, Ferran Prados, Ninon Burgos, Maria A. Zuluaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a semi-supervised domain adaptation framework for brain vessel
segmentation from different image modalities. Existing state-of-the-art methods
focus on a single modality, despite the wide range of available cerebrovascular
imaging techniques. This can lead to significant distribution shifts that
negatively impact the generalization across modalities. By relying on annotated
angiographies and a limited number of annotated venographies, our framework
accomplishes image-to-image translation and semantic segmentation, leveraging a
disentangled and semantically rich latent space to represent heterogeneous data
and perform image-level adaptation from source to target domains. Moreover, we
reduce the typical complexity of cycle-based architectures and minimize the use
of adversarial training, which allows us to build an efficient and intuitive
model with stable training. We evaluate our method on magnetic resonance
angiographies and venographies. While achieving state-of-the-art performance in
the source domain, our method attains a Dice score coefficient in the target
domain that is only 8.9% lower, highlighting its promising potential for robust
cerebrovascular image segmentation across different modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 34th British Machine Vision Conference (BMVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Fan Zhang, Weichen Yu, Qingsong Wen, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realms of computer vision and natural language processing, Large
Vision-Language Models (LVLMs) have become indispensable tools, proficient in
generating textual descriptions based on visual inputs. Despite their
advancements, our investigation reveals a noteworthy bias in the generated
content, where the output is primarily influenced by the underlying Large
Language Models (LLMs) prior rather than the input image. Our empirical
experiments underscore the persistence of this bias, as LVLMs often provide
confident answers even in the absence of relevant images or given incongruent
visual input. To rectify these biases and redirect the model's focus toward
vision information, we introduce two simple, training-free strategies. Firstly,
for tasks such as classification or multi-choice question-answering (QA), we
propose a ``calibration'' step through affine transformation to adjust the
output distribution. This ``Post-Hoc debias'' approach ensures uniform scores
for each answer when the image is absent, serving as an effective
regularization technique to alleviate the influence of LLM priors. For more
intricate open-ended generation tasks, we extend this method to ``Debias
sampling'', drawing inspirations from contrastive decoding methods.
Furthermore, our investigation sheds light on the instability of LVLMs across
various decoding configurations. Through systematic exploration of different
settings, we significantly enhance performance, surpassing reported results and
raising concerns about the fairness of existing evaluations. Comprehensive
experiments substantiate the effectiveness of our proposed strategies in
mitigating biases. These strategies not only prove beneficial in minimizing
hallucinations but also contribute to the generation of more helpful and
precise illustrations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SIGNeRF: Scene Integrated Generation for Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Niklas Dihlmann, Andreas Engelhardt, Hendrik Lensch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in image diffusion models have recently led to notable improvements
in the generation of high-quality images. In combination with Neural Radiance
Fields (NeRFs), they enabled new opportunities in 3D generation. However, most
generative 3D approaches are object-centric and applying them to editing
existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel
approach for fast and controllable NeRF scene editing and scene-integrated
object generation. A new generative update strategy ensures 3D consistency
across the edited images, without requiring iterative optimization. We find
that depth-conditioned diffusion models inherently possess the capability to
generate 3D consistent views by requesting a grid of images instead of single
views. Based on these insights, we introduce a multi-view reference sheet of
modified images. Our method updates an image collection consistently based on
the reference sheet and refines the original NeRF with the newly generated
image set in one go. By exploiting the depth conditioning mechanism of the
image diffusion model, we gain fine control over the spatial location of the
edit and enforce shape guidance by a selected region or an external mesh.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://signerf.jdihlmann.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalStyleFool: Regional Video Style Transfer Attack Using Segment
  Anything Model <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Cao, Jinghao Li, Xi Xiao, Derui Wang, Minhui Xue, Hao Ge, Wei Liu, Guangwu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work has shown that well-crafted adversarial perturbations can
threaten the security of video recognition systems. Attackers can invade such
models with a low query budget when the perturbations are semantic-invariant,
such as StyleFool. Despite the query efficiency, the naturalness of the minutia
areas still requires amelioration, since StyleFool leverages style transfer to
all pixels in each frame. To close the gap, we propose LocalStyleFool, an
improved black-box video adversarial attack that superimposes regional
style-transfer-based perturbations on videos. Benefiting from the popularity
and scalably usability of Segment Anything Model (SAM), we first extract
different regions according to semantic information and then track them through
the video stream to maintain the temporal consistency. Then, we add
style-transfer-based perturbations to several regions selected based on the
associative criterion of transfer-based gradient information and regional area.
Perturbation fine adjustment is followed to make stylized videos adversarial.
We demonstrate that LocalStyleFool can improve both intra-frame and inter-frame
naturalness through a human-assessed survey, while maintaining competitive
fooling rate and query efficiency. Successful experiments on the
high-resolution dataset also showcase that scrupulous segmentation of SAM helps
to improve the scalability of adversarial attacks under high-resolution data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2024 IEEE Security and Privacy Workshops (SPW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TULIP: <span class="highlight-title">Transformer</span> for Upsampling of LiDAR Point Cloud <span class="chip">CVPR20224</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06733v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06733v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Yang, Patrick Pfreundschuh, Roland Siegwart, Marco Hutter, Peyman Moghadam, Vaishakh Patil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR Upsampling is a challenging task for the perception systems of robots
and autonomous vehicles, due to the sparse and irregular structure of
large-scale scene contexts. Recent works propose to solve this problem by
converting LiDAR data from 3D Euclidean space into an image super-resolution
problem in 2D image space. Although their methods can generate high-resolution
range images with fine-grained details, the resulting 3D point clouds often
blur out details and predict invalid points. In this paper, we propose TULIP, a
new method to reconstruct high-resolution LiDAR point clouds from
low-resolution LiDAR input. We also follow a range image-based approach but
specifically modify the patch and window geometries of a Swin-Transformer-based
network to better fit the characteristics of range images. We conducted several
experiments on three public real-world and simulated datasets. TULIP
outperforms state-of-the-art methods in all relevant metrics and generates
robust and more realistic point clouds than prior works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was accepted by CVPR20224</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxin Xu, Zezheng Zhao, Yuxin Cao, Chunyu Chen, Hao Ge, Ziyao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D face reconstruction plays a crucial role in avatar generation,
with significant demand in web-related applications such as generating virtual
financial advisors in FinTech. Current reconstruction methods predominantly
rely on deep learning techniques and employ 2D self-supervision as a means to
guide model learning. However, these methods encounter challenges in capturing
the comprehensive 3D structural information of the face due to the utilization
of 2D images for model training purposes. To overcome this limitation and
enhance the reconstruction of 3D structural features, we propose an innovative
approach that integrates existing 2D features with 3D features to guide the
model learning process. Specifically, we introduce the 3D-ID Loss, which
leverages the high-dimensional structure features extracted from a
Spectral-Based Graph Convolution Encoder applied to the facial mesh. This
approach surpasses the sole reliance on the 3D information provided by the
facial mesh vertices coordinates. Our model is trained using 2D-3D data pairs
from a combination of datasets and achieves state-of-the-art performance on the
NoW benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures. Accepted to WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AEROBLADE: Training-Free Detection of Latent Diffusion Images Using
  Autoencoder Reconstruction Error <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Ricker, Denis Lukovnikov, Asja Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent text-to-image models, anyone can generate deceptively realistic
images with arbitrary contents, fueling the growing threat of visual
disinformation. A key enabler for generating high-resolution images with low
computational cost has been the development of latent diffusion models (LDMs).
In contrast to conventional diffusion models, LDMs perform the denoising
process in the low-dimensional latent space of a pre-trained autoencoder (AE)
instead of the high-dimensional image space. Despite their relevance, the
forensic analysis of LDMs is still in its infancy. In this work we propose
AEROBLADE, a novel detection method which exploits an inherent component of
LDMs: the AE used to transform images between image and latent space. We find
that generated images can be more accurately reconstructed by the AE than real
images, allowing for a simple detection approach based on the reconstruction
error. Most importantly, our method is easy to implement and does not require
any training, yet nearly matches the performance of detectors that rely on
extensive training. We empirically demonstrate that AEROBLADE is effective
against state-of-the-art LDMs, including Stable Diffusion and Midjourney.
Beyond detection, our approach allows for the qualitative analysis of images,
which can be leveraged for identifying inpainted regions. We release our code
and data at https://github.com/jonasricker/aeroblade .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A citizen science toolkit to collect human perceptions of urban
  environments using open street view images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Danish, SM Labib, Britta Ricker, Marco Helbich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Street View-level Imagery (SVI) is a valuable data source for studies (e.g.,
environmental assessments, green space identification or land cover
classification). While commercial SVI is available, such providers commonly
restrict copying or reuse in ways necessary for research. Open SVI datasets are
readily available from less restrictive sources, such as Mapillary, but due to
the heterogeneity of the images, these require substantial preprocessing,
filtering, and careful quality checks. We present an efficient method for
automated downloading, processing, cropping, and filtering open SVI, to be used
in a survey of human perceptions of the streets portrayed in these images. We
demonstrate our open-source reusable SVI preparation and smartphone-friendly
perception-survey software with Amsterdam (Netherlands) as the case study.
Using a citizen science approach, we collected from 331 people 22,637 ratings
about their perceptions for various criteria. We have published our software in
a public repository for future re-use and reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoMo-Bench: a multi-modal, multi-scale and multi-task Forest Monitoring
  Benchmark for remote sensing foundation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Ioannis Bountos, Arthur Ouaknine, David Rolnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forests are an essential part of Earth's ecosystems and natural systems, as
well as providing services on which humanity depends, yet they are rapidly
changing as a result of land use decisions and climate change. Understanding
and mitigating negative effects requires parsing data on forests at global
scale from a broad array of sensory modalities, and recently many such problems
have been approached using machine learning algorithms for remote sensing. To
date, forest-monitoring problems have largely been addressed in isolation.
Inspired by the rise of foundation models for computer vision and remote
sensing, we here present the first unified Forest Monitoring Benchmark
(FoMo-Bench). FoMo-Bench consists of 15 diverse datasets encompassing
satellite, aerial, and inventory data, covering a variety of geographical
regions, and including multispectral, red-green-blue, synthetic aperture radar
(SAR) and LiDAR data with various temporal, spatial and spectral resolutions.
FoMo-Bench includes multiple types of forest-monitoring tasks, spanning
classification, segmentation, and object detection. To further enhance the
diversity of tasks and geographies represented in FoMo-Bench, we introduce a
novel global dataset, TalloS, combining satellite imagery with ground-based
annotations for tree species classification, encompassing 1,000+ categories
across multiple hierarchical taxonomic levels (species, genus, family).
Finally, we propose FoMo-Net, a baseline foundation model with the capacity to
process any combination of commonly used spectral bands in remote sensing,
across diverse ground sampling distances and geographical locations worldwide.
This work aims to inspire research collaborations between machine learning and
forest biology researchers in exploring scalable multi-modal and multi-task
models for forest monitoring. All code and data will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for AI-Generated Content: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Artificial Intelligence Generated Content (AIGC) has been
facilitated by advancements in model algorithms, the increasing scale of
foundation models, and the availability of ample high-quality datasets. While
AIGC has achieved remarkable performance, it still faces several challenges,
such as the difficulty of maintaining up-to-date and long-tail knowledge, the
risk of data leakage, and the high costs associated with training and
inference. Retrieval-Augmented Generation(RAG) has recently emerged as a
paradigm to address such challenges. In particular, RAG introduces the
information retrieval process, which enhances the generation process by
retrieving relevant objects from available data stores, leading to higher
accuracy and better robustness. In this paper, we comprehensively review
existing efforts that integrate RAG technique into AIGC scenarios. We first
classify RAG foundations according to how the retriever augments the generator,
distilling the fundamental abstractions of the augmentation methodologies for
various retrievers and generators. This unified perspective encompasses all RAG
scenarios, illuminating advancements and pivotal technologies that help with
potential future progress. We also summarize additional enhancements methods
for RAG, facilitating effective engineering and implementation of RAG systems.
Then from another view, we survey on practical applications of RAG across
different modalities and tasks, offering valuable references for researchers
and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss
the limitations of current RAG systems, and suggest potential directions for
future research.Project Repo: https://github.com/hymie122/RAG-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Citing 380 papers, 36 pages, 16 figures. Project:
  https://github.com/hymie122/RAG-Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physical 3D Adversarial Attacks against Monocular Depth Estimation in
  Autonomous Driving <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Zheng, Chenhao Lin, Jiahao Sun, Zhengyu Zhao, Qian Li, Chao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based monocular depth estimation (MDE), extensively applied in
autonomous driving, is known to be vulnerable to adversarial attacks. Previous
physical attacks against MDE models rely on 2D adversarial patches, so they
only affect a small, localized region in the MDE map but fail under various
viewpoints. To address these limitations, we propose 3D Depth Fool
(3D$^2$Fool), the first 3D texture-based adversarial attack against MDE models.
3D$^2$Fool is specifically optimized to generate 3D adversarial textures
agnostic to model types of vehicles and to have improved robustness in bad
weather conditions, such as rain and fog. Experimental results validate the
superior performance of our 3D$^2$Fool across various scenarios, including
vehicles, MDE models, weather conditions, and viewpoints. Real-world
experiments with printed 3D textures on physical vehicle models further
demonstrate that our 3D$^2$Fool can cause an MDE error of over 10 meters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly-Supervised Conditional Embedding for Referred Visual Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Lepage, Jérémie Mary, David Picard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new challenge for image similarity search in the
context of fashion, addressing the inherent ambiguity in this domain stemming
from complex images. We present Referred Visual Search (RVS), a task allowing
users to define more precisely the desired similarity, following recent
interest in the industry. We release a new large public dataset,
LAION-RVS-Fashion, consisting of 272k fashion products with 842k images
extracted from LAION, designed explicitly for this task. However, unlike
traditional visual search methods in the industry, we demonstrate that superior
performance can be achieved by bypassing explicit object detection and adopting
weakly-supervised conditional contrastive learning on image tuples. Our method
is lightweight and demonstrates robustness, reaching Recall at one superior to
strong detection-based baselines against 2M distractors. Code, data and models
are available at https://www.github.com/Simon-Lepage/CondViT-LRVSF .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 13 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-criteria Token Fusion with One-step-ahead Attention for Efficient
  Vision <span class="highlight-title">Transformer</span>s <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyeok Lee, Joonmyung Choi, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformer (ViT) has emerged as a prominent backbone for computer
vision. For more efficient ViTs, recent works lessen the quadratic cost of the
self-attention layer by pruning or fusing the redundant tokens. However, these
works faced the speed-accuracy trade-off caused by the loss of information.
Here, we argue that token fusion needs to consider diverse relations between
tokens to minimize information loss. In this paper, we propose a Multi-criteria
Token Fusion (MCTF), that gradually fuses the tokens based on multi-criteria
(e.g., similarity, informativeness, and size of fused tokens). Further, we
utilize the one-step-ahead attention, which is the improved approach to capture
the informativeness of the tokens. By training the model equipped with MCTF
using a token reduction consistency, we achieve the best speed-accuracy
trade-off in the image classification (ImageNet1K). Experimental results prove
that MCTF consistently surpasses the previous reduction methods with and
without training. Specifically, DeiT-T and DeiT-S with MCTF reduce FLOPs by
about 44% while improving the performance (+0.5%, and +0.3%) over the base
model, respectively. We also demonstrate the applicability of MCTF in various
Vision Transformers (e.g., T2T-ViT, LV-ViT), achieving at least 31% speedup
without performance degradation. Code is available at
https://github.com/mlvlab/MCTF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Computer Vision and Pattern Recognition (CVPR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Adaptive Saliency Guidance for Exemplar-free Class Incremental
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xialei Liu, Jiang-Tian Zhai, Andrew D. Bagdanov, Ke Li, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exemplar-free Class Incremental Learning (EFCIL) aims to sequentially learn
tasks with access only to data from the current one. EFCIL is of interest
because it mitigates concerns about privacy and long-term storage of data,
while at the same time alleviating the problem of catastrophic forgetting in
incremental learning. In this work, we introduce task-adaptive saliency for
EFCIL and propose a new framework, which we call Task-Adaptive Saliency
Supervision (TASS), for mitigating the negative effects of saliency drift
between different tasks. We first apply boundary-guided saliency to maintain
task adaptivity and \textit{plasticity} on model attention. Besides, we
introduce task-agnostic low-level signals as auxiliary supervision to increase
the \textit{stability} of model attention. Finally, we introduce a module for
injecting and recovering saliency noise to increase the robustness of saliency
preservation. Our experiments demonstrate that our method can better preserve
saliency maps across tasks and achieve state-of-the-art results on the
CIFAR-100, Tiny-ImageNet, and ImageNet-Subset EFCIL benchmarks. Code is
available at \url{https://github.com/scok30/tass}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effects of Mixed Sample Data Augmentation are Class Dependent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeil Lee, Hansang Lee, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed Sample Data Augmentation (MSDA) techniques, such as Mixup, CutMix, and
PuzzleMix, have been widely acknowledged for enhancing performance in a variety
of tasks. A previous study reported the class dependency of traditional data
augmentation (DA), where certain classes benefit disproportionately compared to
others. This paper reveals a class dependent effect of MSDA, where some classes
experience improved performance while others experience degraded performance.
This research addresses the issue of class dependency in MSDA and proposes an
algorithm to mitigate it. The approach involves training on a mixture of MSDA
and non-MSDA data, which not only mitigates the negative impact on the affected
classes, but also improves overall accuracy. Furthermore, we provide in-depth
analysis and discussion of why MSDA introduced class dependencies and which
classes are most likely to have them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 18 figures, Overall Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although 3D shape matching and interpolation are highly interrelated, they
are often studied separately and applied sequentially to relate different 3D
shapes, thus resulting in sub-optimal performance. In this work we present a
unified framework to predict both point-wise correspondences and shape
interpolation between 3D shapes. To this end, we combine the deep functional
map framework with classical surface deformation models to map shapes in both
spectral and spatial domains. On the one hand, by incorporating spatial maps,
our method obtains more accurate and smooth point-wise correspondences compared
to previous functional map methods for shape matching. On the other hand, by
introducing spectral maps, our method gets rid of commonly used but
computationally expensive geodesic distance constraints that are only valid for
near-isometric shape deformations. Furthermore, we propose a novel test-time
adaptation scheme to capture both pose-dominant and shape-dominant
deformations. Using different challenging datasets, we demonstrate that our
method outperforms previous state-of-the-art methods for both shape matching
and interpolation, even compared to supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CEIMVEN: An Approach of Cutting Edge Implementation of Modified Versions
  of EfficientNet (V1-V2) Architecture for Breast Cancer Detection and
  Classification from Ultrasound Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13356v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13356v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheekar Banerjee, Md. Kamrul Hasan Monir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Undoubtedly breast cancer identifies itself as one of the most widespread and
terrifying cancers across the globe. Millions of women are getting affected
each year from it. Breast cancer remains the major one for being the reason of
largest number of demise of women. In the recent time of research, Medical
Image Computing and Processing has been playing a significant role for
detecting and classifying breast cancers from ultrasound images and mammograms,
along with the celestial touch of deep neural networks. In this research, we
focused mostly on our rigorous implementations and iterative result analysis of
different cutting-edge modified versions of EfficientNet architectures namely
EfficientNet-V1 (b0-b7) and EfficientNet-V2 (b0-b3) with ultrasound image,
named as CEIMVEN. We utilized transfer learning approach here for using the
pre-trained models of EfficientNet versions. We activated the hyper-parameter
tuning procedures, added fully connected layers, discarded the unprecedented
outliers and recorded the accuracy results from our custom modified
EfficientNet architectures. Our deep learning model training approach was
related to both identifying the cancer affected areas with region of interest
(ROI) techniques and multiple classifications (benign, malignant and normal).
The approximate testing accuracies we got from the modified versions of
EfficientNet-V1 (b0- 99.15%, b1- 98.58%, b2- 98.43%, b3- 98.01%, b4- 98.86%,
b5- 97.72%, b6- 97.72%, b7- 98.72%) and EfficientNet-V2 (b0- 99.29%, b1-
99.01%, b2- 98.72%, b3- 99.43%) are showing very bright future and strong
potentials of deep learning approach for the successful detection and
classification of breast cancers from the ultrasound images at a very early
stage. The code for this research is available here:
https://github.com/ac005sheekar/CEIMVEN-Breast.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViT-CoMer: Vision <span class="highlight-title">Transformer</span> with Convolutional Multi-scale Feature
  Interaction for Dense Predictions <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07392v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07392v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, Yifeng Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Vision Transformer (ViT) has achieved significant success in
computer vision, it does not perform well in dense prediction tasks due to the
lack of inner-patch information interaction and the limited diversity of
feature scale. Most existing studies are devoted to designing vision-specific
transformers to solve the above problems, which introduce additional
pre-training costs. Therefore, we present a plain, pre-training-free, and
feature-enhanced ViT backbone with Convolutional Multi-scale feature
interaction, named ViT-CoMer, which facilitates bidirectional interaction
between CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has
the following advantages: (1) We inject spatial pyramid multi-receptive field
convolutional features into the ViT architecture, which effectively alleviates
the problems of limited local information interaction and single-feature
representation in ViT. (2) We propose a simple and efficient CNN-Transformer
bidirectional fusion interaction module that performs multi-scale fusion across
hierarchical features, which is beneficial for handling dense prediction tasks.
(3) We evaluate the performance of ViT-CoMer across various dense prediction
tasks, different frameworks, and multiple advanced pre-training. Notably, our
ViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and
62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art
methods. We hope ViT-CoMer can serve as a new backbone for dense prediction
tasks to facilitate future research. The code will be released at
https://github.com/Traffic-X/ViT-CoMer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterControl: Generate Human Motion Interactions by Controlling Every
  Joint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenzhi Wang, Jingbo Wang, Yixuan Li, Dahua Lin, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-conditioned human motion synthesis has made remarkable progress with the
emergence of diffusion models in recent research. However, the majority of
these motion diffusion models are primarily designed for a single character and
overlook multi-human interactions. In our approach, we strive to explore this
problem by synthesizing human motion with interactions for a group of
characters of any size. The key aspect of our approach is the adaptation of
human-wise interactions as pairs of human joints that can be either in contact
or separated by a desired distance. In contrast to existing methods that
necessitate training motion generation models on multi-human motion datasets
with a fixed number of characters, our approach inherently possesses the
flexibility to model human interactions involving an arbitrary number of
individuals, thereby transcending the limitations imposed by the training data.
We introduce a novel controllable motion generation method, InterControl, to
encourage the synthesized motions maintaining the desired distance between
joint pairs. It consists of a motion controller and an inverse kinematics
guidance module that realistically and accurately aligns the joints of
synthesized characters to the desired location. Furthermore, we demonstrate
that the distance between joint pairs for human-wise interactions can be
generated using an off-the-shelf Large Language Model (LLM). Experimental
results highlight the capability of our framework to generate interactions with
multiple human characters and its potential to work with off-the-shelf
physics-based character simulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Generate human interactions with only single-person data via joint
  contact pairs, code https://github.com/zhenzhiwang/intercontrol</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the remarkable achievements in image generation through diffusion
models, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their memory consumption, which increases
quadratically with the length of the sequence. This limitation presents
significant challenges when attempting to generate longer video sequences using
diffusion models. To overcome this challenge, we propose leveraging state-space
models (SSMs). SSMs have recently gained attention as viable alternatives due
to their linear memory consumption relative to sequence length. In the
experiments, we first evaluate our SSM-based model with UCF101, a standard
benchmark of video generation. In addition, to investigate the potential of
SSMs for longer video generation, we perform an experiment using the MineRL
Navigate dataset, varying the number of frames to 64, 200, and 400. In these
settings, our SSM-based model can considerably save memory consumption for
longer sequences, while maintaining competitive FVD scores to the
attention-based models. Our codes are available at
https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as workshop paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rotation-Invariant <span class="highlight-title">Transformer</span> for Point Cloud Matching <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08231v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08231v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yu, Zheng Qin, Ji Hou, Mahdi Saleh, Dongsheng Li, Benjamin Busam, Slobodan Ilic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intrinsic rotation invariance lies at the core of matching point clouds
with handcrafted descriptors. However, it is widely despised by recent deep
matchers that obtain the rotation invariance extrinsically via data
augmentation. As the finite number of augmented rotations can never span the
continuous SO(3) space, these methods usually show instability when facing
rotations that are rarely seen. To this end, we introduce RoITr, a
Rotation-Invariant Transformer to cope with the pose variations in the point
cloud matching task. We contribute both on the local and global levels.
Starting from the local level, we introduce an attention mechanism embedded
with Point Pair Feature (PPF)-based coordinates to describe the pose-invariant
geometry, upon which a novel attention-based encoder-decoder architecture is
constructed. We further propose a global transformer with rotation-invariant
cross-frame spatial awareness learned by the self-attention mechanism, which
significantly improves the feature distinctiveness and makes the model robust
with respect to the low overlap. Experiments are conducted on both the rigid
and non-rigid public benchmarks, where RoITr outperforms all the
state-of-the-art models by a considerable margin in the low-overlapping
scenarios. Especially when the rotations are enlarged on the challenging
3DLoMatch benchmark, RoITr surpasses the existing methods by at least 13 and 5
percentage points in terms of Inlier Ratio and Registration Recall,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extend Your Own Correspondences: Unsupervised Distant Point Cloud
  Registration by Progressive Distance Extension <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Liu, Hongzi Zhu, Zhenxi Wang, Yunsong Zhou, Shan Chang, Minyi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Registration of point clouds collected from a pair of distant vehicles
provides a comprehensive and accurate 3D view of the driving scenario, which is
vital for driving safety related applications, yet existing literature suffers
from the expensive pose label acquisition and the deficiency to generalize to
new data distributions. In this paper, we propose EYOC, an unsupervised distant
point cloud registration method that adapts to new point cloud distributions on
the fly, requiring no global pose labels. The core idea of EYOC is to train a
feature extractor in a progressive fashion, where in each round, the feature
extractor, trained with near point cloud pairs, can label slightly farther
point cloud pairs, enabling self-supervision on such far point cloud pairs.
This process continues until the derived extractor can be used to register
distant point clouds. Particularly, to enable high-fidelity correspondence
label generation, we devise an effective spatial filtering scheme to select the
most representative correspondences to register a point cloud pair, and then
utilize the aligned point clouds to discover more correct correspondences.
Experiments show that EYOC can achieve comparable performance with
state-of-the-art supervised methods at a lower training cost. Moreover, it
outwits supervised methods regarding generalization performance on new data
distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Model Makes Clustering A Better Initialization For Cold-Start
  Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yuan, Chuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning selects the most informative samples from the unlabelled
dataset to annotate in the context of a limited annotation budget. While
numerous methods have been proposed for subsequent sample selection based on an
initialized model, scant attention has been paid to the indispensable phase of
active learning: selecting samples for model cold-start initialization. Most of
the previous studies resort to random sampling or naive clustering. However,
random sampling is prone to fluctuation, and naive clustering suffers from
convergence speed, particularly when dealing with high-dimensional data such as
imaging data. In this work, we propose to integrate foundation models with
clustering methods to select samples for cold-start active learning
initialization. Foundation models refer to those trained on massive datasets by
the self-supervised paradigm and capable of generating informative and
compacted embeddings for various downstream tasks. Leveraging these embeddings
to replace raw features such as pixel values, clustering quickly converges and
identifies better initial samples. For a comprehensive comparison, we included
a classic ImageNet-supervised model to acquire embeddings. Experiments on two
clinical tasks of image classification and segmentation demonstrated that
foundation model-based clustering efficiently pinpointed informative initial
samples, leading to models showcasing enhanced performance than the baseline
methods. We envisage that this study provides an effective paradigm for future
cold-start active learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with
  Iterative Diffusion-Based Refinement <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17456v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17456v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuming Liu, Guangming Wang, Weicai Ye, Chaokang Jiang, Jinru Han, Zhe Liu, Guofeng Zhang, Dalong Du, Hesheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow estimation, which aims to predict per-point 3D displacements of
dynamic scenes, is a fundamental task in the computer vision field. However,
previous works commonly suffer from unreliable correlation caused by locally
constrained searching ranges, and struggle with accumulated inaccuracy arising
from the coarse-to-fine structure. To alleviate these problems, we propose a
novel uncertainty-aware scene flow estimation network (DifFlow3D) with the
diffusion probabilistic model. Iterative diffusion-based refinement is designed
to enhance the correlation robustness and resilience to challenging cases, e.g.
dynamics, noisy inputs, repetitive patterns, etc. To restrain the generation
diversity, three key flow-related features are leveraged as conditions in our
diffusion model. Furthermore, we also develop an uncertainty estimation module
within diffusion to evaluate the reliability of estimated scene flow. Our
DifFlow3D achieves state-of-the-art performance, with 24.0% and 29.1% EPE3D
reduction respectively on FlyingThings3D and KITTI 2015 datasets. Notably, our
method achieves an unprecedented millimeter-level accuracy (0.0078m in EPE3D)
on the KITTI dataset. Additionally, our diffusion-based refinement paradigm can
be readily integrated as a plug-and-play module into existing scene flow
networks, significantly increasing their estimation accuracy. Codes are
released at https://github.com/IRMVLab/DifFlow3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version of CVPR 2024. Codes are released at
  https://github.com/IRMVLab/DifFlow3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection
  in Aerial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02200v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02200v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanchao Huang, Wei Li, Xiang-Gen Xia, Hao Wang, Ran Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arbitrary-oriented object detection (AOOD) has been widely applied to locate
and classify objects with diverse orientations in remote sensing images.
However, the inconsistent features for the localization and classification
tasks in AOOD models may lead to ambiguity and low-quality object predictions,
which constrains the detection performance. In this article, an AOOD method
called task-wise sampling convolutions (TS-Conv) is proposed. TS-Conv
adaptively samples task-wise features from respective sensitive regions and
maps these features together in alignment to guide a dynamic label assignment
for better predictions. Specifically, sampling positions of the localization
convolution in TS-Conv are supervised by the oriented bounding box (OBB)
prediction associated with spatial coordinates, while sampling positions and
convolutional kernel of the classification convolution are designed to be
adaptively adjusted according to different orientations for improving the
orientation robustness of features. Furthermore, a dynamic
task-consistent-aware label assignment (DTLA) strategy is developed to select
optimal candidate positions and assign labels dynamically according to ranked
task-aware scores obtained from TS-Conv. Extensive experiments on several
public datasets covering multiple scenes, multimodal images, and multiple
categories of objects demonstrate the effectiveness, scalability, and superior
performance of the proposed TS-Conv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 13 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws For Dense Retrieval <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up neural models has yielded significant advancements in a wide array
of tasks, particularly in language generation. Previous studies have found that
the performance of neural models frequently adheres to predictable scaling
laws, correlated with factors such as training set size and model size. This
insight is invaluable, especially as large-scale experiments grow increasingly
resource-intensive. Yet, such scaling law has not been fully explored in dense
retrieval due to the discrete nature of retrieval metrics and complex
relationships between training data and model sizes in retrieval tasks. In this
study, we investigate whether the performance of dense retrieval models follows
the scaling law as other neural models. We propose to use contrastive
log-likelihood as the evaluation metric and conduct extensive experiments with
dense retrieval models implemented with different numbers of parameters and
trained with different amounts of annotated data. Results indicate that, under
our settings, the performance of dense retrieval models follows a precise
power-law scaling related to the model size and the number of annotations.
Additionally, we examine scaling with prevalent data augmentation methods to
assess the impact of annotation quality, and apply the scaling law to find the
best resource allocation strategy under a budget constraint. We believe that
these insights will significantly contribute to understanding the scaling
effect of dense retrieval models and offer meaningful guidance for future
research endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Content Recommendation: Knowledge Graph-Based Semantic
  Contrastive Learning for Diversity and Cold-Start Users <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Kim, Scott Rome, Kevin Foley, Mayur Nankani, Rimon Melamed, Javier Morales, Abhay Yadav, Maria Peifer, Sardar Hamidian, H. Howie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenges related to data sparsity, cold-start problems, and
diversity in recommendation systems is both crucial and demanding. Many current
solutions leverage knowledge graphs to tackle these issues by combining both
item-based and user-item collaborative signals. A common trend in these
approaches focuses on improving ranking performance at the cost of escalating
model complexity, reducing diversity, and complicating the task. It is
essential to provide recommendations that are both personalized and diverse,
rather than solely relying on achieving high rank-based performance, such as
Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task
learning approach, training on user-item and item-item interactions. We apply
item-based contrastive learning on descriptive text, sampling positive and
negative pairs based on item metadata. Our approach allows the model to better
understand the relationships between entities within the knowledge graph by
utilizing semantic information from text. It leads to more accurate, relevant,
and diverse user recommendations and a benefit that extends even to cold-start
users who have few interactions with items. We perform extensive experiments on
two widely used datasets to validate the effectiveness of our approach. Our
findings demonstrate that jointly training user-item interactions and
item-based signals using synopsis text is highly effective. Furthermore, our
results provide evidence that item-based contrastive learning enhances the
quality of entity embeddings, as indicated by metrics such as uniformity and
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Recommend or Not: Recommendability Identification in Conversations
  with <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefan Wang, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most current recommender systems primarily focus on what to recommend,
assuming users always require personalized recommendations. However, with the
widely spread of ChatGPT and other chatbots, a more crucial problem in the
context of conversational systems is how to minimize user disruption when we
provide recommendation services for users. While previous research has
extensively explored different user intents in dialogue systems, fewer efforts
are made to investigate whether recommendations should be provided. In this
paper, we formally define the recommendability identification problem, which
aims to determine whether recommendations are necessary in a specific scenario.
First, we propose and define the recommendability identification task, which
investigates the need for recommendations in the current conversational
context. A new dataset is constructed. Subsequently, we discuss and evaluate
the feasibility of leveraging pre-trained language models (PLMs) for
recommendability identification. Finally, through comparative experiments, we
demonstrate that directly employing PLMs with zero-shot results falls short of
meeting the task requirements. Besides, fine-tuning or utilizing soft prompt
techniques yields comparable results to traditional classification methods. Our
work is the first to study recommendability before recommendation and provides
preliminary ways to make it a fundamental component of the future
recommendation system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Antitrust, Amazon, and Algorithmic Auditing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhisek Dash, Abhijnan Chakraborty, Saptarshi Ghosh, Animesh Mukherjee, Jens Frankenreiter, Stefan Bechtold, Krishna P. Gummadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In digital markets, antitrust law and special regulations aim to ensure that
markets remain competitive despite the dominating role that digital platforms
play today in everyone's life. Unlike traditional markets, market participant
behavior is easily observable in these markets. We present a series of
empirical investigations into the extent to which Amazon engages in practices
that are typically described as self-preferencing. We discuss how the computer
science tools used in this paper can be used in a regulatory environment that
is based on algorithmic auditing and requires regulating digital markets at
scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted to appear at Journal of Institutional and
  Theoretical Economics (JITE) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity,
  and Seasonality into Tourism Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashmi Banerjee, Tunar Mahmudov, Emil Adler, Fitri Nur Aisyah, Wolfgang Wörndl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era of information overload and complex decision-making processes,
Recommender Systems (RS) have emerged as indispensable tools across diverse
domains, particularly travel and tourism. These systems simplify trip planning
by offering personalized recommendations that consider individual preferences
and address broader challenges like seasonality, travel regulations, and
capacity constraints. The intricacies of the tourism domain, characterized by
multiple stakeholders, including consumers, item providers, platforms, and
society, underscore the complexity of achieving balance among diverse
interests. Although previous research has focused on fairness in Tourism
Recommender Systems (TRS) from a multistakeholder perspective, limited work has
focused on generating sustainable recommendations.
  Our paper introduces a novel approach for assigning a sustainability
indicator (SF index) for city trips accessible from the users' starting point,
integrating Co2e analysis, destination popularity, and seasonal demand. Our
methodology involves comprehensive data gathering on transportation modes and
emissions, complemented by analyses of destination popularity and seasonal
demand. A user study validates our index, showcasing its practicality and
efficacy in providing well-rounded and sustainable city trip recommendations.
Our findings contribute significantly to the evolution of responsible tourism
strategies, harmonizing the interests of tourists, local communities, and the
environment while paving the way for future research in responsible and
equitable tourism practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Behavior-Based Recommendation System for E-commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Barzegar Nozari, Mahdi Divsalar, Sepehr Akbarzadeh Abkenar, Mohammadreza Fadavi Amiri, Ali Divsalar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of existing recommender systems rely on user ratings, which are
limited by the lack of user collaboration and the sparsity problem. To address
these issues, this study proposes a behavior-based recommender system that
leverages customers' natural behaviors, such as browsing and clicking, on
e-commerce platforms. The proposed recommendation system involves clustering
active customers, determining neighborhoods, collecting similar users,
calculating product reputation based on similar users, and recommending
high-reputation products. To overcome the complexity of customer behaviors and
traditional clustering methods, an unsupervised clustering approach based on
product categories is developed to enhance the recommendation methodology. This
study makes notable contributions in several aspects. Firstly, a groundbreaking
behavior-based recommendation methodology is developed, incorporating customer
behavior to generate accurate and tailored recommendations leading to improved
customer satisfaction and engagement. Secondly, an original unsupervised
clustering method, focusing on product categories, enables more precise
clustering and facilitates accurate recommendations. Finally, an approach to
determine neighborhoods for active customers within clusters is established,
ensuring grouping of customers with similar behavioral patterns to enhance
recommendation accuracy and relevance. The proposed recommendation methodology
and clustering method contribute to improved recommendation performance,
offering valuable insights for researchers and practitioners in the field of
e-commerce recommendation systems. Additionally, the proposed method
outperforms benchmark methods in experiments conducted using a behavior dataset
from the well-known e-commerce site Alibaba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Generative Recommendation via Content and Collaboration
  Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Wang, Zhaochun Ren, Weiwei Sun, Jiyuan Yang, Zhixiang Liang, Xin Chen, Ruobing Xie, Su Yan, Xu Zhang, Pengjie Ren, Zhumin Chen, Xin Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation has emerged as a promising paradigm aimed at
augmenting recommender systems with recent advancements in generative
artificial intelligence. This task has been formulated as a
sequence-to-sequence generation process, wherein the input sequence encompasses
data pertaining to the user's previously interacted items, and the output
sequence denotes the generative identifier for the suggested item. However,
existing generative recommendation approaches still encounter challenges in (i)
effectively integrating user-item collaborative signals and item content
information within a unified generative framework, and (ii) executing an
efficient alignment between content information and collaborative signals.
  In this paper, we introduce content-based collaborative generation for
recommender systems, denoted as ColaRec. To capture collaborative signals, the
generative item identifiers are derived from a pretrained collaborative
filtering model, while the user is represented through the aggregation of
interacted items' content. Subsequently, the aggregated textual description of
items is fed into a language model to encapsulate content information. This
integration enables ColaRec to amalgamate collaborative signals and content
information within an end-to-end framework. Regarding the alignment, we propose
an item indexing task to facilitate the mapping between the content-based
semantic space and the interaction-based collaborative space. Additionally, a
contrastive loss is introduced to ensure that items with similar collaborative
GIDs possess comparable content representations, thereby enhancing alignment.
To validate the efficacy of ColaRec, we conduct experiments on three benchmark
datasets. Empirical results substantiate the superior performance of ColaRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Embeddings for Graph Collaborative Filtering <span class="chip">SIGIR '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xurong Liang, Tong Chen, Lizhen Cui, Yang Wang, Meng Wang, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are currently one of the most performant
collaborative filtering methods. Meanwhile, owing to the use of an embedding
table to represent each user/item as a distinct vector, GNN-based recommenders
have inherited the long-standing defect of parameter inefficiency. As a common
practice for scalable embeddings, parameter sharing enables the use of fewer
embedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most
existing methods are a heuristically designed, predefined mapping from each
user's/item's ID to the corresponding meta-embedding indexes, thus simplifying
the optimization problem into learning only the meta-embeddings. However, in
the context of GNN-based collaborative filtering, such a fixed mapping omits
the semantic correlations between entities that are evident in the user-item
interaction graph, leading to suboptimal recommendation performance. To this
end, we propose Lightweight Embeddings for Graph Collaborative Filtering
(LEGCF), a parameter-efficient embedding framework dedicated to GNN-based
recommenders. LEGCF innovatively introduces an assignment matrix as an extra
learnable component on top of meta-embeddings. To jointly optimize these two
heavily entangled components, aside from learning the meta-embeddings by
minimizing the recommendation loss, LEGCF further performs efficient assignment
update by enforcing a novel semantic similarity constraint and finding its
closed-form solution based on matrix pseudo-inverse. The meta-embeddings and
assignment matrix are alternately updated, where the latter is sparsified on
the fly to ensure negligible storage overhead. Extensive experiments on three
benchmark datasets have verified LEGCF's smallest trade-off between size and
performance, with consistent accuracy gain over state-of-the-art baselines. The
codebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoy Effect In Search Interaction: Understanding User Behavior and
  Measuring System Vulnerability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Jiqun Liu, Hanpei Fang, Yuankai Luo, Tetsuya Sakai, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the decoy effect's underexplored influence on user search
interactions and methods for measuring information retrieval (IR) systems'
vulnerability to this effect. It explores how decoy results alter users'
interactions on search engine result pages, focusing on metrics like
click-through likelihood, browsing time, and perceived document usefulness. By
analyzing user interaction logs from multiple datasets, the study demonstrates
that decoy results significantly affect users' behavior and perceptions.
Furthermore, it investigates how different levels of task difficulty and user
knowledge modify the decoy effect's impact, finding that easier tasks and lower
knowledge levels lead to higher engagement with target documents. In terms of
IR system evaluation, the study introduces the DEJA-VU metric to assess
systems' susceptibility to the decoy effect, testing it on specific retrieval
tasks. The results show differences in systems' effectiveness and
vulnerability, contributing to our understanding of cognitive biases in search
behavior and suggesting pathways for creating more balanced and bias-aware IR
evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELTA: <span class="highlight-title">Pre-train</span> a Discriminative Encoder for Legal Case Retrieval via
  Structural Word Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Qingyao Ai, Xinyan Han, Jia Chen, Qian Dong, Yiqun Liu, Chong Chen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research demonstrates the effectiveness of using pre-trained language
models for legal case retrieval. Most of the existing works focus on improving
the representation ability for the contextualized embedding of the [CLS] token
and calculate relevance using textual semantic similarity. However, in the
legal domain, textual semantic similarity does not always imply that the cases
are relevant enough. Instead, relevance in legal cases primarily depends on the
similarity of key facts that impact the final judgment. Without proper
treatments, the discriminative ability of learned representations could be
limited since legal cases are lengthy and contain numerous non-key facts. To
this end, we introduce DELTA, a discriminative model designed for legal case
retrieval. The basic idea involves pinpointing key facts in legal cases and
pulling the contextualized embedding of the [CLS] token closer to the key facts
while pushing away from the non-key facts, which can warm up the case embedding
space in an unsupervised manner. To be specific, this study brings the word
alignment mechanism to the contextual masked auto-encoder. First, we leverage
shallow decoders to create information bottlenecks, aiming to enhance the
representation ability. Second, we employ the deep decoder to enable
translation between different structures, with the goal of pinpointing key
facts to enhance discriminative ability. Comprehensive experiments conducted on
publicly available legal benchmarks show that our approach can outperform
existing state-of-the-art methods in legal case retrieval. It provides a new
perspective on the in-depth understanding and processing of legal case
documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Recommendation with Latent Relations based on Large Language
  Model <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghao Yang, Weizhi Ma, Peijie Sun, Qingyao Ai, Yiqun Liu, Mingchen Cai, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems predict items that may interest users by
modeling their preferences based on historical interactions. Traditional
sequential recommendation methods rely on capturing implicit collaborative
filtering signals among items. Recent relation-aware sequential recommendation
models have achieved promising performance by explicitly incorporating item
relations into the modeling of user historical sequences, where most relations
are extracted from knowledge graphs. However, existing methods rely on manually
predefined relations and suffer the sparsity issue, limiting the generalization
ability in diverse scenarios with varied item relations. In this paper, we
propose a novel relation-aware sequential recommendation framework with Latent
Relation Discovery (LRD). Different from previous relation-aware models that
rely on predefined rules, we propose to leverage the Large Language Model (LLM)
to provide new types of relations and connections between items. The motivation
is that LLM contains abundant world knowledge, which can be adopted to mine
latent relations of items for recommendation. Specifically, inspired by that
humans can describe relations between items using natural language, LRD
harnesses the LLM that has demonstrated human-like knowledge to obtain language
knowledge representations of items. These representations are fed into a latent
relation discovery module based on the discrete state variational autoencoder
(DVAE). Then the self-supervised relation discovery tasks and recommendation
tasks are jointly optimized. Experimental results on multiple public datasets
demonstrate our proposed latent relations discovery method can be incorporated
with existing relation-aware sequential recommendation models and significantly
improve the performance. Further analysis experiments indicate the
effectiveness and reliability of the discovered latent relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Common Sense Enhanced Knowledge-based Recommendation with Large Language
  Model <span class="chip">DASFAA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghao Yang, Weizhi Ma, Peijie Sun, Min Zhang, Qingyao Ai, Yiqun Liu, Mingchen Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-based recommendation models effectively alleviate the data sparsity
issue leveraging the side information in the knowledge graph, and have achieved
considerable performance. Nevertheless, the knowledge graphs used in previous
work, namely metadata-based knowledge graphs, are usually constructed based on
the attributes of items and co-occurring relations (e.g., also buy), in which
the former provides limited information and the latter relies on sufficient
interaction data and still suffers from cold start issue. Common sense, as a
form of knowledge with generality and universality, can be used as a supplement
to the metadata-based knowledge graph and provides a new perspective for
modeling users' preferences. Recently, benefiting from the emergent world
knowledge of the large language model, efficient acquisition of common sense
has become possible. In this paper, we propose a novel knowledge-based
recommendation framework incorporating common sense, CSRec, which can be
flexibly coupled to existing knowledge-based methods. Considering the challenge
of the knowledge gap between the common sense-based knowledge graph and
metadata-based knowledge graph, we propose a knowledge fusion approach based on
mutual information maximization theory. Experimental results on public datasets
demonstrate that our approach significantly improves the performance of
existing knowledge-based recommendation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Situation-aware Enhancer for Personalized Recommendation <span class="chip">DASFAA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Li, Peijie Sun, Chumeng Jiang, Weizhi Ma, Qingyao Ai, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When users interact with Recommender Systems (RecSys), current situations,
such as time, location, and environment, significantly influence their
preferences. Situations serve as the background for interactions, where
relationships between users and items evolve with situation changes. However,
existing RecSys treat situations, users, and items on the same level. They can
only model the relations between situations and users/items respectively,
rather than the dynamic impact of situations on user-item associations (i.e.,
user preferences). In this paper, we provide a new perspective that takes
situations as the preconditions for users' interactions. This perspective
allows us to separate situations from user/item representations, and capture
situations' influences over the user-item relationship, offering a more
comprehensive understanding of situations. Based on it, we propose a novel
Situation-Aware Recommender Enhancer (SARE), a pluggable module to integrate
situations into various existing RecSys. Since users' perception of situations
and situations' impact on preferences are both personalized, SARE includes a
Personalized Situation Fusion (PSF) and a User-Conditioned Preference Encoder
(UCPE) to model the perception and impact of situations, respectively. We
conduct experiments of applying SARE on seven backbones in various settings on
two real-world datasets. Experimental results indicate that SARE improves the
recommendation performances significantly compared with backbones and SOTA
situation-aware baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Conference on Database Systems for
  Advanced Applications (DASFAA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Recommender System for NFT Collectibles with Item Feature <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjoo Choi, Seonmi Kim, Yejin Kim, Youngbin Lee, Joohwan Hong, Yongjae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been actively studied and applied in various domains
to deal with information overload. Although there are numerous studies on
recommender systems for movies, music, and e-commerce, comparatively less
attention has been paid to the recommender system for NFTs despite the
continuous growth of the NFT market. This paper presents a recommender system
for NFTs that utilizes a variety of data sources, from NFT transaction records
to external item features, to generate precise recommendations that cater to
individual preferences. We develop a data-efficient graph-based recommender
system to efficiently capture the complex relationship between each item and
users and generate node(item) embeddings which incorporate both node feature
information and graph structure. Furthermore, we exploit inputs beyond
user-item interactions, such as image feature, text feature, and price feature.
Numerical experiments verify the performance of the graph-based recommender
system improves significantly after utilizing all types of item features as
side information, thereby outperforming all other baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AAAI 2023 Bridge on AI for Financial Services
  (https://sites.google.com/view/aaai-ai-fin/home)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Out-of-Vocabulary Handling in Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Shiao, Mingxuan Ju, Zhichun Guo, Xin Chen, Evangelos Papalexakis, Tong Zhao, Neil Shah, Yozen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems (RS) are an increasingly relevant area for both
academic and industry researchers, given their widespread impact on the daily
online experiences of billions of users. One common issue in real RS is the
cold-start problem, where users and items may not contain enough information to
produce high-quality recommendations. This work focuses on a complementary
problem: recommending new users and items unseen (out-of-vocabulary, or OOV) at
training time. This setting is known as the inductive setting and is especially
problematic for factorization-based models, which rely on encoding only those
users/items seen at training time with fixed parameter vectors. Many existing
solutions applied in practice are often naive, such as assigning OOV
users/items to random buckets. In this work, we tackle this problem and propose
approaches that better leverage available user/item features to improve OOV
handling at the embedding table level. We discuss general-purpose plug-and-play
approaches that are easily applicable to most RS models and improve inductive
performance without negatively impacting transductive model performance. We
extensively evaluate 9 OOV embedding methods on 5 models across 4 datasets
(spanning different domains). One of these datasets is a proprietary production
dataset from a prominent RS employed by a large social platform serving
hundreds of millions of daily active users. In our experiments, we find that
several proposed methods that exploit feature similarity using LSH consistently
outperform alternatives on most model-dataset combinations, with the best
method showing a mean improvement of 3.74% over the industry standard baseline
in inductive performance. We release our code and hope our work helps
practitioners make more informed decisions when handling OOV for their RS and
further inspires academic research into improving OOV support in RS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era
  of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer structure has achieved great success in multiple applied machine
learning communities, such as natural language processing (NLP), computer
vision (CV) and information retrieval (IR). Transformer architecture's core
mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$
time complexity in inference. Many works have been proposed to improve the
attention mechanism's scalability, such as Flash Attention and Multi-query
Attention. A different line of work aims to design new mechanisms to replace
attention. Recently, a notable model structure -- Mamba, which is based on
state space models, has achieved transformer-equivalent performance in multiple
sequence modeling tasks.
  In this work, we examine \mamba's efficacy through the lens of a classical IR
task -- document ranking. A reranker model takes a query and a document as
input, and predicts a scalar relevance score. This task demands the language
model's ability to comprehend lengthy contextual inputs and to capture the
interaction between query and document tokens. We find that (1) Mamba models
achieve competitive performance compared to transformer-based models with the
same training recipe; (2) but also have a lower training throughput in
comparison to efficient transformer implementations such as flash attention. We
hope this study can serve as a starting point to explore Mamba models in other
classical IR tasks. Our code implementation and trained checkpoints are made
public to facilitate
reproducibility.\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Backpropagation in Two Tower Recommendation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erjia Chen, Bang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed extensive researches on developing two tower
recommendation models for relieving information overload. Four building modules
can be identified in such models, namely, user-item encoding, negative
sampling, loss computing and back-propagation updating. To the best of our
knowledge, existing algorithms have researched only on the first three modules,
yet neglecting the backpropagation module. They all adopt a kind of two
backpropagation strategy, which are based on an implicit assumption of equally
treating users and items in the training phase. In this paper, we challenge
such an equal training assumption and propose a novel one backpropagation
updating strategy, which keeps the normal gradient backpropagation for the item
encoding tower, but cuts off the backpropagation for the user encoding tower.
Instead, we propose a moving-aggregation updating strategy to update a user
encoding in each training epoch. Except the proposed backpropagation updating
module, we implement the other three modules with the most straightforward
choices. Experiments on four public datasets validate the effectiveness and
efficiency of our model in terms of improved recommendation performance and
reduced computation overload over the state-of-the-art competitors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can AI Models Appreciate Document Aesthetics? An Exploration of
  Legibility and Layout Quality in Relation to Prediction Confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiu-Wei Yang, Abhinav Agrawal, Pavlos Fragkogiannis, Shubham Nitin Mulay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A well-designed document communicates not only through its words but also
through its visual eloquence. Authors utilize aesthetic elements such as
colors, fonts, graphics, and layouts to shape the perception of information.
Thoughtful document design, informed by psychological insights, enhances both
the visual appeal and the comprehension of the content. While state-of-the-art
document AI models demonstrate the benefits of incorporating layout and image
data, it remains unclear whether the nuances of document aesthetics are
effectively captured. To bridge the gap between human cognition and AI
interpretation of aesthetic elements, we formulated hypotheses concerning AI
behavior in document understanding tasks, specifically anchored in document
design principles. With a focus on legibility and layout quality, we tested
four aspects of aesthetic effects: noise, font-size contrast, alignment, and
complexity, on model confidence using correlational analysis. The results and
observations highlight the value of model analysis rooted in document design
theories. Our work serves as a trailhead for further studies and we advocate
for continued research in this topic to deepen our understanding of how AI
interprets document aesthetics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval
  and Responsible Research Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neda Taghizadeh Serajeh, Iman Mohammadi, Vittorio Fuccella, Mattia De Rosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and accurate information extraction from scientific papers is
significant in the rapidly developing human-computer interaction research in
the literature review process. Our paper introduces and analyses a new
information retrieval system using state-of-the-art Large Language Models
(LLMs) in combination with structured text analysis techniques to extract
experimental data from HCI literature, emphasizing key elements. Then We
analyze the challenges and risks of using LLMs in the world of research. We
performed a comprehensive analysis on our conducted dataset, which contained
the specified information of 300 CHI 2020-2022 papers, to evaluate the
performance of the two large language models, GPT-3.5 (text-davinci-003) and
Llama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model
gains an accuracy of 58\% and a mean absolute error of 7.00. In contrast, the
Llama2 model indicates an accuracy of 56\% with a mean absolute error of 7.63.
The ability to answer questions was also included in the system in order to
work with streamlined data. By evaluating the risks and opportunities presented
by LLMs, our work contributes to the ongoing dialogue on establishing
methodological validity and ethical guidelines for LLM use in HCI data work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, CHI2024 Workshop on LLMs as Research Tools: Applications and
  Evaluations in HCI Data Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based IR systems. Yet, failures remain frequent, the models used
often being unable to retrieve documents relevant to the user's query. We
address this challenge by proposing a lightweight abstention mechanism tailored
for real-world constraints, with particular emphasis placed on the reranking
phase. We introduce a protocol for evaluating abstention strategies in a
black-box scenario, demonstrating their efficacy, and propose a simple yet
effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable Generation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06747v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06747v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wu, Jialiang Zhou, Ailong He, Shuguang Han, Jufeng Chen, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to business-to-consumer (B2C) e-commerce systems,
consumer-to-consumer (C2C) e-commerce platforms usually encounter the
limited-stock problem, that is, a product can only be sold one time in a C2C
system. This poses several unique challenges for click-through rate (CTR)
prediction. Due to limited user interactions for each product (i.e. item), the
corresponding item embedding in the CTR model may not easily converge. This
makes the conventional sequence modeling based approaches cannot effectively
utilize user history information since historical user behaviors contain a
mixture of items with different volume of stocks. Particularly, the attention
mechanism in a sequence model tends to assign higher score to products with
more accumulated user interactions, making limited-stock products being ignored
and contribute less to the final output. To this end, we propose the Meta-Split
Network (MSNet) to split user history sequence regarding to the volume of stock
for each product, and adopt differentiated modeling approaches for different
sequences. As for the limited-stock products, a meta-learning approach is
applied to address the problem of inconvergence, which is achieved by designing
meta scaling and shifting networks with ID and side information. In addition,
traditional approach can hardly update item embedding once the product is
consumed. Thereby, we propose an auxiliary loss that makes the parameters
updatable even when the product is no longer in distribution. To the best of
our knowledge, this is the first solution addressing the recommendation of
limited-stock product. Experimental results on the production dataset and
online A/B testing demonstrate the effectiveness of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WWW 2024. This work has already been deployed on the
  Xianyu platform in Alibaba. The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPR -- Efficient, large-scale log storage and retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Reichinger, Thomas Krismayer, Jan Rellermeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern, large scale monitoring systems have to process and store vast amounts
of log data in near real-time. At query time the systems have to find relevant
logs based on the content of the log message using support structures that can
scale to these amounts of data while still being efficient to use. We present
our novel Compressed Probabilistic Retrieval algorithm (COPR), capable of
answering Multi-Set Multi-Membership-Queries, that can be used as an
alternative to existing indexing structures for streamed log data. In our
experiments, COPR required up to 93% less storage space than the tested
state-of-the-art inverted index and had up to four orders of magnitude less
false-positives than the tested state-of-the-art membership sketch.
Additionally, COPR achieved up to 250 times higher query throughput than the
tested inverted index and up to 240 times higher query throughput than the
tested membership sketch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MA4DIV: Multi-Agent Reinforcement Learning for Search Result
  Diversification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong Ma, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Simiu Gu, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of search result diversification (SRD) is to ensure that
selected documents cover as many different subtopics as possible. Existing
methods primarily utilize a paradigm of "greedy selection", i.e., selecting one
document with the highest diversity score at a time. These approaches tend to
be inefficient and are easily trapped in a suboptimal state. In addition, some
other methods aim to approximately optimize the diversity metric, such as
$\alpha$-NDCG, but the results still remain suboptimal. To address these
challenges, we introduce Multi-Agent reinforcement learning (MARL) for search
result DIVersity, which called MA4DIV. In this approach, each document is an
agent and the search result diversification is modeled as a cooperative task
among multiple agents. This approach allows for directly optimizing the
diversity metrics, such as $\alpha$-NDCG, while achieving high training
efficiency. We conducted preliminary experiments on public TREC datasets to
demonstrate the effectiveness and potential of MA4DIV. Considering the limited
number of queries in public TREC datasets, we construct a large-scale dataset
from industry sources and show that MA4DIV achieves substantial improvements in
both effectiveness and efficiency than existing baselines on a industrial scale
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaT: Constraints as Terminations for Legged Locomotion Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Chane-Sane, Pierre-Alexandre Leziart, Thomas Flayols, Olivier Stasse, Philippe Souères, Nicolas Mansard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (RL) has demonstrated impressive results in
solving complex robotic tasks such as quadruped locomotion. Yet, current
solvers fail to produce efficient policies respecting hard constraints. In this
work, we advocate for integrating constraints into robot learning and present
Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing
from classical constrained RL formulations, we reformulate constraints through
stochastic terminations during policy learning: any violation of a constraint
triggers a probability of terminating potential future rewards the RL agent
could attain. We propose an algorithmic approach to this formulation, by
minimally modifying widely used off-the-shelf RL algorithms in robot learning
(such as Proximal Policy Optimization). Our approach leads to excellent
constraint adherence without introducing undue complexity and computational
overhead, thus mitigating barriers to broader adoption. Through empirical
evaluation on the real quadruped robot Solo crossing challenging obstacles, we
demonstrate that CaT provides a compelling solution for incorporating
constraints into RL frameworks. Videos and code are available at
https://constraints-as-terminations.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://constraints-as-terminations.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usage-Specific Survival Modeling Based on Operational Data and Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Mattias Krysander, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate predictions of when a component will fail are crucial when planning
maintenance, and by modeling the distribution of these failure times, survival
models have shown to be particularly useful in this context. The presented
methodology is based on conventional neural network-based survival models that
are trained using data that is continuously gathered and stored at specific
times, called snapshots. An important property of this type of training data is
that it can contain more than one snapshot from a specific individual which
results in that standard maximum likelihood training can not be directly
applied since the data is not independent. However, the papers show that if the
data is in a specific format where all snapshot times are the same for all
individuals, called homogeneously sampled, maximum likelihood training can be
applied and produce desirable results. In many cases, the data is not
homogeneously sampled and in this case, it is proposed to resample the data to
make it homogeneously sampled. How densely the dataset is sampled turns out to
be an important parameter; it should be chosen large enough to produce good
results, but this also increases the size of the dataset which makes training
slow. To reduce the number of samples needed during training, the paper also
proposes a technique to, instead of resampling the dataset once before the
training starts, randomly resample the dataset at the start of each epoch
during the training. The proposed methodology is evaluated on both a simulated
dataset and an experimental dataset of starter battery failures. The results
show that if the data is homogeneously sampled the methodology works as
intended and produces accurate survival models. The results also show that
randomly resampling the dataset on each epoch is an effective way to reduce the
size of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear model reduction for operator learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Eivazi, Stefan Wittek, Andreas Rausch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operator learning provides methods to approximate mappings between
infinite-dimensional function spaces. Deep operator networks (DeepONets) are a
notable architecture in this field. Recently, an extension of DeepONet based on
model reduction and neural networks, proper orthogonal decomposition
(POD)-DeepONet, has been able to outperform other architectures in terms of
accuracy for several benchmark tests. We extend this idea towards nonlinear
model order reduction by proposing an efficient framework that combines neural
networks with kernel principal component analysis (KPCA) for operator learning.
Our results demonstrate the superior performance of KPCA-DeepONet over
POD-DeepONet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a Tiny Paper at ICLR 2024 (Notable)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality Prediction Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Traffic Flow Prediction using Cellular Automata-based
  Model and CNN-LSTM architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Yang, Kshitij Jerath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have attempted to use deep learning to predict future states of
traffic flow, but have met with mixed results. These approaches face two key
challenges. First, training deep learning neural networks requires large
amounts of training data which are not yet easily available for traffic flow
systems. Second, even when data is available, the neural networks require
access to historical data that covers most possible traffic flow dynamics to
successfully predict future traffic states. Specifically, these deep learning
approaches do not fully leverage domain-knowledge about traffic flow dynamics,
despite a significant existing knowledge-base. In this work, we propose to
solve both issues using a Convolutional Neural Network (CNNs) with Long Short
Term Memory (LSTM) deep learning architecture to successfully predict traffic
flow, while leveraging a cellular automata-based statistical mechanics model of
traffic flow to generate training and test data. Another major contribution of
this paper is the insight that training data for a large traffic system can
actually be sampled from the simulations of a much smaller traffic system. This
is achieved through observing that the normalized energy distribution of the
statistical mechanics model is scale invariant, which significantly eases the
burden of data generation for large scale traffic systems. The resulting
simulations indicate good agreement between the predicted and the true traffic
flow dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Wasserstein Distances with Applications in Bayesian OT Flow
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannis Chemseddine, Paul Hagemann, Christian Wald, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In inverse problems, many conditional generative models approximate the
posterior measure by minimizing a distance between the joint measure and its
learned approximation. While this approach also controls the distance between
the posterior measures in the case of the Kullback--Leibler divergence, this is
in general not hold true for the Wasserstein distance. In this paper, we
introduce a conditional Wasserstein distance via a set of restricted couplings
that equals the expected Wasserstein distance of the posteriors. Interestingly,
the dual formulation of the conditional Wasserstein-1 flow resembles losses in
the conditional Wasserstein GAN literature in a quite natural way. We derive
theoretical properties of the conditional Wasserstein distance, characterize
the corresponding geodesics and velocity fields as well as the flow ODEs.
Subsequently, we propose to approximate the velocity fields by relaxing the
conditional Wasserstein distance. Based on this, we propose an extension of OT
Flow Matching for solving Bayesian inverse problems and demonstrate its
numerical advantages on an inverse problem and class-conditional image
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper supersedes arXiv:2310.13433</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fpga-Based Neural Thrust Controller for UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui, Christian Hochberger, Heinz Koepp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of unmanned aerial vehicles (UAVs) has improved a variety of
fields by providing a versatile, cost-effective and accessible platform for
implementing state-of-the-art algorithms. To accomplish a broader range of
tasks, there is a growing need for enhanced on-board computing to cope with
increasing complexity and dynamic environmental conditions. Recent advances
have seen the application of Deep Neural Networks (DNNs), particularly in
combination with Reinforcement Learning (RL), to improve the adaptability and
performance of UAVs, especially in unknown environments. However, the
computational requirements of DNNs pose a challenge to the limited computing
resources available on many UAVs. This work explores the use of Field
Programmable Gate Arrays (FPGAs) as a viable solution to this challenge,
offering flexibility, high performance, energy and time efficiency. We propose
a novel hardware board equipped with an Artix-7 FPGA for a popular open-source
micro-UAV platform. We successfully validate its functionality by implementing
an RL-based low-level controller using real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning with Orthonormal Anchors (CLOA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InceptionTime vs. Wavelet -- A comparison for time series classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Klenkert, Daniel Schaeffer, Julian Stauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks were used to classify infrasound data. Two different
approaches were compared. One based on the direct classification of time series
data, using a custom implementation of the InceptionTime network. For the other
approach, we generated 2D images of the wavelet transformation of the signals,
which were subsequently classified using a ResNet implementation. Choosing
appropriate hyperparameter settings, both achieve a classification accuracy of
above 90 %, with the direct approach reaching 95.2 %.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representatividad Muestral en la Incertidumbre Simétrica Multivariada
  para la Selección de Atributos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Sosa-Cabrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we analyze the behavior of the multivariate symmetric
uncertainty (MSU) measure through the use of statistical simulation techniques
under various mixes of informative and non-informative randomly generated
features. Experiments show how the number of attributes, their cardinalities,
and the sample size affect the MSU. In this thesis, through observation of
results, it is proposed an heuristic condition that preserves good quality in
the MSU under different combinations of these three factors, providing a new
useful criterion to help drive the process of dimension reduction.
  --
  En el presente trabajo hemos analizado el comportamiento de una versi\'on
multivariada de la incertidumbre sim\'etrica a trav\'es de t\'ecnicas de
simulaci\'on estad\'isticas sobre varias combinaciones de atributos
informativos y no-informativos generados de forma aleatoria. Los experimentos
muestran como el n\'umero de atributos, sus cardinalidades y el tama\~no
muestral afectan al MSU como medida. En esta tesis, mediante la observaci\'on
de resultados hemos propuesto una condici\'on que preserva una buena calidad en
el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual
provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\'on
de dimensionalidad.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, in Spanish. Advisors: Miguel Garc\'ia-Torres, Santiago
  G\'omez-Guerrero, Christian E. Schaerer Serra</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Contrastive Learning with <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Samsung/NL-ITI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Checking Beyond Training Set <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming for Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Eini Porat, Danny Eytan, Uri Shalit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, AMIA Informatics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network-Based Piecewise Survival Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Erik Frisk, Mattias Krysander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a family of neural network-based survival models is presented.
The models are specified based on piecewise definitions of the hazard function
and the density function on a partitioning of the time; both constant and
linear piecewise definitions are presented, resulting in a family of four
models. The models can be seen as an extension of the commonly used
discrete-time and piecewise exponential models and thereby add flexibility to
this set of standard models. Using a simulated dataset the models are shown to
perform well compared to the highly expressive, state-of-the-art energy-based
model, while only requiring a fraction of the computation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s-based architectures for stroke segmentation: A <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke remains a significant global health concern, necessitating precise and
efficient diagnostic tools for timely intervention and improved patient
outcomes. The emergence of deep learning methodologies has transformed the
landscape of medical image analysis. Recently, Transformers, initially designed
for natural language processing, have exhibited remarkable capabilities in
various computer vision applications, including medical image analysis. This
comprehensive review aims to provide an in-depth exploration of the
cutting-edge Transformer-based architectures applied in the context of stroke
segmentation. It commences with an exploration of stroke pathology, imaging
modalities, and the challenges associated with accurate diagnosis and
segmentation. Subsequently, the review delves into the fundamental ideas of
Transformers, offering detailed insights into their architectural intricacies
and the underlying mechanisms that empower them to effectively capture complex
spatial information within medical images. The existing literature is
systematically categorized and analyzed, discussing various approaches that
leverage Transformers for stroke segmentation. A critical assessment is
provided, highlighting the strengths and limitations of these methods,
including considerations of performance and computational efficiency.
Additionally, this review explores potential avenues for future research and
development
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusion approaches for emotion recognition from speech using acoustic and
  text-based features <span class="chip">ICASSP 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Pepino, Pablo Riera, Luciana Ferrer, Agustin Gravano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study different approaches for classifying emotions from
speech using acoustic and text-based features. We propose to obtain
contextualized word embeddings with BERT to represent the information contained
in speech transcriptions and show that this results in better performance than
using Glove embeddings. We also propose and compare different strategies to
combine the audio and text modalities, evaluating them on IEMOCAP and
MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is
beneficial on both datasets, though only subtle differences are observed across
the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect
that the criteria used to define the cross-validation folds have on results. In
particular, the standard way of creating folds for this dataset results in a
highly optimistic estimation of performance for the text-based system,
suggesting that some previous works may overestimate the advantage of
incorporating transcriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages. Accepted in ICASSP 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First Experiences with the Identification of People at Risk for Diabetes
  in Argentina using Machine Learning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enzo Rucci, Gonzalo Tittarelli, Franco Ronchetti, Jorge F. Elgart, Laura Lanzarini, Juan José Gagliardino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for
medicine due to the absence of pathogenic symptoms and the lack of known
associated risk factors. Even though some proposals for machine learning models
enable the identification of people at risk, the nature of the condition makes
it so that a model suitable for one population may not necessarily be suitable
for another. In this article, the development and assessment of predictive
models to identify people at risk for T2D and PD specifically in Argentina are
discussed. First, the database was thoroughly preprocessed and three specific
datasets were generated considering a compromise between the number of records
and the amount of available variables. After applying 5 different
classification models, the results obtained show that a very good performance
was observed for two datasets with some of these models. In particular, RF, DT,
and ANN demonstrated great classification power, with good values for the
metrics under consideration. Given the lack of this type of tool in Argentina,
this work represents the first step towards the development of more
sophisticated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computer Science - CACIC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Lipschitz Estimation for CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf Sulehman, Tingting Mu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the Lipschitz constant of deep neural networks is of growing
interest as it is useful for informing on generalisability and adversarial
robustness. Convolutional neural networks (CNNs) in particular, underpin much
of the recent success in computer vision related applications. However,
although existing methods for estimating the Lipschitz constant can be tight,
they have limited scalability when applied to CNNs. To tackle this, we propose
a novel method to accelerate Lipschitz constant estimation for CNNs. The core
idea is to divide a large convolutional block via a joint layer and width-wise
partition, into a collection of smaller blocks. We prove an upper-bound on the
Lipschitz constant of the larger block in terms of the Lipschitz constants of
the smaller blocks. Through varying the partition factor, the resulting method
can be adjusted to prioritise either accuracy or scalability and permits
parallelisation. We demonstrate an enhanced scalability and comparable accuracy
to existing baselines through a range of experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Peridynamic Neural Operators: Discover Biotissue
  Constitutive Law and Microstructure From Digital Image Correlation
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siavash Jafarzadeh, Stewart Silling, Lu Zhang, Colton Ross, Chung-Hao Lee, S. M. Rakibur Rahman, Shuodao Wang, Yue Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human tissues are highly organized structures with specific collagen fiber
arrangements varying from point to point. The effects of such heterogeneity
play an important role for tissue function, and hence it is of critical to
discover and understand the distribution of such fiber orientations from
experimental measurements, such as the digital image correlation data. To this
end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)
approach, for data-driven constitutive modeling of heterogeneous anisotropic
materials. The goal is to learn both a nonlocal constitutive law together with
the material microstructure, in the form of a heterogeneous fiber orientation
field, from loading field-displacement field measurements. To this end, we
propose a two-phase learning approach. Firstly, we learn a homogeneous
constitutive law in the form of a neural network-based kernel function and a
nonlocal bond force, to capture complex homogeneous material responses from
data. Then, in the second phase we reinitialize the learnt bond force and the
kernel function, and training them together with a fiber orientation field for
each material point. Owing to the state-based peridynamic skeleton, our
HeteroPNO-learned material models are objective and have the balance of linear
and angular momentum guaranteed. Moreover, the effects from heterogeneity and
nonlinear constitutive relationship are captured by the kernel function and the
bond force respectively, enabling physical interpretability. As a result, our
HeteroPNO architecture can learn a constitutive model for a biological tissue
with anisotropic heterogeneous response undergoing large deformation regime.
Moreover, the framework is capable to provide displacement and stress field
predictions for new and unseen loading instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency
  Attacks in Computer Vision <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Müller, Erwin Quiring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resource efficiency plays an important role for machine learning nowadays.
The energy and decision latency are two critical aspects to ensure a
sustainable and practical application. Unfortunately, the energy consumption
and decision latency are not robust against adversaries. Researchers have
recently demonstrated that attackers can compute and submit so-called sponge
examples at inference time to increase the energy consumption and decision
latency of neural networks. In computer vision, the proposed strategy crafts
inputs with less activation sparsity which could otherwise be used to
accelerate the computation. In this paper, we analyze the mechanism how these
energy-latency attacks reduce activation sparsity. In particular, we find that
input uniformity is a key enabler. A uniform image, that is, an image with
mostly flat, uniformly colored surfaces, triggers more activations due to a
specific interplay of convolution, batch normalization, and ReLU activation.
Based on these insights, we propose two new simple, yet effective strategies
for crafting sponge examples: sampling images from a probability distribution
and identifying dense, yet inconspicuous inputs in natural datasets. We
empirically examine our findings in a comprehensive evaluation with multiple
image classification models and show that our attack achieves the same sparsity
effect as prior sponge-example methods, but at a fraction of computation
effort. We also show that our sponge examples transfer between different neural
networks. Finally, we discuss applications of our findings for the good by
improving efficiency by increasing sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the DLSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One flow to correct them all: improving simulations in high-energy
  physics with a single normalising flow and a switch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caio Cesar Daumann, Mauro Donega, Johannes Erdmann, Massimiliano Galli, Jan Lukas Späh, Davide Valsecchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulated events are key ingredients in almost all high-energy physics
analyses. However, imperfections in the simulation can lead to sizeable
differences between the observed data and simulated events. The effects of such
mismodelling on relevant observables must be corrected either effectively via
scale factors, with weights or by modifying the distributions of the
observables and their correlations. We introduce a correction method that
transforms one multidimensional distribution (simulation) into another one
(data) using a simple architecture based on a single normalising flow with a
boolean condition. We demonstrate the effectiveness of the method on a
physics-inspired toy dataset with non-trivial mismodelling of several
observables and their correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Optimizing Hyperparameters for Quantum Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabrina Herbst, Vincenzo De Maio, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing capabilities of Machine Learning (ML) models go hand in hand
with an immense amount of data and computational power required for training.
Therefore, training is usually outsourced into HPC facilities, where we have
started to experience limits in scaling conventional HPC hardware, as theorized
by Moore's law. Despite heavy parallelization and optimization efforts, current
state-of-the-art ML models require weeks for training, which is associated with
an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum
Machine Learning (QML), can offer significant theoretical speed-ups and
enhanced expressive power. However, training QML models requires tuning various
hyperparameters, which is a nontrivial task and suboptimal choices can highly
affect the trainability and performance of the models. In this study, we
identify the most impactful hyperparameters and collect data about the
performance of QML models. We compare different configurations and provide
researchers with performance data and concrete suggestions for hyperparameter
selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteinGen: Generating Fidelitous and Diverse Graph Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gesine Reinert, Wenkai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating graphs that preserve characteristic structures while promoting
sample diversity can be challenging, especially when the number of graph
observations is small. Here, we tackle the problem of graph generation from
only one observed graph. The classical approach of graph generation from
parametric models relies on the estimation of parameters, which can be
inconsistent or expensive to compute due to intractable normalisation
constants. Generative modelling based on machine learning techniques to
generate high-quality graph samples avoids parameter estimation but usually
requires abundant training samples. Our proposed generating procedure,
SteinGen, which is phrased in the setting of graphs as realisations of
exponential random graph models, combines ideas from Stein's method and MCMC by
employing Markovian dynamics which are based on a Stein operator for the target
model. SteinGen uses the Glauber dynamics associated with an estimated Stein
operator to generate a sample, and re-estimates the Stein operator from the
sample after every sampling step. We show that on a class of exponential random
graph models this novel "estimation and re-estimation" generation strategy
yields high distributional similarity (high fidelity) to the original data,
combined with high sample diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Graph Neural Networks for Water Distribution Systems <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise-Robust Keyword Spotting through <span class="highlight-title">Self-supervised</span> <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Mørk, Holger Severin Bovbjerg, Gergely Kiss, Zheng-Hua Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants are now widely available, and to activate them a keyword
spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using
supervised learning methods and require a large amount of labelled data to
achieve a good performance. Leveraging unlabelled data through self-supervised
learning (SSL) has been shown to increase the accuracy in clean conditions.
This paper explores how SSL pretraining such as Data2Vec can be used to enhance
the robustness of KWS models in noisy conditions, which is under-explored.
  Models of three different sizes are pretrained using different pretraining
approaches and then fine-tuned for KWS. These models are then tested and
compared to models trained using two baseline supervised learning methods, one
being standard training using clean data and the other one being multi-style
training (MTR). The results show that pretraining and fine-tuning on clean data
is superior to supervised learning on clean data across all testing conditions,
and superior to supervised MTR for testing conditions of SNR above 5 dB. This
indicates that pretraining alone can increase the model's robustness. Finally,
it is found that using noisy data for pretraining models, especially with the
Data2Vec-denoising approach, significantly enhances the robustness of KWS
models in noisy conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-aware semantic relevance predicting Chinese sentence reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several influential computational models and metrics have
been proposed to predict how humans comprehend and process sentence. One
particularly promising approach is contextual semantic similarity. Inspired by
the attention algorithm in Transformer and human memory mechanisms, this study
proposes an ``attention-aware'' approach for computing contextual semantic
relevance. This new approach takes into account the different contributions of
contextual parts and the expectation effect, allowing it to incorporate
contextual information fully. The attention-aware approach also facilitates the
simulation of existing reading models and evaluate them. The resulting
``attention-aware'' metrics of semantic relevance can more accurately predict
fixation durations in Chinese reading tasks recorded in an eye-tracking corpus
than those calculated by existing approaches. The study's findings further
provide strong support for the presence of semantic preview benefits in Chinese
naturalistic reading. Furthermore, the attention-aware metrics of semantic
relevance, being memory-based, possess high interpretability from both
linguistic and cognitive standpoints, making them a valuable computational tool
for modeling eye-movements in reading and further gaining insight into the
process of language comprehension. Our approach underscores the potential of
these metrics to advance our comprehension of how humans understand and process
language, ultimately leading to a better understanding of language
comprehension and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ skscope: Fast Sparsity-Constrained Optimization in Python 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhi Wang, Jin Zhu, Peng Chen, Huiyang Peng, Xiaoke Zhang, Anran Wang, Yu Zheng, Junxian Zhu, Xueqin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying iterative solvers on sparsity-constrained optimization (SCO)
requires tedious mathematical deduction and careful programming/debugging that
hinders these solvers' broad impact. In the paper, the library skscope is
introduced to overcome such an obstacle. With skscope, users can solve the SCO
by just programming the objective function. The convenience of skscope is
demonstrated through two examples in the paper, where sparse linear regression
and trend filtering are addressed with just four lines of code. More
importantly, skscope's efficient implementation allows state-of-the-art solvers
to quickly attain the sparse solution regardless of the high dimensionality of
parameter space. Numerical experiments reveal the available solvers in skscope
can achieve up to 80x speedup on the competing relaxation solutions obtained
via the benchmarked convex solver. skscope is published on the Python Package
Index (PyPI) and Conda, and its source code is available at:
https://github.com/abess-team/skscope.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Robust Reinforcement-Learning: Principles and Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Yamagata, Raul Santos-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has shown remarkable success in solving
relatively complex tasks, yet the deployment of RL systems in real-world
scenarios poses significant challenges related to safety and robustness. This
paper aims to identify and further understand those challenges thorough the
exploration of the main dimensions of the safe and robust RL landscape,
encompassing algorithmic, ethical, and practical considerations. We conduct a
comprehensive review of methodologies and open problems that summarizes the
efforts in recent years to address the inherent risks associated with RL
applications.
  After discussing and proposing definitions for both safe and robust RL, the
paper categorizes existing research works into different algorithmic approaches
that enhance the safety and robustness of RL agents. We examine techniques such
as uncertainty estimation, optimisation methodologies, exploration-exploitation
trade-offs, and adversarial training. Environmental factors, including
sim-to-real transfer and domain adaptation, are also scrutinized to understand
how RL systems can adapt to diverse and dynamic surroundings. Moreover, human
involvement is an integral ingredient of the analysis, acknowledging the broad
set of roles that humans can take in this context.
  Importantly, to aid practitioners in navigating the complexities of safe and
robust RL implementation, this paper introduces a practical checklist derived
from the synthesized literature. The checklist encompasses critical aspects of
algorithm design, training environment considerations, and ethical guidelines.
It will serve as a resource for developers and policymakers alike to ensure the
responsible deployment of RL systems in many application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs <span class="chip">ICME2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies reveal a significant theoretical link between variational
autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to
estimate the theoretical upper bound of the information rate-distortion
function of images. Such estimated theoretical bounds substantially exceed the
performance of existing neural image codecs (NICs). To narrow this gap, we
propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The
proposed BG-VAE leverages the theoretical bound to guide the NIC model towards
enhanced performance. We implement the BG-VAE using Hierarchical VAEs and
demonstrate its effectiveness through extensive experiments. Along with
advanced neural network blocks, we provide a versatile, variable-rate NIC that
outperforms existing methods when considering both rate-distortion performance
and computational complexity. The code is available at BG-VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Multimedia and Expo (ICME2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Line Search Methods for Large Scale Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Algorithms for Regularized Nonnegative Scale-invariant
  Low-rank Approximation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy E. Cohen, Valentin Leplat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularized nonnegative low-rank approximations such as sparse Nonnegative
Matrix Factorization or sparse Nonnegative Tucker Decomposition are an
important branch of dimensionality reduction models with enhanced
interpretability. However, from a practical perspective, the choice of
regularizers and regularization coefficients, as well as the design of
efficient algorithms, is challenging because of the multifactor nature of these
models and the lack of theory to back these choices. This paper aims at
improving upon these issues. By studying a more general model called the
Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance
inherent to low-rank approximation models causes an implicit regularization
with both unexpected beneficial and detrimental effects. This observation
allows to better understand the effect of regularization functions in low-rank
approximation models, to guide the choice of the regularization
hyperparameters, and to design balancing strategies to enhance the convergence
speed of dedicated optimization algorithms. Some of these results were already
known but restricted to specific instances of regularized low-rank
approximations. We also derive a generic Majorization Minimization algorithm
that handles many regularized nonnegative low-rank approximations, with
convergence guarantees. We showcase our contributions on sparse Nonnegative
Matrix Factorization, ridge-regularized Canonical Polyadic decomposition and
sparse Nonnegative Tucker Decomposition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection
  of Pathological Pulmonary CT scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aissam Djahnine, Alexandre Popoff, Emilien Jupin-Delevaux, Vincent Cottin, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised pathology detection can be implemented by training a model on
healthy data only and measuring the deviation from the training set upon
inference, for example with CNN-based feature extraction and one-class
classifiers, or reconstruction-score-based methods such as AEs, GANs and
Diffusion models. Normalizing Flows (NF) have the ability to directly learn the
probability distribution of training examples through an invertible
architecture. We leverage this property in a novel 3D NF-based model named
CT-3DFlow, specifically tailored for patient-level pulmonary pathology
detection in chest CT data. Our model is trained unsupervised on healthy 3D
pulmonary CT patches, and detects deviations from its log-likelihood
distribution as anomalies. We aggregate patches-level likelihood values from a
patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.
Out-of-distribution detection performance is evaluated using expert annotations
on a separate chest CT test dataset, outperforming other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Maximum Consensus over Noisy Links 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Naveen K. D. Venkategowda, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a distributed algorithm, termed noise-robust distributed maximum
consensus (RD-MC), for estimating the maximum value within a multi-agent
network in the presence of noisy communication links. Our approach entails
redefining the maximum consensus problem as a distributed optimization problem,
allowing a solution using the alternating direction method of multipliers.
Unlike existing algorithms that rely on multiple sets of noise-corrupted
estimates, RD-MC employs a single set, enhancing both robustness and
efficiency. To further mitigate the effects of link noise and improve
robustness, we apply moving averaging to the local estimates. Through extensive
simulations, we demonstrate that RD-MC is significantly more robust to
communication link noise compared to existing maximum-consensus algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence for <span class="highlight-title">Transformer</span> Fine-tuning with Line Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct mineral content prediction from drill core images via transfer
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romana Boiger, Sergey V. Churakov, Ignacio Ballester Llagaria, Georg Kosakowski, Raphael Wüst, Nikolaos I. Prasianakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep subsurface exploration is important for mining, oil and gas industries,
as well as in the assessment of geological units for the disposal of chemical
or nuclear waste, or the viability of geothermal energy systems. Typically,
detailed examinations of subsurface formations or units are performed on
cuttings or core materials extracted during drilling campaigns, as well as on
geophysical borehole data, which provide detailed information about the
petrophysical properties of the rocks. Depending on the volume of rock samples
and the analytical program, the laboratory analysis and diagnostics can be very
time-consuming. This study investigates the potential of utilizing machine
learning, specifically convolutional neural networks (CNN), to assess the
lithology and mineral content solely from analysis of drill core images, aiming
to support and expedite the subsurface geological exploration. The paper
outlines a comprehensive methodology, encompassing data preprocessing, machine
learning methods, and transfer learning techniques. The outcome reveals a
remarkable 96.7% accuracy in the classification of drill core segments into
distinct formation classes. Furthermore, a CNN model was trained for the
evaluation of mineral content using a learning data set from multidimensional
log analysis data (silicate, total clay, carbonate). When benchmarked against
laboratory XRD measurements on samples from the cores, both the advanced
multidimensional log analysis model and the neural network approach developed
here provide equally good performance. This work demonstrates that deep
learning and particularly transfer learning can support extracting
petrophysical properties, including mineral content and formation
classification, from drill core images, thus offering a road map for enhancing
model performance and data set quality in image-based analysis of drill cores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning in PINNs: Phase transition, total diffusion, and generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the learning dynamics of fully-connected neural networks
through the lens of gradient signal-to-noise ratio (SNR), examining the
behavior of first-order optimizers like Adam in non-convex objectives. By
interpreting the drift/diffusion phases in the information bottleneck theory,
focusing on gradient homogeneity, we identify a third phase termed ``total
diffusion", characterized by equilibrium in the learning rates and homogeneous
gradients. This phase is marked by an abrupt SNR increase, uniform residuals
across the sample space and the most rapid training convergence. We propose a
residual-based re-weighting scheme to accelerate this diffusion in quadratic
loss functions, enhancing generalization. We also explore the information
compression phenomenon, pinpointing a significant saturation-induced
compression of activations at the total diffusion phase, with deeper layers
experiencing negligible information loss. Supported by experimental data on
physics-informed neural networks (PINNs), which underscore the importance of
gradient homogeneity due to their PDE-based sample inter-dependence, our
findings suggest that recognizing phase transitions could refine ML
optimization strategies for improved generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Employing Weather Forecast Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Vaz, Gabriela Schütz, Carlos Guerrero, Pedro J. S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Young-Jae Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are five types of trajectory prediction tasks: deterministic,
stochastic, domain adaptation, momentary observation, and few-shot. These
associated tasks are defined by various factors, such as the length of input
paths, data split and pre-processing methods. Interestingly, even though they
commonly take sequential coordinates of observations as input and infer future
paths in the same coordinates as output, designing specialized architectures
for each task is still necessary. For the other task, generality issues can
lead to sub-optimal performances. In this paper, we propose SingularTrajectory,
a diffusion-based universal trajectory prediction framework to reduce the
performance gap across the five tasks. The core of SingularTrajectory is to
unify a variety of human dynamics representations on the associated tasks. To
do this, we first build a Singular space to project all types of motion
patterns from each task into one embedding space. We next propose an adaptive
anchor working in the Singular space. Unlike traditional fixed anchor methods
that sometimes yield unacceptable paths, our adaptive anchor enables correct
anchors, which are put into a wrong location, based on a traversability map.
Finally, we adopt a diffusion-based predictor to further enhance the prototype
paths using a cascaded denoising process. Our unified framework ensures the
generality across various benchmark settings such as input modality, and
trajectory lengths. Extensive experiments on five public benchmarks demonstrate
that SingularTrajectory substantially outperforms existing models, highlighting
its effectiveness in estimating general dynamics of human movements. Code is
publicly available at https://github.com/inhwanbae/SingularTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRESCO: Federated Reinforcement Energy System for Cooperative
  Optimization <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Mauricio Cuadrado, Roberto Alejandro Gutierrez, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in renewable energy is creating new dynamics in the energy grid that
promise to create a cleaner and more participative energy grid, where
technology plays a crucial part in making the required flexibility to achieve
the vision of the next-generation grid. This work presents FRESCO, a framework
that aims to ease the implementation of energy markets using a hierarchical
control architecture of reinforcement learning agents trained using federated
learning. The core concept we are proving is that having greedy agents subject
to changing conditions from a higher level agent creates a cooperative setup
that will allow for fulfilling all the individual objectives. This paper
presents a general overview of the framework, the current progress, and some
insights we obtained from the recent results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tiny Paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Policy Learning for Smart Grids: FL TRPO Approach <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Li, Nicolas Mauricio Cuadrado, Samuel Horváth, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The smart grid domain requires bolstering the capabilities of existing energy
management systems; Federated Learning (FL) aligns with this goal as it
demonstrates a remarkable ability to train models on heterogeneous datasets
while maintaining data privacy, making it suitable for smart grid applications,
which often involve disparate data distributions and interdependencies among
features that hinder the suitability of linear models. This paper introduces a
framework that combines FL with a Trust Region Policy Optimization (FL TRPO)
aiming to reduce energy-associated emissions and costs. Our approach reveals
latent interconnections and employs personalized encoding methods to capture
unique insights, understanding the relationships between features and optimal
strategies, allowing our model to generalize to previously unseen data.
Experimental results validate the robustness of our approach, affirming its
proficiency in effectively learning policy models for smart grid challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Workshop: Tackling Climate Change with Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Vegetation Modeling with <span class="highlight-title">Pre-Train</span>ed Weather <span class="highlight-title">Transformer</span>s <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Janetzky, Florian Gallusser, Simon Hentschel, Andreas Hotho, Anna Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate vegetation models can produce further insights into the complex
interaction between vegetation activity and ecosystem processes. Previous
research has established that long-term trends and short-term variability of
temperature and precipitation affect vegetation activity. Motivated by the
recent success of Transformer-based Deep Learning models for medium-range
weather forecasting, we adapt the publicly available pre-trained FourCastNet to
model vegetation activity while accounting for the short-term dynamics of
climate variability. We investigate how the learned global representation of
the atmosphere's state can be transferred to model the normalized difference
vegetation index (NDVI). Our model globally estimates vegetation activity at a
resolution of \SI{0.25}{\degree} while relying only on meteorological data. We
demonstrate that leveraging pre-trained weather models improves the NDVI
estimates compared to learning an NDVI model from scratch. Additionally, we
compare our results to other recent data-driven NDVI modeling approaches from
machine learning and ecology literature. We further provide experimental
evidence on how much data and training time is necessary to turn FourCastNet
into an effective vegetation model. Code and models will be made available upon
publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tackling Climate Change with Machine Learning Workshop @ ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Active Learning in Conditional Trust Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate collaborative active learning, a paradigm in
which multiple collaborators explore a new domain by leveraging their combined
machine learning capabilities without disclosing their existing data and
models. Instead, the collaborators share prediction results from the new domain
and newly acquired labels. This collaboration offers several advantages: (a) it
addresses privacy and security concerns by eliminating the need for direct
model and data disclosure; (b) it enables the use of different data sources and
insights without direct data exchange; and (c) it promotes cost-effectiveness
and resource efficiency through shared labeling costs. To realize these
benefits, we introduce a collaborative active learning framework designed to
fulfill the aforementioned objectives. We validate the effectiveness of the
proposed framework through simulations. The results demonstrate that
collaboration leads to higher AUC scores compared to independent efforts,
highlighting the framework's ability to overcome the limitations of individual
models. These findings support the use of collaborative approaches in active
learning, emphasizing their potential to enhance outcomes through collective
expertise and shared resources. Our work provides a foundation for further
research on collaborative active learning and its practical applications in
various domains where data privacy, cost efficiency, and model performance are
critical considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 9 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemRoDe: Macro Adversarial Training to Learn Representations That are
  Robust to Word-Level Attacks <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are indispensable tools for natural language processing
tasks, but their vulnerability to adversarial attacks remains a concern. While
current research has explored adversarial training techniques, their
improvements to defend against word-level attacks have been limited. In this
work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a
Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing
inspiration from recent studies in the image domain, we investigate and later
confirm that in a discrete data setting such as language, adversarial samples
generated via word substitutions do indeed belong to an adversarial domain
exhibiting a high Wasserstein distance from the base domain. Our method learns
a robust representation that bridges these two domains. We hypothesize that if
samples were not projected into an adversarial domain, but instead to a domain
with minimal shift, it would improve attack robustness. We align the domains by
incorporating a new distance-based objective. With this, our model is able to
learn more generalized representations by aligning the model's high-level
output features and therefore better handling unseen adversarial samples. This
method can be generalized across word embeddings, even when they share minimal
overlap at both vocabulary and word-substitution levels. To evaluate the
effectiveness of our approach, we conduct experiments on BERT and RoBERTa
models on three datasets. The results demonstrate promising state-of-the-art
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NAACL 2024 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Topos of <span class="highlight-title">Transformer</span> Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Jacopo Villani, Peter McBurney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer neural network has significantly out-shined all other neural
network architectures as the engine behind large language models. We provide a
theoretical analysis of the expressivity of the transformer architecture
through the lens of topos theory. From this viewpoint, we show that many common
neural network architectures, such as the convolutional, recurrent and graph
convolutional networks, can be embedded in a pretopos of piecewise-linear
functions, but that the transformer necessarily lives in its topos completion.
In particular, this suggests that the two network families instantiate
different fragments of logic: the former are first order, whereas transformers
are higher-order reasoners. Furthermore, we draw parallels with architecture
search and gradient descent, integrating our analysis in the framework of
cybernetic agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Spectrogram Analysis in a Multiple Classifier Fusion Framework for
  Power Grid Classification Using Electric Network Frequency <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Tzolopoulos, Christos Korgialas, Constantine Kotropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Electric Network Frequency (ENF) serves as a unique signature inherent to
power distribution systems. Here, a novel approach for power grid
classification is developed, leveraging ENF. Spectrograms are generated from
audio and power recordings across different grids, revealing distinctive ENF
patterns that aid in grid classification through a fusion of classifiers. Four
traditional machine learning classifiers plus a Convolutional Neural Network
(CNN), optimized using Neural Architecture Search, are developed for One-vs-All
classification. This process generates numerous predictions per sample, which
are then compiled and used to train a shallow multi-label neural network
specifically designed to model the fusion process, ultimately leading to the
conclusive class prediction for each sample. Experimental findings reveal that
both validation and testing accuracy outperform those of current
state-of-the-art classifiers, underlining the effectiveness and robustness of
the proposed methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13th International Conference on Pattern Recognition Applications and
  Methods (ICPRAM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-based Graph Learning with Consistency and Specificity for
  Multi-view Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Shi, Lei Cao, Yunshan Ye, Yu Zhao, Badong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning is widely recognized as a crucial technique in multi-view
clustering. Existing graph learning methods typically involve constructing an
adaptive neighbor graph based on probabilistic neighbors and then learning a
consensus graph to for clustering, however, they are confronted with two
limitations. Firstly, they often rely on Euclidean distance to measure
similarity when constructing the adaptive neighbor graph, which proves
inadequate in capturing the intrinsic structure among data points in many
real-world scenarios. Secondly, most of these methods focus solely on consensus
graph, ignoring view-specific graph information. In response to the
aforementioned drawbacks, we in this paper propose a novel tensor-based graph
learning framework that simultaneously considers consistency and specificity
for multi-view clustering. Specifically, we calculate the similarity distance
on the Stiefel manifold to preserve the intrinsic structure among data points.
By making an assumption that the learned neighbor graph of each view comprises
both a consistent graph and a view-specific graph, we formulate a new
tensor-based target graph learning paradigm. Owing to the benefits of tensor
singular value decomposition (t-SVD) in uncovering high-order correlations,
this model is capable of achieving a complete understanding of the target
graph. Furthermore, we develop an iterative algorithm to solve the proposed
objective optimization problem. Experiments conducted on real-world datasets
have demonstrated the superior performance of the proposed method over some
state-of-the-art multi-view clustering methods. The source code has been
released on https://github.com/lshi91/CSTGL-Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stragglers-Aware Low-Latency Synchronous Federated Learning via
  Layer-Wise Model Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Lang, Alejandro Cohen, Nir Shlezinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synchronous federated learning (FL) is a popular paradigm for collaborative
edge learning. It typically involves a set of heterogeneous devices locally
training neural network (NN) models in parallel with periodic centralized
aggregations. As some of the devices may have limited computational resources
and varying availability, FL latency is highly sensitive to stragglers.
Conventional approaches discard incomplete intra-model updates done by
stragglers, alter the amount of local workload and architecture, or resort to
asynchronous settings; which all affect the trained model performance under
tight training latency constraints. In this work, we propose straggler-aware
layer-wise federated learning (SALF) that leverages the optimization procedure
of NNs via backpropagation to update the global model in a layer-wise fashion.
SALF allows stragglers to synchronously convey partial gradients, having each
layer of the global model be updated independently with a different
contributing set of users. We provide a theoretical analysis, establishing
convergence guarantees for the global model under mild assumptions on the
distribution of the participating devices, revealing that SALF converges at the
same asymptotic rate as FL with no timing limitations. This insight is matched
with empirical observations, demonstrating the performance gains of SALF
compared to alternative mechanisms mitigating the device heterogeneity gap in
FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ship in Sight: Diffusion Models for Ship-Image Super Resolution <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, remarkable advancements have been achieved in the field of
image generation, primarily driven by the escalating demand for high-quality
outcomes across various image generation subtasks, such as inpainting,
denoising, and super resolution. A major effort is devoted to exploring the
application of super-resolution techniques to enhance the quality of
low-resolution images. In this context, our method explores in depth the
problem of ship image super resolution, which is crucial for coastal and port
surveillance. We investigate the opportunity given by the growing interest in
text-to-image diffusion models, taking advantage of the prior knowledge that
such foundation models have already learned. In particular, we present a
diffusion-model-based architecture that leverages text conditioning during
training while being class-aware, to best preserve the crucial details of the
ships during the generation of the super-resoluted image. Since the specificity
of this task and the scarcity availability of off-the-shelf data, we also
introduce a large labeled ship dataset scraped from online ship images, mostly
from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method
achieves more robust results than other deep learning models previously
employed for super resolution, as proven by the multiple experiments performed.
Moreover, we investigate how this model can benefit downstream tasks, such as
classification and object detection, thus emphasizing practical implementation
in a real-world scenario. Experimental results show flexibility, reliability,
and impressive performance of the proposed framework over state-of-the-art
methods for different tasks. The code is available at:
https://github.com/LuigiSigillo/ShipinSight .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 International Joint Conference on Neural Networks
  (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Multiple Kernel Learning approaches for multi-omics data
  integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitja Briscik, Gabriele Tazza, Marie-Agnes Dillies, László Vidács, Sébastien Dejean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in high-throughput technologies have originated an ever-increasing
availability of omics datasets. The integration of multiple heterogeneous data
sources is currently an issue for biology and bioinformatics. Multiple kernel
learning (MKL) has shown to be a flexible and valid approach to consider the
diverse nature of multi-omics inputs, despite being an underused tool in
genomic data mining.We provide novel MKL approaches based on different kernel
fusion strategies.To learn from the meta-kernel of input kernels, we
adaptedunsupervised integration algorithms for supervised tasks with support
vector machines.We also tested deep learning architectures for kernel fusion
and classification.The results show that MKL-based models can compete with more
complex, state-of-the-art, supervised multi-omics integrative approaches.
Multiple kernel learning offers a natural framework for predictive models in
multi-omics genomic data. Our results offer a direction for bio-data mining
research and further development of methods for heterogeneous data integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Artificial Neural Twin -- Process Optimization and Continual
  Learning in Distributed Process Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Emmert, Ronald Mendez, Houman Mirzaalian Dastjerdi, Christopher Syben, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial process optimization and control is crucial to increase economic
and ecologic efficiency. However, data sovereignty, differing goals, or the
required expert knowledge for implementation impede holistic implementation.
Further, the increasing use of data-driven AI-methods in process models and
industrial sensory often requires regular fine-tuning to accommodate
distribution drifts. We propose the Artificial Neural Twin, which combines
concepts from model predictive control, deep learning, and sensor networks to
address these issues. Our approach introduces differentiable data fusion to
estimate the state of distributed process steps and their dependence on input
data. By treating the interconnected process steps as a quasi neural-network,
we can backpropagate loss gradients for process optimization or model
fine-tuning to process parameters or AI models respectively. The concept is
demonstrated on a virtual machine park simulated in Unity, consisting of bulk
material processes in plastic recycling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Macroscale fracture surface segmentation via semi-supervised learning
  considering the structural similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Rosenberger, Johannes Tlatlik, Sebastian Münstermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To this date the safety assessment of materials, used for example in the
nuclear power sector, commonly relies on a fracture mechanical analysis
utilizing macroscopic concepts, where a global load quantity K or J is compared
to the materials fracture toughness curve. Part of the experimental effort
involved in these concepts is dedicated to the quantitative analysis of
fracture surfaces. Within the scope of this study a methodology for the
semi-supervised training of deep learning models for fracture surface
segmentation on a macroscopic level was established. Therefore, three distinct
and unique datasets were created to analyze the influence of structural
similarity on the segmentation capability. The structural similarity differs
due to the assessed materials and specimen, as well as imaging-induced variance
due to fluctuations in image acquisition in different laboratories. The
datasets correspond to typical isolated laboratory conditions, complex
real-world circumstances, and a curated subset of the two. We implemented a
weak-to-strong consistency regularization for semi-supervised learning. On the
heterogeneous dataset we were able to train robust and well-generalizing models
that learned feature representations from images across different domains
without observing a significant drop in prediction quality. Furthermore, our
approach reduced the number of labeled images required for training by a factor
of 6. To demonstrate the success of our method and the benefit of our approach
for the fracture mechanics assessment, we utilized the models for initial crack
size measurements with the area average method. For the laboratory setting, the
deep learning assisted measurements proved to have the same quality as manual
measurements. For models trained on the heterogeneous dataset, very good
measurement accuracies with mean deviations smaller than 1 % could be
achieved...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>During review title changed to: Deep learning based initial crack
  size measurements utilizing macroscale fracture surface segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Dataset</span> for Pharmacovigilance in German, French, and Japanese:
  Annotating Adverse Drug Reactions across Languages <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas Lavergne, Aurélie Névéol, Patrick Paroubek, Philippe Thomas, Tomohiro Nishiyama, Sebastian Möller, Eiji Aramaki, Yuji Matsumoto, Roland Roller, Pierre Zweigenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-generated data sources have gained significance in uncovering Adverse
Drug Reactions (ADRs), with an increasing number of discussions occurring in
the digital world. However, the existing clinical corpora predominantly revolve
around scientific articles in English. This work presents a multilingual corpus
of texts concerning ADRs gathered from diverse sources, including patient fora,
social media, and clinical reports in German, French, and Japanese. Our corpus
contains annotations covering 12 entity types, four attribute types, and 13
relation types. It contributes to the development of real-world multilingual
language models for healthcare. We provide statistics to highlight certain
challenges associated with the corpus and conduct preliminary experiments
resulting in strong baselines for extracting entities and relations between
these entities, both within and across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking-Assisted Object Detection with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin, Jia-Fong Yeh, Hung-Ting Su, Winston Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based object detection has recently garnered attention in the computer
vision community due to the exceptional properties of event cameras, such as
high dynamic range and no motion blur. However, feature asynchronism and
sparsity cause invisible objects due to no relative motion to the camera,
posing a significant challenge in the task. Prior works have studied various
memory mechanisms to preserve as many features as possible at the current time,
guided by temporal clues. While these implicit-learned memories retain some
short-term information, they still struggle to preserve long-term features
effectively. In this paper, we consider those invisible objects as
pseudo-occluded objects and aim to reveal their features. Firstly, we introduce
visibility attribute of objects and contribute an auto-labeling algorithm to
append additional visibility labels on an existing event camera dataset.
Secondly, we exploit tracking strategies for pseudo-occluded objects to
maintain their permanence and retain their bounding boxes, even when features
have not been available for a very long time. These strategies can be treated
as an explicit-learned memory guided by the tracking objective to record the
displacements of objects across frames. Lastly, we propose a spatio-temporal
feature aggregation module to enrich the latent features and a consistency loss
to increase the robustness of the overall pipeline. We conduct comprehensive
experiments to verify our method's effectiveness where still objects are
retained but real occluded objects are discarded. The results demonstrate that
(1) the additional visibility labels can assist in supervised training, and (2)
our method outperforms state-of-the-art approaches with a significant
improvement of 7.9% absolute mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Distributed Nonnegative Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonnegative matrix factorization (NMF) is an effective data representation
tool with numerous applications in signal processing and machine learning.
However, deploying NMF in a decentralized manner over ad-hoc networks
introduces privacy concerns due to the conventional approach of sharing raw
data among network agents. To address this, we propose a privacy-preserving
algorithm for fully-distributed NMF that decomposes a distributed large data
matrix into left and right matrix factors while safeguarding each agent's local
data privacy. It facilitates collaborative estimation of the left matrix factor
among agents and enables them to estimate their respective right factors
without exposing raw data. To ensure data privacy, we secure information
exchanges between neighboring agents utilizing the Paillier cryptosystem, a
probabilistic asymmetric algorithm for public-key cryptography that allows
computations on encrypted data without decryption. Simulation results conducted
on synthetic and real-world datasets demonstrate the effectiveness of the
proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Algorithms: A New Frontier in Financial Crime Prevention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham Itzhak Weinberg, Alessio Faccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial crimes fast proliferation and sophistication require novel
approaches that provide robust and effective solutions. This paper explores the
potential of quantum algorithms in combating financial crimes. It highlights
the advantages of quantum computing by examining traditional and Machine
Learning (ML) techniques alongside quantum approaches. The study showcases
advanced methodologies such as Quantum Machine Learning (QML) and Quantum
Artificial Intelligence (QAI) as powerful solutions for detecting and
preventing financial crimes, including money laundering, financial crime
detection, cryptocurrency attacks, and market manipulation. These quantum
approaches leverage the inherent computational capabilities of quantum
computers to overcome limitations faced by classical methods. Furthermore, the
paper illustrates how quantum computing can support enhanced financial risk
management analysis. Financial institutions can improve their ability to
identify and mitigate risks, leading to more robust risk management strategies
by exploiting the quantum advantage. This research underscores the
transformative impact of quantum algorithms on financial risk management. By
embracing quantum technologies, organisations can enhance their capabilities to
combat evolving threats and ensure the integrity and stability of financial
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of the Principal Component Analysis onto High-Performance
  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and
  Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Martel, R. Lazcano, J. Lopez, D. Madroñal, R. Salvador, S. Lopez, E. Juarez, R. Guerra, C. Sanz, R. Sarmiento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction represents a critical preprocessing step in order to
increase the efficiency and the performance of many hyperspectral imaging
algorithms. However, dimensionality reduction algorithms, such as the Principal
Component Analysis (PCA), suffer from their computationally demanding nature,
becoming advisable for their implementation onto high-performance computer
architectures for applications under strict latency constraints. This work
presents the implementation of the PCA algorithm onto two different
high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and
a Kalray manycore, uncovering a highly valuable set of tips and tricks in order
to take full advantage of the inherent parallelism of these high-performance
computing platforms, and hence, reducing the time that is required to process a
given hyperspectral image. Moreover, the achieved results obtained with
different hyperspectral images have been compared with the ones that were
obtained with a field programmable gate array (FPGA)-based implementation of
the PCA algorithm that has been recently published, providing, for the first
time in the literature, a comprehensive analysis in order to highlight the pros
and cons of each option.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Contrastive Learning for Online Clinical Time-Series
  Applications <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Baldenweg, Manuel Burger, Gunnar Rätsch, Rita Kuznetsova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)
contain a diverse set of data modalities. While prior works have successfully
leveraged multiple modalities in supervised settings, we apply advanced
self-supervised multi-modal contrastive learning techniques to ICU data,
specifically focusing on clinical notes and time-series for clinically relevant
online prediction tasks. We introduce a loss function Multi-Modal Neighborhood
Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the
excellent linear probe and zero-shot performance of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Workshop Paper at TS4H@ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using
  SDO/HMI Data and an Attention-Aided Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhui Xu, Jason T. L. Wang, Haimin Wang, Haodi Jiang, Qin Li, Yasser Abduallah, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image super-resolution has been an important subject in image processing and
recognition. Here, we present an attention-aided convolutional neural network
(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to
enhance the quality of line-of-sight (LOS) magnetograms of solar active regions
(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and
Heliospheric Observatory (SOHO). The ground-truth labels used for training
SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic
Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist
of strong magnetic fields in which magnetic energy can suddenly be released to
produce extreme space weather events, such as solar flares, coronal mass
ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which
is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI
magnetograms allow for better understanding and forecasting of violent events
of space weather. Experimental results show that SolarCNN improves the quality
of SOHO/MDI magnetograms in terms of the structural similarity index measure
(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise
ratio (PSNR).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic
  Communication Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Change Sign Detection by Fusing Mixture Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Urano, Ryo Yuki, Kenji Yamanishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an early detection method for cluster structural changes.
Cluster structure refers to discrete structural characteristics, such as the
number of clusters, when data are represented using finite mixture models, such
as Gaussian mixture models. We focused on scenarios in which the cluster
structure gradually changed over time. For finite mixture models, the concept
of mixture complexity (MC) measures the continuous cluster size by considering
the cluster proportion bias and overlap between clusters. In this paper, we
propose MC fusion as an extension of MC to handle situations in which multiple
mixture numbers are possible in a finite mixture model. By incorporating the
fusion of multiple models, our approach accurately captured the cluster
structure during transitional periods of gradual change. Moreover, we introduce
a method for detecting changes in the cluster structure by examining the
transition of MC fusion. We demonstrate the effectiveness of our method through
empirical analysis using both artificial and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSF-GAN: DownStream Feedback Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perets, Nadav Rappoport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branch-Tuning: Balancing Stability and Plasticity for Continual
  <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has emerged as an effective paradigm for
deriving general representations from vast amounts of unlabeled data. However,
as real-world applications continually integrate new content, the high
computational and resource demands of SSL necessitate continual learning rather
than complete retraining. This poses a challenge in striking a balance between
stability and plasticity when adapting to new information. In this paper, we
employ Centered Kernel Alignment for quantitatively analyzing model stability
and plasticity, revealing the critical roles of batch normalization layers for
stability and convolutional layers for plasticity. Motivated by this, we
propose Branch-tuning, an efficient and straightforward method that achieves a
balance between stability and plasticity in continual SSL. Branch-tuning
consists of branch expansion and compression, and can be easily applied to
various SSL methods without the need of modifying the original methods,
retaining old data or models. We validate our method through incremental
experiments on various benchmark datasets, demonstrating its effectiveness and
practical value in real-world scenarios. We hope our work offers new insights
for future continual self-supervised learning research. The code will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Image <span class="highlight-title">Transformer</span>s for Prostate Cancer Detection from
  Ultrasound Data <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar, Amoon Jamzad, Mahdi Gilany, Minh Nguyen Nhat To, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in
ultrasound images typically employ convolutional networks (CNNs) to detect
cancer in small regions of interest (ROI) along a needle trace region. However,
this approach suffers from weak labelling, since the ground-truth
histopathology labels do not describe the properties of individual ROIs.
Recently, multi-scale approaches have sought to mitigate this issue by
combining the context awareness of transformers with a CNN feature extractor to
detect cancer from multiple ROIs using multiple-instance learning (MIL). In
this work, we present a detailed study of several image transformer
architectures for both ROI-scale and multi-scale classification, and a
comparison of the performance of CNNs and transformers for ultrasound-based
prostate cancer classification. We also design a novel multi-objective learning
strategy that combines both ROI and core predictions to further mitigate label
noise. METHODS: We evaluate 3 image transformers on ROI-scale cancer
classification, then use the strongest model to tune a multi-scale classifier
with MIL. We train our MIL models using our novel multi-objective learning
strategy and compare our results to existing baselines. RESULTS: We find that
for both ROI-scale and multi-scale PCa detection, image transformer backbones
lag behind their CNN counterparts. This deficit in performance is even more
noticeable for larger models. When using multi-objective learning, we can
improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a
specificity of 66.3%. CONCLUSION: Convolutional networks are better suited for
modelling sparse datasets of prostate ultrasounds, producing more robust
features than transformers in PCa detection. Multi-scale methods remain the
best architecture for this task, with multi-objective learning presenting an
effective way to improve performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>early draft, 7 pages; Accepted to SPIE Medical Imaging 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier or Wavelet bases as counterpart self-attention in spikformer for
  efficient visual classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Wang, Duzhen Zhang, Tilelin Zhang, Bo Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-efficient spikformer has been proposed by integrating the biologically
plausible spiking neural network (SNN) and artificial Transformer, whereby the
Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower
computational cost. However, it seems that self-attention is not always
necessary, especially in sparse spike-form calculation manners. In this paper,
we innovatively replace vanilla SSA (using dynamic bases calculating from Query
and Key) with spike-form Fourier Transform, Wavelet Transform, and their
combinations (using fixed triangular or wavelets bases), based on a key
hypothesis that both of them use a set of basis functions for information
transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is
proposed and verified in visual classification tasks, including both static
image and event-based video datasets. The FWformer can achieve comparable or
even higher accuracies ($0.4\%$-$1.5\%$), higher running speed ($9\%$-$51\%$
for training and $19\%$-$70\%$ for inference), reduced theoretical energy
consumption ($20\%$-$25\%$), and reduced GPU memory usage ($4\%$-$26\%$),
compared to the standard spikformer. Our result indicates the continuous
refinement of new Transformers, that are inspired either by biological
discovery (spike-form), or information theory (Fourier or Wavelet Transform),
is promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2308.02557</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Transformer</span>-Based Framework for Payload Malware Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Deployment of <span class="highlight-title">Pre-train</span>ed Language-Conditioned
  Imitation Learning Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Wu, Bruce D. Lee, Kostas Daniilidis, Bernadette Bucher, Nikolai Matni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale robotic policies trained on data from diverse tasks and robotic
platforms hold great promise for enabling general-purpose robots; however,
reliable generalization to new environment conditions remains a major
challenge. Toward addressing this challenge, we propose a novel approach for
uncertainty-aware deployment of pre-trained language-conditioned imitation
learning agents. Specifically, we use temperature scaling to calibrate these
models and exploit the calibrated model to make uncertainty-aware decisions by
aggregating the local information of candidate actions. We implement our
approach in simulation using three such pre-trained models, and showcase its
potential to significantly enhance task completion rates. The accompanying code
is accessible at the link:
https://github.com/BobWu1998/uncertainty_quant_all.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergon Cugler de Moraes Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Optimal Fair Classification with Bounded Demographic Disparity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianli Zeng, Guang Cheng, Edgar Dobriban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the disparate impact of statistical machine learning methods is
crucial for ensuring fairness. While extensive research aims to reduce
disparity, the effect of using a \emph{finite dataset} -- as opposed to the
entire population -- remains unclear. This paper explores the statistical
foundations of fair binary classification with two protected groups, focusing
on controlling demographic disparity, defined as the difference in acceptance
rates between the groups. Although fairness may come at the cost of accuracy
even with infinite data, we show that using a finite sample incurs additional
costs due to the need to estimate group-specific acceptance thresholds. We
study the minimax optimal classification error while constraining demographic
disparity to a user-specified threshold. To quantify the impact of fairness
constraints, we introduce a novel measure called \emph{fairness-aware excess
risk} and derive a minimax lower bound on this measure that all classifiers
must satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding
method with an offset that we show attains the minimax lower bound. Our lower
bound proofs involve several innovations. Experiments support that
FairBayes-DDP+ controls disparity at the user-specified level, while being
faster and having a more favorable fairness-accuracy tradeoff than several
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large communication costs are a critical bottleneck in training
state-of-the-art neural networks on distributed systems. This paper introduces
AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by
Agarwal's algorithm for matrix multiplication, for parallelizing tensor
computations in deep learning, AxoNN employs two key strategies to minimize
communication overhead. First, we optimize communication by overlapping
expensive collective operations (reduce-scatter, all-gather, all-reduce) with
computations. Our experiments with a 20-billion parameter transformer model
demonstrate that these optimizations deliver nearly 53\% improvement. Second,
we present an analytical model to assist users in identifying
communication-minimizing configurations within the vast search space defined by
our 4D algorithm. This model empowers practitioners by simplifying the tuning
process for their specific training workloads. When training an 80-billion
parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a
state-of-the-art framework, by a significant 26%. Additionally, it achieves 57%
of the theoretical peak FLOP/s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Patel, Sangeetha Abdu Jyothi, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CrystalBox, a novel, model-agnostic, posthoc explainability
framework for Deep Reinforcement Learning (DRL) controllers in the large family
of input-driven environments which includes computer systems. We combine the
natural decomposability of reward functions in input-driven environments with
the explanatory power of decomposed returns. We propose an efficient algorithm
to generate future-based explanations across both discrete and continuous
control environments. Using applications such as adaptive bitrate streaming and
congestion control, we demonstrate CrystalBox's capability to generate
high-fidelity explanations. We further illustrate its higher utility across
three practical use cases: contrastive explanations, network observability, and
guided reward design, as opposed to prior explainability techniques that
identify salient features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>228 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Novel Federated Learning Framework over LEO Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplified Diffusion Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/checkcrab/SDSB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preventing Arbitrarily High Confidence on Far-Away Data in
  Point-Estimated Discriminative Neural Networks <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Rashid, Serena Hacker, Guojun Zhang, Agustinus Kristiadi, Pascal Poupart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discriminatively trained, deterministic neural networks are the de facto
choice for classification problems. However, even though they achieve
state-of-the-art results on in-domain test sets, they tend to be overconfident
on out-of-distribution (OOD) data. For instance, ReLU networks - a popular
class of neural network architectures - have been shown to almost always yield
high confidence predictions when the test data are far away from the training
set, even when they are trained with OOD data. We overcome this problem by
adding a term to the output of the neural network that corresponds to the logit
of an extra class, that we design to dominate the logits of the original
classes as we move away from the training data.This technique provably prevents
arbitrarily high confidence on far-away test data while maintaining a simple
discriminative point-estimate training. Evaluation on various benchmarks
demonstrates strong performance against competitive baselines on both far-away
and realistic OOD data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nesting Particle Filters for Experimental Design in Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahel Iqbal, Adrien Corenflos, Simo Särkkä, Hany Abdulsamad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach to Bayesian experimental design
for non-exchangeable data that formulates it as risk-sensitive policy
optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential
Monte Carlo technique to infer optimal designs, and embed it into a particle
Markov chain Monte Carlo framework to perform gradient-based policy
amortization. Our approach is distinct from other amortized experimental design
techniques, as it does not rely on contrastive estimators. Numerical validation
on a set of dynamical systems showcases the efficacy of our method in
comparison to other state-of-the-art strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Review</span> of Community Detection in Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiakang Li, Songning Lai, Zhihao Shuai, Yuan Tan, Yifan Jia, Mianyang Yu, Zichen Song, Xiaokang Peng, Ziyang Xu, Yongxin Ni, Haifeng Qiu, Jiayu Yang, Yutong Liu, Yonggang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of complex networks has significantly advanced our understanding of
community structures which serves as a crucial feature of real-world graphs.
Detecting communities in graphs is a challenging problem with applications in
sociology, biology, and computer science. Despite the efforts of an
interdisciplinary community of scientists, a satisfactory solution to this
problem has not yet been achieved. This review article delves into the topic of
community detection in graphs, which serves as a thorough exposition of various
community detection methods from perspectives of modularity-based method,
spectral clustering, probabilistic modelling, and deep learning. Along with the
methods, a new community detection method designed by us is also presented.
Additionally, the performance of these methods on the datasets with and without
ground truth is compared. In conclusion, this comprehensive review provides a
deep understanding of community detection in graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, P S Pravin, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational efficiency and non-adversarial robustness are critical factors
in real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters non-adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Networks. This model is explicitly designed for fast and robust
optimization-based tasks and outperforms existing recurrent units across a
spectrum of engineering tasks in terms of computational efficiency and
non-adversarial robustness, including real-world solar irradiance prediction
for Solar PV system planning at LHT Holdings in Singapore and real-time Model
Predictive Control optimization for a nonlinear chemical reactor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Data Mesh with Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Misconceptions in Social Bots Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LCANets++: Robust Audio Classification using Multi-layer Neural Networks
  with Lateral Competition <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio classification aims at recognizing audio signals, including speech
commands or sound events. However, current audio classifiers are susceptible to
perturbations and adversarial attacks. In addition, real-world audio
classification tasks often suffer from limited labeled data. To help bridge
these gaps, previous work developed neuro-inspired convolutional neural
networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)
in the first layer (i.e., LCANets) for computer vision. LCANets learn in a
combination of supervised and unsupervised learning, reducing dependency on
labeled samples. Motivated by the fact that auditory cortex is also sparse, we
extend LCANets to audio recognition tasks and introduce LCANets++, which are
CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that
LCANets++ are more robust than standard CNNs and LCANets against perturbations,
e.g., background noise, as well as black-box and white-box attacks, e.g.,
evasion and fast gradient sign (FGSM) attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE International Conference on Acoustics, Speech
  and Signal Processing Workshops (ICASSPW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Action <span class="highlight-title">Transformer</span> with Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Staroverov, Egor Cherepanov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of transformers in offline reinforcement learning has
become a rapidly developing area. This is due to their ability to treat the
agent's trajectory in the environment as a sequence, thereby reducing the
policy learning problem to sequence modeling. In environments where the agent's
decisions depend on past events, it is essential to capture both the event
itself and the decision point in the context of the model. However, the
quadratic complexity of the attention mechanism limits the potential for
context expansion. One solution to this problem is to enhance transformers with
memory mechanisms. In this paper, we propose the Recurrent Action Transformer
with Memory (RATE) - a model that incorporates recurrent memory. To evaluate
our model, we conducted extensive experiments on both memory-intensive
environments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo
control environments. The results show that the use of memory can significantly
improve performance in memory-intensive environments while maintaining or
improving results in classic environments. We hope that our findings will
stimulate research on memory mechanisms for transformers applicable to offline
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative <span class="highlight-title">Pre-Train</span>ing of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewoong Lee, JinKyou Choi, Min Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Regret Learning in Bilateral Trade via Global Budget Balance <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilateral trade models the problem of intermediating between two rational
agents -- a seller and a buyer -- both characterized by a private valuation for
an item they want to trade. We study the online learning version of the
problem, in which at each time step a new seller and buyer arrive and the
learner has to set prices for them without any knowledge about their
(adversarially generated) valuations.
  In this setting, known impossibility results rule out the existence of
no-regret algorithms when budget balanced has to be enforced at each time step.
In this paper, we introduce the notion of \emph{global budget balance}, which
only requires the learner to fulfill budget balance over the entire time
horizon. Under this natural relaxation, we provide the first no-regret
algorithms for adversarial bilateral trade under various feedback models.
First, we show that in the full-feedback model, the learner can guarantee
$\tilde O(\sqrt{T})$ regret against the best fixed prices in hindsight, and
that this bound is optimal up to poly-logarithmic terms. Second, we provide a
learning algorithm guaranteeing a $\tilde O(T^{3/4})$ regret upper bound with
one-bit feedback, which we complement with a $\Omega(T^{5/7})$ lower bound that
holds even in the two-bit feedback model. Finally, we introduce and analyze an
alternative benchmark that is provably stronger than the best fixed prices in
hindsight and is inspired by the literature on bandits with knapsacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Detection of Machine-Generated Text using Style Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. However, such abuse may be
counteracted with the ability to detect whether a piece of text was composed by
a language model rather than a human author. Some previous approaches to this
problem have relied on supervised methods by training on corpora of confirmed
human- and machine- written documents. Unfortunately, model under-specification
poses an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of newer language
models producing still more fluent text than the models used to train the
detectors. Other approaches require access to the models that may have
generated a document in question, which is often impractical. In light of these
challenges, we pursue a fundamentally different approach not relying on samples
from language models of concern at training time. Instead, we propose to
leverage representations of writing style estimated from human-authored text.
Indeed, we find that features effective at distinguishing among human authors
are also effective at distinguishing human from machine authors, including
state-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.
Furthermore, given a handful of examples composed by each of several specific
language models of interest, our approach affords the ability to predict which
model generated a given document. The code and data to reproduce our
experiments are available at
https://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical predictions from the model to identify true
labels for most training examples. First, we introduce a cross selection
strategy, which enables two deep models to select true labels of partially
labeled data for each other. Besides, we propose a novel consistency
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Optimized Orthogonal Basis Piecewise Polynomial
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Waclawek, Stefan Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,
like trajectory planning, to approximate position profiles given in the form of
a set of points. While the approximation target along with domain-specific
requirements, like Ck -continuity, can be formulated as a system of equations
and a result can be computed directly, such closed-form solutions posses
limited flexibility with respect to polynomial degrees, polynomial bases or
adding further domain-specific requirements. Sufficiently complex optimization
goals soon call for the use of numerical methods, like gradient descent. Since
gradient descent lies at the heart of training Artificial Neural Networks
(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set
of gradient-based optimizers potentially suitable for a wide range of
optimization problems beyond the training task for ANNs. Our approach is to
utilize the versatility of PP models and combine it with the potential of
modern ML optimizers for the use in function approximation in 1D trajectory
planning in the context of electronic cam design. We utilize available
optimizers of the ML framework TensorFlow directly, outside of the scope of
ANNs, to optimize model parameters of our PP model. In this paper, we show how
an orthogonal polynomial basis contributes to improving approximation and
continuity optimization performance. Utilizing Chebyshev polynomials of the
first kind, we develop a novel regularization approach enabling clearly
improved convergence behavior. We show that, using this regularization
approach, Chebyshev basis performs better than power basis for all relevant
optimizers in the combined approximation and continuity optimization setting
and demonstrate usability of the presented approach within the electronic cam
domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to LION18</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient <span class="highlight-title">Transformer</span>-Based 3D Human Pose
  Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Limit Order Book Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Briola, Silvia Bartolucci, Tomaso Aste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We exploit cutting-edge deep learning methodologies to explore the
predictability of high-frequency Limit Order Book mid-price changes for a
heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we
release `LOBFrame', an open-source code base to efficiently process large-scale
Limit Order Book data and quantitatively assess state-of-the-art deep learning
models' forecasting capabilities. Our results are twofold. We demonstrate that
the stocks' microstructural characteristics influence the efficacy of deep
learning methods and that their high forecasting power does not necessarily
correspond to actionable trading signals. We argue that traditional machine
learning metrics fail to adequately assess the quality of forecasts in the
Limit Order Book context. As an alternative, we propose an innovative
operational framework that evaluates predictions' practicality by focusing on
the probability of accurately forecasting complete transactions. This work
offers academics and practitioners an avenue to make informed and robust
decisions on the application of deep learning techniques, their scope and
limitations, effectively exploiting emergent statistical properties of the
Limit Order Book.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 14 figures, 12 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIGraph: Generative <span class="highlight-title">Self-supervised</span> Learning for Class-Imbalanced Node
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in graph data presents significant challenges for node
classification. While existing methods, such as SMOTE-based approaches,
partially mitigate this issue, they still exhibit limitations in constructing
imbalanced graphs. Generative self-supervised learning (SSL) methods,
exemplified by graph autoencoders (GAEs), offer a promising solution by
directly generating minority nodes from the data itself, yet their potential
remains underexplored. In this paper, we delve into the shortcomings of
SMOTE-based approaches in the construction of imbalanced graphs. Furthermore,
we introduce VIGraph, a simple yet effective generative SSL approach that
relies on the Variational GAE as the fundamental model. VIGraph strictly
adheres to the concept of imbalance when constructing imbalanced graphs and
innovatively leverages the variational inference (VI) ability of Variational
GAE to generate nodes for minority classes. VIGraph introduces comprehensive
training strategies, including cross-view contrastive learning at the decoding
phase to capture semantic knowledge, adjacency matrix reconstruction to
preserve graph structure, and alignment strategy to ensure stable training.
VIGraph can generate high-quality nodes directly usable for classification,
eliminating the need to integrate the generated nodes back to the graph as well
as additional retraining found in SMOTE-based methods. We conduct extensive
experiments, results from which demonstrate the superiority and generality of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel
  Segmentation via Two-Phase Training Angiography-to-Venography Translation <span class="chip">BMVC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla, Ferran Prados, Ninon Burgos, Maria A. Zuluaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a semi-supervised domain adaptation framework for brain vessel
segmentation from different image modalities. Existing state-of-the-art methods
focus on a single modality, despite the wide range of available cerebrovascular
imaging techniques. This can lead to significant distribution shifts that
negatively impact the generalization across modalities. By relying on annotated
angiographies and a limited number of annotated venographies, our framework
accomplishes image-to-image translation and semantic segmentation, leveraging a
disentangled and semantically rich latent space to represent heterogeneous data
and perform image-level adaptation from source to target domains. Moreover, we
reduce the typical complexity of cycle-based architectures and minimize the use
of adversarial training, which allows us to build an efficient and intuitive
model with stable training. We evaluate our method on magnetic resonance
angiographies and venographies. While achieving state-of-the-art performance in
the source domain, our method attains a Dice score coefficient in the target
domain that is only 8.9% lower, highlighting its promising potential for robust
cerebrovascular image segmentation across different modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 34th British Machine Vision Conference (BMVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Amos Storkey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline pretraining with a static dataset followed by online fine-tuning
(offline-to-online, or OtO) is a paradigm well matched to a real-world RL
deployment process. In this scenario, we aim to find the best-performing policy
within a limited budget of online interactions. Previous work in the OtO
setting has focused on correcting for bias introduced by the policy-constraint
mechanisms of offline RL algorithms. Such constraints keep the learned policy
close to the behavior policy that collected the dataset, but we show this can
unnecessarily limit policy performance if the behavior policy is far from
optimal. Instead, we forgo constraints and frame OtO RL as an exploration
problem that aims to maximize the benefit of online data-collection. We first
study the major online RL exploration methods based on intrinsic rewards and
UCB in the OtO setting, showing that intrinsic rewards add training instability
through reward-function modification, and UCB methods are myopic and it is
unclear which learned-component's ensemble to use for action selection. We then
introduce an algorithm for planning to go out-of-distribution (PTGOOD) that
avoids these issues. PTGOOD uses a non-myopic planning procedure that targets
exploration in relatively high-reward regions of the state-action space
unlikely to be visited by the behavior policy. By leveraging concepts from the
Conditional Entropy Bottleneck, PTGOOD encourages data collected online to
provide new information relevant to improving the final deployment policy
without altering rewards. We show empirically in several continuous control
tasks that PTGOOD significantly improves agent returns during online
fine-tuning and avoids the suboptimal policy convergence that many of our
baselines exhibit in several environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models via Policy-Guided Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Rigter, Jun Yamada, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for short trajectories, with the exception of autoregressive diffusion. For
short trajectories, PolyGRAD obtains similar errors to autoregressive
diffusion, but with lower computational requirements. For long trajectories,
PolyGRAD obtains comparable performance to baselines. Our experiments
demonstrate that PolyGRAD enables performant policies to be trained via
on-policy RL in imagination for MuJoCo continuous control domains. Thus,
PolyGRAD introduces a new paradigm for accurate on-policy world modelling
without autoregressive sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR, March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emerging Trends in Federated Learning: From Model Fusion to Federated X
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.12920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.12920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxiong Ji, Yue Tan, Teemu Saravirta, Zhiqin Yang, Yixin Liu, Lauri Vasankari, Shirui Pan, Guodong Long, Anwar Walid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a new learning paradigm that decouples data collection
and model training via multi-party computation and model aggregation. As a
flexible learning setting, federated learning has the potential to integrate
with other learning frameworks. We conduct a focused survey of federated
learning in conjunction with other learning algorithms. Specifically, we
explore various learning algorithms to improve the vanilla federated averaging
algorithm and review model fusion methods such as adaptive aggregation,
regularization, clustered methods, and Bayesian methods. Following the emerging
trends, we also discuss federated learning in the intersection with other
learning paradigms, termed federated X learning, where X includes multitask
learning, meta-learning, transfer learning, unsupervised learning, and
reinforcement learning. In addition to reviewing state-of-the-art studies, this
paper also identifies key challenges and applications in this field, while also
highlighting promising future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the International Journal of Machine Learning and
  Cybernetics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High Dimensional Distributed Gradient Descent with Arbitrary Number of
  Byzantine Attackers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puning Zhao, Zhiguo Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust distributed learning with Byzantine failures has attracted extensive
research interests in recent years. However, most of existing methods suffer
from curse of dimensionality, which is increasingly serious with the growing
complexity of modern machine learning models. In this paper, we design a new
method that is suitable for high dimensional problems, under arbitrary number
of Byzantine attackers. The core of our design is a direct high dimensional
semi-verified mean estimation method. Our idea is to identify a subspace first.
The components of mean value perpendicular to this subspace can be estimated
via gradient vectors uploaded from worker machines, while the components within
this subspace are estimated using auxiliary dataset. We then use our new method
as the aggregator of distributed learning problems. Our theoretical analysis
shows that the new method has minimax optimal statistical rates. In particular,
the dependence on dimensionality is significantly improved compared with
previous works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rousseau, Alessandra Pascale, John Dinsmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic Bayes risk of semi-supervised learning with uncertain
  labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Leger, Romain Couillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article considers a semi-supervised classification setting on a Gaussian
mixture model, where the data is not labeled strictly as usual, but instead
with uncertain labels. Our main aim is to compute the Bayes risk for this
model. We compare the behavior of the Bayes risk and the best known algorithm
for this model. This comparison eventually gives new insights over the
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Physics-embedded Deep Learning Framework for Cloth Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Delicate cloth simulations have long been desired in computer graphics.
Various methods were proposed to improve engaged force interactions, collision
handling, and numerical integrations. Deep learning has the potential to
achieve fast and real-time simulation, but common neural network structures
often demand many parameters to capture cloth dynamics. This paper proposes a
physics-embedded learning framework that directly encodes physical features of
cloth simulation. The convolutional neural network is used to represent spatial
correlations of the mass-spring system, after which three branches are designed
to learn linear, nonlinear, and time derivate features of cloth physics. The
framework can also integrate with other external forces and collision handling
through either traditional simulators or sub neural networks. The model is
tested across different cloth animation cases, without training with new data.
Agreement with baselines and predictive realism successfully validate its
generalization ability. Inference efficiency of the proposed model also defeats
traditional physics simulation. This framework is also designed to easily
integrate with other visual refinement techniques like wrinkle carving, which
leaves significant chances to incorporate prevailing macing learning techniques
in 3D cloth amination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A derivation is incomplete, and updations are being processed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret-Based Defense in Adversarial Reinforcement Learning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06912v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06912v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a "neighborhood" of
observations to the received "observation". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meshal Alharbi, Mardavij Roozbehani, Munther Dahleh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of sample complexity of online reinforcement learning is often
studied in the literature without taking into account any partial knowledge
about the system dynamics that could potentially accelerate the learning
process. In this paper, we study the sample complexity of online Q-learning
methods when some prior knowledge about the dynamics is available or can be
learned efficiently. We focus on systems that evolve according to an additive
disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$
represents the underlying system dynamics, and $W_h$ are unknown disturbances
independent of states and actions. In the setting of finite episodic Markov
decision processes with $S$ states, $A$ actions, and episode length $H$, we
present an optimistic Q-learning algorithm that achieves
$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of
$f$, where $T$ is the total number of interactions with the system. This is in
contrast to the typical $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{SAT})$ regret
for existing Q-learning methods. Further, if only a noisy estimate $\hat{f}$ of
$f$ is available, our method can learn an approximately optimal policy in a
number of samples that is independent of the cardinalities of state and action
spaces. The sub-optimality gap depends on the approximation error $\hat{f}-f$,
as well as the Lipschitz constant of the corresponding optimal value function.
Our approach does not require modeling of the transition probabilities and
enjoys the same memory complexity as model-free methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 38th Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised AUC Optimization: A Unified Partial AUC Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since acquiring perfect supervision is usually difficult, real-world machine
learning tasks often confront inaccurate, incomplete, or inexact supervision,
collectively referred to as weak supervision. In this work, we present WSAUC, a
unified framework for weakly supervised AUC optimization problems, which covers
noisy label learning, positive-unlabeled learning, multi-instance learning, and
semi-supervised learning scenarios. Within the WSAUC framework, we first frame
the AUC optimization problems in various weakly supervised scenarios as a
common formulation of minimizing the AUC risk on contaminated sets, and
demonstrate that the empirical risk minimization problems are consistent with
the true AUC. Then, we introduce a new type of partial AUC, specifically, the
reversed partial AUC (rpAUC), which serves as a robust training objective for
AUC maximization in the presence of contaminated labels. WSAUC offers a
universal solution for AUC optimization in various weakly supervised scenarios
by maximizing the empirical rpAUC. Theoretical and experimental results under
multiple settings support the effectiveness of WSAUC on a range of weakly
supervised AUC optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Model Makes Clustering A Better Initialization For Cold-Start
  Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yuan, Chuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning selects the most informative samples from the unlabelled
dataset to annotate in the context of a limited annotation budget. While
numerous methods have been proposed for subsequent sample selection based on an
initialized model, scant attention has been paid to the indispensable phase of
active learning: selecting samples for model cold-start initialization. Most of
the previous studies resort to random sampling or naive clustering. However,
random sampling is prone to fluctuation, and naive clustering suffers from
convergence speed, particularly when dealing with high-dimensional data such as
imaging data. In this work, we propose to integrate foundation models with
clustering methods to select samples for cold-start active learning
initialization. Foundation models refer to those trained on massive datasets by
the self-supervised paradigm and capable of generating informative and
compacted embeddings for various downstream tasks. Leveraging these embeddings
to replace raw features such as pixel values, clustering quickly converges and
identifies better initial samples. For a comprehensive comparison, we included
a classic ImageNet-supervised model to acquire embeddings. Experiments on two
clinical tasks of image classification and segmentation demonstrated that
foundation model-based clustering efficiently pinpointed informative initial
samples, leading to models showcasing enhanced performance than the baseline
methods. We envisage that this study provides an effective paradigm for future
cold-start active learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim, Hyoungshick Kim, Jin B. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper provides empirical comparisons between recent IDSs to provide an
objective comparison between them to help users choose the most appropriate
solution based on their requirements. Our results show that no one solution is
the best, but is dependent on external variables such as the types of attacks,
complexity, and network environment in the dataset. For example, BoT_IoT and
Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural
network performed the best when tested using the BoT_IoT dataset while HELAD
performed the best when tested using the Stratosphere IoT dataset. So although
we found that a deep neural network solution had the highest average F1 scores
on tested datasets, it is not always the best-performing one. We further
discuss difficulties in using IDS from literature and project repositories,
which complicated drawing definitive conclusions regarding IDS selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBQ: Cross-Block Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpeng Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PPO (Proximal Policy Optimization) algorithm has demonstrated excellent
performance in many fields, and it is considered as a simple version of TRPO
(Trust Region Policy Optimization) algorithm. However, the ratio clipping
operation in PPO may not always effectively enforce the trust region
constraints, this can be a potential factor affecting the stability of the
algorithm. In this paper, we propose Simple Policy Optimization (SPO)
algorithm, which introduces a novel clipping method for KL divergence between
the old and current policies. Extensive experimental results in Atari 2600
environments indicate that, compared to the mainstream variants of PPO, SPO
achieves better sample efficiency, extremely low KL divergence, and higher
policy entropy, and is robust to the increase in network depth or complexity.
More importantly, SPO maintains the simplicity of an unconstrained first-order
algorithm. Code is available at
https://github.com/MyRepositories-hub/Simple-Policy-Optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering and Mitigating Visual Biases through Keyword Explanation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11104v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11104v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing biases in computer vision models is crucial for real-world AI
deployments. However, mitigating visual biases is challenging due to their
unexplainable nature, often identified indirectly through visualization or
sample statistics, which necessitates additional human supervision for
interpretation. To tackle this issue, we propose the Bias-to-Text (B2T)
framework, which interprets visual biases as keywords. Specifically, we extract
common keywords from the captions of mispredicted images to identify potential
biases in the model. We then validate these keywords by measuring their
similarity to the mispredicted images using a vision-language scoring model.
The keyword explanation form of visual bias offers several advantages, such as
a clear group naming for bias discovery and a natural extension for debiasing
using these group names. Our experiments demonstrate that B2T can identify
known biases, such as gender bias in CelebA, background bias in Waterbirds, and
distribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in
larger datasets, such as Dollar Street and ImageNet. For example, we discovered
a contextual bias between "bee" and "flower" in ImageNet. We also highlight
various applications of B2T keywords, including debiased training, CLIP
prompting, and model comparison.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale
  Recommendation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong Zhao, Tong Yang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the growing memory demands of embedding tables in Deep Learning
Recommendation Models (DLRMs) pose great challenges for model training and
deployment. Existing embedding compression solutions cannot simultaneously meet
three key design requirements: memory efficiency, low latency, and adaptability
to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,
and Fast Embedding compression framework that addresses the above requirements.
The design philosophy of CAFE is to dynamically allocate more memory resources
to important features (called hot features), and allocate less memory to
unimportant ones. In CAFE, we propose a fast and lightweight sketch data
structure, named HotSketch, to capture feature importance and report hot
features in real time. For each reported hot feature, we assign it a unique
embedding. For the non-hot features, we allow multiple features to share one
embedding by using hash embedding technique. Guided by our design philosophy,
we further propose a multi-level hash embedding framework to optimize the
embedding tables of non-hot features. We theoretically analyze the accuracy of
HotSketch, and analyze the model convergence against deviation. Extensive
experiments show that CAFE significantly outperforms existing embedding
compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo
Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The
source codes of CAFE are available at GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real Acoustic Fields: An Audio-Visual Room Acoustics <span class="highlight-title">Dataset</span> and
  Benchmark <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, Alexander Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new dataset called Real Acoustic Fields (RAF) that captures real
acoustic room data from multiple modalities. The dataset includes high-quality
and densely captured room impulse response data paired with multi-view images,
and precise 6DoF pose tracking data for sound emitters and listeners in the
rooms. We used this dataset to evaluate existing methods for novel-view
acoustic synthesis and impulse response generation which previously relied on
synthetic data. In our evaluation, we thoroughly assessed existing audio and
audio-visual models against multiple criteria and proposed settings to enhance
their performance on real-world data. We also conducted experiments to
investigate the impact of incorporating visual data (i.e., images and depth)
into neural acoustic field models. Additionally, we demonstrated the
effectiveness of a simple sim2real approach, where a model is pre-trained with
simulated data and fine-tuned with sparse real-world data, resulting in
significant improvements in the few-shot learning approach. RAF is the first
dataset to provide densely captured room acoustic data, making it an ideal
resource for researchers working on audio and audio-visual neural acoustic
field modeling techniques. Demos and datasets are available on our project
page: https://facebookresearch.github.io/real-acoustic-fields/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project site:
  https://facebookresearch.github.io/real-acoustic-fields/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Textual <span class="highlight-title">Prompt</span> to AI-Generated Image Quality Assessment <span class="chip">ICME2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Qu, Haohui Li, Wei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike
traditional image quality assessment (IQA) on natural scenarios, AGIs quality
assessment (AGIQA) takes the correspondence of image and its textual prompt
into consideration. This is coupled in the ground truth score, which confuses
the unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs
Quality Assessment via Image and Prompt), a multimodal framework for AGIQA via
corresponding image and prompt incorporation. Specifically, we propose a novel
incremental pretraining task named Image2Prompt for better understanding of
AGIs and their corresponding textual prompts. An effective and efficient
image-prompt fusion module, along with a novel special [QA] token, are also
applied. Both are plug-and-play and beneficial for the cooperation of image and
its corresponding prompt. Experiments demonstrate that our IP-IQA achieves the
state-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, accepted by ICME2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Cache Important Contents for Multi-modal Service in Dynamic
  Networks: A DRL-based Caching Scheme 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Zhang, Marc St-Hilaire, Xin Wei, Haiwei Dong, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous evolution of networking technologies, multi-modal
services that involve video, audio, and haptic contents are expected to become
the dominant multimedia service in the near future. Edge caching is a key
technology that can significantly reduce network load and content transmission
latency, which is critical for the delivery of multi-modal contents. However,
existing caching approaches only rely on a limited number of factors, e.g.,
popularity, to evaluate their importance for caching, which is inefficient for
caching multi-modal contents, especially in dynamic network environments. To
overcome this issue, we propose a content importance-based caching scheme which
consists of a content importance evaluation model and a caching model. By
leveraging dueling double deep Q networks (D3QN) model, the content importance
evaluation model can adaptively evaluate contents' importance in dynamic
networks. Based on the evaluated contents' importance, the caching model can
easily cache and evict proper contents to improve caching efficiency. The
simulation results show that the proposed content importance-based caching
scheme outperforms existing caching schemes in terms of caching hit ratio (at
least 15% higher), reduced network load (up to 22% reduction), average number
of hops (up to 27% lower), and unsatisfied requests ratio (more than 47%
reduction).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think before You Leap: Content-Aware Low-Cost Edge-Assisted Video
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Yan, Yi Wang, Xuedou Xiao, Zhiqing Luo, Jianhua He, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offloading computing to edge servers is a promising solution to support
growing video understanding applications at resource-constrained IoT devices.
Recent efforts have been made to enhance the scalability of such systems by
reducing inference costs on edge servers. However, existing research is not
directly applicable to pixel-level vision tasks such as video semantic
segmentation (VSS), partly due to the fluctuating VSS accuracy and segment
bitrate caused by the dynamic video content. In response, we present Penance, a
new edge inference cost reduction framework. By exploiting softmax outputs of
VSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes
model selection and compression settings to minimize the inference cost while
meeting the required accuracy within the available bandwidth constraints. We
implement Penance in a commercial IoT device with only CPUs. Experimental
results show that Penance consumes a negligible 6.8% more computation resources
than the optimal strategy while satisfying accuracy and bandwidth constraints
with a low failure rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive <span class="highlight-title">Pre-Train</span>ing with Multi-View Fusion for No-Reference Point
  Cloud Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Jenq-Neng Hwang, Xiaozhong Xu, Shan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-reference point cloud quality assessment (NR-PCQA) aims to automatically
evaluate the perceptual quality of distorted point clouds without available
reference, which have achieved tremendous improvements due to the utilization
of deep neural networks. However, learning-based NR-PCQA methods suffer from
the scarcity of labeled data and usually perform suboptimally in terms of
generalization. To solve the problem, we propose a novel contrastive
pre-training framework tailored for PCQA (CoPA), which enables the pre-trained
model to learn quality-aware representations from unlabeled data. To obtain
anchors in the representation space, we project point clouds with different
distortions into images and randomly mix their local patches to form mixed
images with multiple distortions. Utilizing the generated anchors, we constrain
the pre-training process via a quality-aware contrastive loss following the
philosophy that perceptual quality is closely related to both content and
distortion. Furthermore, in the model fine-tuning stage, we propose a
semantic-guided multi-view fusion module to effectively integrate the features
of projected images from multiple perspectives. Extensive experiments show that
our method outperforms the state-of-the-art PCQA methods on popular benchmarks.
Further investigations demonstrate that CoPA can also benefit existing
learning-based PCQA models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring LLMs as a Source of Targeted Synthetic Textual Data to
  Minimize High Confidence Misclassifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) models optimized for predictive performance
often make high confidence errors and suffer from vulnerability to adversarial
and out-of-distribution data. Existing work has mainly focused on mitigation of
such errors using either humans or an automated approach. In this study, we
explore the usage of large language models (LLMs) for data augmentation as a
potential solution to the issue of NLP models making wrong predictions with
high confidence during classification tasks. We compare the effectiveness of
synthetic data generated by LLMs with that of human data obtained via the same
procedure. For mitigation, humans or LLMs provide natural language
characterizations of high confidence misclassifications to generate synthetic
data, which are then used to extend the training set. We conduct an extensive
evaluation of our approach on three classification tasks and demonstrate its
effectiveness in reducing the number of high confidence misclassifications
present in the model, all while maintaining the same level of accuracy.
Moreover, we find that the cost gap between humans and LLMs surpasses an order
of magnitude, as LLMs attain human-like performance while being more scalable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChroniclingAmericaQA: A Large-scale Question Answering <span class="highlight-title">Dataset</span> based on
  Historical American Newspaper Pages <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Jamshid Mozafari, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have
significantly advanced in recent years due to the rapid development of deep
learning techniques and, more recently, large language models. At the same
time, many benchmark datasets have become available for QA and MRC tasks.
However, most existing large-scale benchmark datasets have been created
predominantly using synchronous document collections like Wikipedia or the Web.
Archival document collections, such as historical newspapers, contain valuable
information from the past that is still not widely used to train large language
models. To further contribute to advancing QA and MRC tasks and to overcome the
limitation of previous datasets, we introduce ChroniclingAmericaQA, a
large-scale dataset with 485K question-answer pairs created based on the
historical newspaper collection Chronicling America. Our dataset is constructed
from a subset of the Chronicling America newspaper collection spanning 120
years. One of the significant challenges for utilizing digitized historical
newspaper collections is the low quality of OCR text. Therefore, to enable
realistic testing of QA models, our dataset can be used in three different
ways: answering questions from raw and noisy content, answering questions from
cleaner, corrected version of the content, as well as answering questions from
scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA
spans the longest time period among available QA datasets make it quite a
unique and useful resource.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbing Weirds Language (Models): Evaluation of English Zero-Derivation
  in Five LLMs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich Schütze, Leonie Weissweiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)
is a hallmark of English morphology. In conversion, a word with one part of
speech is placed in a non-prototypical context, where it is coerced to behave
as if it had a different part of speech. However, while this process affects a
large part of the English lexicon, little work has been done to establish the
degree to which language models capture this type of generalization. This paper
reports the first study on the behavior of large language models with reference
to conversion. We design a task for testing lexical-syntactic flexibility --
the degree to which models can generalize over words in a construction with a
non-prototypical part of speech. This task is situated within a natural
language inference paradigm. We test the abilities of five language models --
two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral
7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,
followed by GPT-3.5, but that the open source language models are also able to
perform it and that the 7B parameter Mistral displays as little difference
between its baseline performance on the natural language inference task and the
non-prototypical syntactic category task, as the massive GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArabicaQA: A Comprehensive <span class="highlight-title">Dataset</span> for Arabic Question Answering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Language Model (GLM): A new graph-based approach to detect social
  instabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul Singh Inda, Jithendra Sai Veeramaneni, Étienne Voutaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This scientific report presents a novel methodology for the early prediction
of important political events using News datasets. The methodology leverages
natural language processing, graph theory, clique analysis, and semantic
relationships to uncover hidden predictive signals within the data. Initially,
we designed a preliminary version of the method and tested it on a few events.
This analysis revealed limitations in the initial research phase. We then
enhanced the model in two key ways: first, we added a filtration step to only
consider politically relevant news before further processing; second, we
adjusted the input features to make the alert system more sensitive to
significant spikes in the data. After finalizing the improved methodology, we
tested it on eleven events including US protests, the Ukraine war, and French
protests. Results demonstrate the superiority of our approach compared to
baseline methods. Through targeted refinements, our model can now provide
earlier and more accurate predictions of major political events based on subtle
patterns in news data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Compressed Language Models Less Subgroup Robust? <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Gee, Andrea Zugarini, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Offer an Alternative to the Traditional Approach
  of Topic Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modelling, as a well-established unsupervised technique, has found
extensive use in automatically detecting significant topics within a corpus of
documents. However, classic topic modelling approaches (e.g., LDA) have certain
drawbacks, such as the lack of semantic understanding and the presence of
overlapping topics. In this work, we investigate the untapped potential of
large language models (LLMs) as an alternative for uncovering the underlying
topics within extensive text corpora. To this end, we introduce a framework
that prompts LLMs to generate topics from a given set of documents and
establish evaluation protocols to assess the clustering efficacy of LLMs. Our
findings indicate that LLMs with appropriate prompts can stand out as a viable
alternative, capable of generating relevant topic titles and adhering to human
guidelines to refine and merge topics. Through in-depth experiments and
evaluation, we summarise the advantages and constraints of employing LLMs in
topic extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generator-Retriever-Generator Approach for Open-Domain Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11278v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11278v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain question answering (QA) tasks usually require the retrieval of
relevant information from a large corpus to generate accurate answers. We
propose a novel approach called Generator-Retriever-Generator (GRG) that
combines document retrieval techniques with a large language model (LLM), by
first prompting the model to generate contextual documents based on a given
question. In parallel, a dual-encoder network retrieves documents that are
relevant to the question from an external corpus. The generated and retrieved
documents are then passed to the second LLM, which generates the final answer.
By combining document retrieval and LLM generation, our approach addresses the
challenges of open-domain QA, such as generating informative and contextually
relevant answers. GRG outperforms the state-of-the-art generate-then-read and
retrieve-then-read pipelines (GENREAD and RFiD) improving their performance by
at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,
respectively. We provide code, datasets, and checkpoints at
https://github.com/abdoelsayed2016/GRG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for AI policy act, if designed by the governments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMuRD: Annotated Arabic-English Receipt <span class="highlight-title">Dataset</span> for Key Information
  Extraction and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of key information from receipts is a complex task that
involves the recognition and extraction of text from scanned receipts. This
process is crucial as it enables the retrieval of essential content and
organizing it into structured documents for easy access and analysis. In this
paper, we present AMuRD, a novel multilingual human-annotated dataset
specifically designed for information extraction from receipts. This dataset
comprises $47,720$ samples and addresses the key challenges in information
extraction and item classification - the two critical aspects of data analysis
in the retail industry. Each sample includes annotations for item names and
attributes such as price, brand, and more. This detailed annotation facilitates
a comprehensive understanding of each item on the receipt. Furthermore, the
dataset provides classification into $44$ distinct product categories. This
classification feature allows for a more organized and efficient analysis of
the items, enhancing the usability of the dataset for various applications. In
our study, we evaluated various language model architectures, e.g., by
fine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional
results, with an F1 score of 97.43\% and accuracy of 94.99\% in information
extraction and classification, and an even higher F1 score of 98.51\% and
accuracy of 97.06\% observed in specific tasks. The dataset and code are
publicly accessible for further
researchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training <span class="highlight-title">BERT</span> Models to Carry Over a Coding System Developed on One
  Corpus to Another <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalma Galambos, Pál Zsámboki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes how we train BERT models to carry over a coding system
developed on the paragraphs of a Hungarian literary journal to another. The aim
of the coding system is to track trends in the perception of literary
translation around the political transformation in 1989 in Hungary. To evaluate
not only task performance but also the consistence of the annotation, moreover,
to get better predictions from an ensemble, we use 10-fold crossvalidation.
Extensive hyperparameter tuning is used to obtain the best possible results and
fair comparisons. To handle label imbalance, we use loss functions and metrics
robust to it. Evaluation of the effect of domain shift is carried out by
sampling a test set from the target domain. We establish the sample size by
estimating the bootstrapped confidence interval via simulations. This way, we
show that our models can carry over one annotation system to the target domain.
Comparisons are drawn to provide insights such as learning multilabel
correlations and confidence penalty improve resistance to domain shift, and
domain adaptation on OCR-ed text on another domain improves performance almost
to the same extent as that on the corpus under study. See our code at
https://codeberg.org/zsamboki/bert-annotator-ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version, to be presented at the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blinded by Generated Contexts: How Language Models Merge Generated and
  Retrieved Contexts for Open-Domain QA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While auxiliary information has become a key to enhancing Large Language
Models (LLMs), relatively little is known about how LLMs merge these contexts,
specifically contexts generated by LLMs and those retrieved from external
sources. To investigate this, we formulate a systematic framework to identify
whether LLMs' responses, derived from the integration of generated and
retrieved contexts, are attributed to either generated or retrieved contexts.
To easily trace the origin of the response, we construct datasets with
conflicting contexts, i.e., each question is paired with both generated and
retrieved contexts, yet only one of them contains the correct answer. Our
experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to
favor generated contexts, even when they provide incorrect information. We
further identify two key factors contributing to this bias: i) contexts
generated by LLMs typically show greater similarity to the questions,
increasing their likelihood of being selected; ii) the segmentation process
used in retrieved contexts disrupts their completeness, thereby hindering their
full utilization in LLMs. Our analysis enhances the understanding of how LLMs
merge diverse contexts, offering valuable insights for advancing current
augmentation methods for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Video Object Segmentation via Modulated Cross-Attention Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Shaker, Syed Talal Wasim, Martin Danelljan, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, transformer-based approaches have shown promising results for
semi-supervised video object segmentation. However, these approaches typically
struggle on long videos due to increased GPU memory demands, as they frequently
expand the memory bank every few frames. We propose a transformer-based
approach, named MAVOS, that introduces an optimized and dynamic long-term
modulated cross-attention (MCA) memory to model temporal smoothness without
requiring frequent memory expansion. The proposed MCA effectively encodes both
local and global features at various levels of granularity while efficiently
maintaining consistent speed regardless of the video length. Extensive
experiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,
demonstrate the effectiveness of our proposed contributions leading to
real-time inference and markedly reduced memory demands without any degradation
in segmentation accuracy on long videos. Compared to the best existing
transformer-based approach, our MAVOS increases the speed by 7.6x, while
significantly reducing the GPU memory by 87% with comparable segmentation
performance on short and long video datasets. Notably on the LVOS dataset, our
MAVOS achieves a J&F score of 63.3% while operating at 37 frames per second
(FPS) on a single V100 GPU. Our code and models will be publicly available at:
https://github.com/Amshaker/MAVOS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture
  Synthesis <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Hamza Mughal, Rishabh Dabral, Ikhsanul Habibie, Lucia Donatelli, Marc Habermann, Christian Theobalt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gestures play a key role in human communication. Recent methods for co-speech
gesture generation, while managing to generate beat-aligned motions, struggle
generating gestures that are semantically aligned with the utterance. Compared
to beat gestures that align naturally to the audio signal, semantically
coherent gestures require modeling the complex interactions between the
language and human motion, and can be controlled by focusing on certain words.
Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal
gesture synthesis, which can not only generate gestures based on multi-modal
speech inputs, but can also facilitate controllability in gesture synthesis.
Our method proposes two guidance objectives that allow the users to modulate
the impact of different conditioning modalities (e.g. audio vs text) as well as
to choose certain words to be emphasized during gesturing. Our method is
versatile in that it can be trained either for generating monologue gestures or
even the conversational gestures. To further advance the research on
multi-party interactive gestures, the DnD Group Gesture dataset is released,
which contains 6 hours of gesture data showing 5 people interacting with one
another. We compare our method with several recent works and demonstrate
effectiveness of our method on a variety of tasks. We urge the reader to watch
our supplementary video at our website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Project Page:
  https://vcai.mpi-inf.mpg.de/projects/ConvoFusion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniVid: A Generative Framework for Universal Video Understanding <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core of video understanding tasks, such as recognition, captioning, and
tracking, is to automatically detect objects or actions in a video and analyze
their temporal evolution. Despite sharing a common goal, different tasks often
rely on distinct model architectures and annotation formats. In contrast,
natural language processing benefits from a unified output space, i.e., text
sequences, which simplifies the training of powerful foundational language
models, such as GPT-3, with extensive training corpora. Inspired by this, we
seek to unify the output space of video understanding tasks by using languages
as labels and additionally introducing time and box tokens. In this way, a
variety of video tasks could be formulated as video-grounded token generation.
This enables us to address various types of video tasks, including
classification (such as action recognition), captioning (covering clip
captioning, video question answering, and dense video captioning), and
localization tasks (such as visual object tracking) within a fully shared
encoder-decoder architecture, following a generative framework. Through
comprehensive experiments, we demonstrate such a simple and straightforward
idea is quite effective and can achieve state-of-the-art or competitive results
on seven video benchmarks, providing a novel perspective for more universal
video understanding. Code is available at https://github.com/wangjk666/OmniVid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingping Sun, Yanjun Wang, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiyi Mei, Chi Sing Leung, Ziwei Liu, Lei Yang, Zhongang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh
recovery) involves the human body, hand, and expression estimation. Most
existing methods have tackled this task in a two-stage manner, first detecting
the human body part with an off-the-shelf detection model and inferring the
different human body parts individually. Despite the impressive results
achieved, these methods suffer from 1) loss of valuable contextual information
via cropping, 2) introducing distractions, and 3) lacking inter-association
among different persons and body parts, inevitably causing performance
degradation, especially for crowded scenes. To address these issues, we
introduce a novel all-in-one-stage framework, AiOS, for multiple expressive
human pose and shape recovery without an additional human detection step.
Specifically, our method is built upon DETR, which treats multi-person
whole-body mesh recovery task as a progressive set prediction problem with
various sequential detection. We devise the decoder tokens and extend them to
our task. Specifically, we first employ a human token to probe a human location
in the image and encode global features for each instance, which provides a
coarse location for the later transformer block. Then, we introduce a
joint-related token to probe the human joint in the image and encoder a
fine-grained local feature, which collaborates with the global feature to
regress the whole-body mesh. This straightforward but effective model
outperforms previous state-of-the-art methods by a 9% reduction in NMVE on
AGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a
3% reduction in PVE on EgoBody.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://ttxskk.github.io/AiOS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track Everything Everywhere Fast and Robustly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, Kostas Daniilidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel test-time optimization approach for efficiently and
robustly tracking any pixel at any time in a video. The latest state-of-the-art
optimization-based tracking technique, OmniMotion, requires a prohibitively
long optimization time, rendering it impractical for downstream applications.
OmniMotion is sensitive to the choice of random seeds, leading to unstable
convergence. To improve efficiency and robustness, we introduce a novel
invertible deformation network, CaDeX++, which factorizes the function
representation into a local spatial-temporal feature grid and enhances the
expressivity of the coupling blocks with non-linear functions. While CaDeX++
incorporates a stronger geometric bias within its architectural design, it also
takes advantage of the inductive bias provided by the vision foundation models.
Our system utilizes monocular depth estimation to represent scene geometry and
enhances the objective by incorporating DINOv2 long-term semantics to regulate
the optimization process. Our experiments demonstrate a substantial improvement
in training speed (more than \textbf{10 times} faster), robustness, and
accuracy in tracking over the SoTA optimization-based method OmniMotion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://timsong412.github.io/FastOmniTrack/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explaining Hypercomplex Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Lopez, Eleonora Grassucci, Debora Capriotti, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypercomplex neural networks are gaining increasing interest in the deep
learning community. The attention directed towards hypercomplex models
originates from several aspects, spanning from purely theoretical and
mathematical characteristics to the practical advantage of lightweight models
over conventional networks, and their unique properties to capture both global
and local relations. In particular, a branch of these architectures,
parameterized hypercomplex neural networks (PHNNs), has also gained popularity
due to their versatility across a multitude of application domains.
Nonetheless, only few attempts have been made to explain or interpret their
intricacies. In this paper, we propose inherently interpretable PHNNs and
quaternion-like networks, thus without the need for any post-hoc method. To
achieve this, we define a type of cosine-similarity transform within the
parameterized hypercomplex domain. This PHB-cos transform induces weight
alignment with relevant input features and allows to reduce the model into a
single linear transform, rendering it directly interpretable. In this work, we
start to draw insights into how this unique branch of neural models operates.
We observe that hypercomplex networks exhibit a tendency to concentrate on the
shape around the main object of interest, in addition to the shape of the
object itself. We provide a thorough analysis, studying single neurons of
different layers and comparing them against how real-valued networks learn. The
code of the paper is available at https://github.com/ispamm/HxAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted at IEEE WCCI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastCAR: Fast Classification And Regression Multi-Task Learning via Task
  Consolidation for Modelling a Continuous Property Variable of Object Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anoop Kini, Andreas Jansche, Timo Bernthaler, Gerhard Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)
for a classification and a regression task, despite task heterogeneity with
only subtle correlation. It addresses object classification and continuous
property variable regression, a crucial use case in science and engineering.
FastCAR involves a labeling transformation approach that can be used with a
single-task regression network architecture. FastCAR outperforms traditional
MTL model families, parametrized in the landscape of architecture and loss
weighting schemes, when learning of both tasks are collectively considered
(classification accuracy of 99.54%, regression mean absolute percentage error
of 2.3%). The experiments performed used an Advanced Steel Property dataset
contributed by us. The dataset comprises 4536 images of 224x224 pixels,
annotated with object classes and hardness properties that take continuous
values. With the labeling transformation and single-task regression network
architecture, FastCAR achieves reduced latency and time efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AID: Attention Interpolation of Text-to-Image Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional diffusion models can create unseen images in various settings,
aiding image interpolation. Interpolation in latent spaces is well-studied, but
interpolation with specific conditions like text or poses is less understood.
Simple approaches, such as linear interpolation in the space of conditions,
often result in images that lack consistency, smoothness, and fidelity. To that
end, we introduce a novel training-free technique named Attention Interpolation
via Diffusion (AID). Our key contributions include 1) proposing an inner/outer
interpolated attention layer; 2) fusing the interpolated attention with
self-attention to boost fidelity; and 3) applying beta distribution to
selection to increase smoothness. We also present a variant, Prompt-guided
Attention Interpolation via Diffusion (PAID), that considers interpolation as a
condition-dependent generative process. This method enables the creation of new
images with greater consistency, smoothness, and efficiency, and offers control
over the exact path of interpolation. Our approach demonstrates effectiveness
for conceptual and spatial interpolation. Code and demo are available at
https://github.com/QY-H00/attention-interpolation-diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TC4D: Trajectory-Conditioned Text-to-4D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, Andrea Tagliasacchi, David B. Lindell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes
using supervision from pre-trained text-to-video models. However, existing
representations for motion, such as deformation models or time-dependent neural
representations, are limited in the amount of motion they can generate-they
cannot synthesize motion extending far beyond the bounding box used for volume
rendering. The lack of a more flexible motion model contributes to the gap in
realism between 4D generation methods and recent, near-photorealistic video
generation models. Here, we propose TC4D: trajectory-conditioned text-to-4D
generation, which factors motion into global and local components. We represent
the global motion of a scene's bounding box using rigid transformation along a
trajectory parameterized by a spline. We learn local deformations that conform
to the global trajectory using supervision from a text-to-video model. Our
approach enables the synthesis of scenes animated along arbitrary trajectories,
compositional scene generation, and significant improvements to the realism and
amount of generated motion, which we evaluate qualitatively and through a user
study. Video results can be viewed on our website:
https://sherwinbahmani.github.io/tc4d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sherwinbahmani.github.io/tc4d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Near-Field Lighting for Monocular Depth Estimation from
  Endoscopy Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Paruchuri, Samuel Ehrenstein, Shuxian Wang, Inbar Fried, Stephen M. Pizer, Marc Niethammer, Roni Sengupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation in endoscopy videos can enable assistive and
robotic surgery to obtain better coverage of the organ and detection of various
health issues. Despite promising progress on mainstream, natural image depth
estimation, techniques perform poorly on endoscopy images due to a lack of
strong geometric features and challenging illumination effects. In this paper,
we utilize the photometric cues, i.e., the light emitted from an endoscope and
reflected by the surface, to improve monocular depth estimation. We first
create two novel loss functions with supervised and self-supervised variants
that utilize a per-pixel shading representation. We then propose a novel depth
refinement network (PPSNet) that leverages the same per-pixel shading
representation. Finally, we introduce teacher-student transfer learning to
produce better depth maps from both synthetic data with supervision and
clinical data with self-supervision. We achieve state-of-the-art results on the
C3VD dataset while estimating high-quality depth maps from clinical data. Our
code, pre-trained models, and supplementary materials can be found on our
project page: https://ppsnet.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 7 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing
  Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashir Noman, Mustansar Fiaz, Hisham Cholakkal, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has shown remarkable success in remote sensing change detection
(CD), aiming to identify semantic change regions between co-registered
satellite image pairs acquired at distinct time stamps. However, existing
convolutional neural network and transformer-based frameworks often struggle to
accurately segment semantic change regions. Moreover, transformers-based
methods with standard self-attention suffer from quadratic computational
complexity with respect to the image resolution, making them less practical for
CD tasks with limited training data. To address these issues, we propose an
efficient change detection framework, ELGC-Net, which leverages rich contextual
information to precisely estimate change regions while reducing the model size.
Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The
focus of our design is the introduction of an Efficient Local-Global Context
Aggregator module within the encoder, capturing enhanced global context and
local spatial information through a novel pooled-transpose (PT) attention and
depthwise convolution, respectively. The PT attention employs pooling
operations for robust feature extraction and minimizes computational cost with
transposed attention. Extensive experiments on three challenging CD datasets
demonstrate that ELGC-Net outperforms existing methods. Compared to the recent
transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in
intersection over union metric on the LEVIR-CD dataset, while significantly
reducing trainable parameters. Our proposed ELGC-Net sets a new
state-of-the-art performance in remote sensing change detection benchmarks.
Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly
reduced computational complexity, suitable for resource-constrained settings,
while achieving comparable performance. Project url
https://github.com/techmn/elgcnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IEEE TGRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yiwei, Tang Chao, Aghabiglou Amir, Chu Chung San, Wiaux Yves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serpent: Scalable and Efficient Image Restoration via Multi-scale
  Structured State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shahab Sepehri, Zalan Fabian, Mahdi Soltanolkotabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of computational building blocks of efficient image restoration
architectures is dominated by a combination of convolutional processing and
various attention mechanisms. However, convolutional filters are inherently
local and therefore struggle at modeling long-range dependencies in images. On
the other hand, attention excels at capturing global interactions between
arbitrary image regions, however at a quadratic cost in image dimension. In
this work, we propose Serpent, an architecture that leverages recent advances
in state space models (SSMs) in its core computational block. SSMs, originally
introduced for sequence modeling, can maintain a global receptive field with a
favorable linear scaling in input size. Our preliminary results demonstrate
that Serpent can achieve reconstruction quality on par with state-of-the-art
techniques, while requiring orders of magnitude less compute (up to $150$ fold
reduction in FLOPS) and a factor of up to $5\times$ less GPU memory while
maintaining a compact model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, preliminary workshop submission of a
  comprehensive work to be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D
  Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering
fidelity and efficiency compared to NeRF-based neural scene representations.
While demonstrating the potential for real-time rendering, 3D-GS encounters
rendering bottlenecks in large scenes with complex details due to an excessive
number of Gaussian primitives located within the viewing frustum. This
limitation is particularly noticeable in zoom-out views and can lead to
inconsistent rendering speeds in scenes with varying details. Moreover, it
often struggles to capture the corresponding level of details at different
scales with its heuristic density control operation. Inspired by the
Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an
LOD-structured 3D Gaussian approach supporting level-of-detail decomposition
for scene representation that contributes to the final rendering results. Our
model dynamically selects the appropriate level from the set of
multi-resolution anchor points, ensuring consistent rendering performance with
adaptive LOD adjustments while maintaining high-fidelity rendering results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://city-super.github.io/octree-gs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on 3D Egocentric Human Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mushfiqur Azam, Kevin Desai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric human pose estimation aims to estimate human body poses and
develop body representations from a first-person camera perspective. It has
gained vast popularity in recent years because of its wide range of
applications in sectors like XR-technologies, human-computer interaction, and
fitness tracking. However, to the best of our knowledge, there is no systematic
literature review based on the proposed solutions regarding egocentric 3D human
pose estimation. To that end, the aim of this survey paper is to provide an
extensive overview of the current state of egocentric pose estimation research.
In this paper, we categorize and discuss the popular datasets and the different
pose estimation models, highlighting the strengths and weaknesses of different
methods by comparative analysis. This survey can be a valuable resource for
both researchers and practitioners in the field, offering insights into key
concepts and cutting-edge solutions in egocentric pose estimation, its
wide-ranging applications, as well as the open problems with future scope.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently revolutionized radiance field
reconstruction, achieving high quality novel view synthesis and fast rendering
speed without baking. However, 3DGS fails to accurately represent surfaces due
to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian
Splatting (2DGS), a novel approach to model and reconstruct geometrically
accurate radiance fields from multi-view images. Our key idea is to collapse
the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D
Gaussians, 2D Gaussians provide view-consistent geometry while modeling
surfaces intrinsically. To accurately recover thin surfaces and achieve stable
optimization, we introduce a perspective-accurate 2D splatting process
utilizing ray-splat intersection and rasterization. Additionally, we
incorporate depth distortion and normal consistency terms to further enhance
the quality of the reconstructions. We demonstrate that our differentiable
renderer allows for noise-free and detailed geometry reconstruction while
maintaining competitive appearance quality, fast training speed, and real-time
rendering. Our code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sen2Fire: A Challenging Benchmark <span class="highlight-title">Dataset</span> for Wildfire Detection using
  Sentinel Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Xu, Amanda Berg, Leif Haglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing satellite imagery for wildfire detection presents substantial
potential for practical applications. To advance the development of machine
learning algorithms in this domain, our study introduces the \textit{Sen2Fire}
dataset--a challenging satellite remote sensing dataset tailored for wildfire
detection. This dataset is curated from Sentinel-2 multi-spectral data and
Sentinel-5P aerosol product, comprising a total of 2466 image patches. Each
patch has a size of 512$\times$512 pixels with 13 bands. Given the distinctive
sensitivities of various wavebands to wildfire responses, our research focuses
on optimizing wildfire detection by evaluating different wavebands and
employing a combination of spectral indices, such as normalized burn ratio
(NBR) and normalized difference vegetation index (NDVI). The results suggest
that, in contrast to using all bands for wildfire detection, selecting specific
band combinations yields superior performance. Additionally, our study
underscores the positive impact of integrating Sentinel-5 aerosol data for
wildfire detection. The code and dataset are available online
(https://zenodo.org/records/10881058).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior and Pragmatic Talking Face Generation with Teacher-Student
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Liang, Jianwen Jiang, Tianyun Zhong, Gaojie Lin, Zhengkun Rong, Jiaqi Yang, Yongming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking face generation technology creates talking videos from arbitrary
appearance and motion signal, with the "arbitrary" offering ease of use but
also introducing challenges in practical applications. Existing methods work
well with standard inputs but suffer serious performance degradation with
intricate real-world ones. Moreover, efficiency is also an important concern in
deployment. To comprehensively address these issues, we introduce SuperFace, a
teacher-student framework that balances quality, robustness, cost and
editability. We first propose a simple but effective teacher model capable of
handling inputs of varying qualities to generate high-quality results. Building
on this, we devise an efficient distillation strategy to acquire an
identity-specific student model that maintains quality with significantly
reduced computational load. Our experiments validate that SuperFace offers a
more comprehensive solution than existing methods for the four mentioned
objectives, especially in reducing FLOPs by 99\% with the student model.
SuperFace can be driven by both video and audio and allows for localized facial
attributes editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepfake Generation and Detection: A Benchmark and <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gan Pei, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In addition to the advancements in deepfake generation, corresponding
detection technologies need to continuously evolve to regulate the potential
misuse of deepfakes, such as for privacy invasion and phishing attacks. This
survey comprehensively reviews the latest developments in deepfake generation
and detection, summarizing and analyzing the current state of the art in this
rapidly evolving field. We first unify task definitions, comprehensively
introduce datasets and metrics, and discuss the development of generation and
detection technology frameworks. Then, we discuss the development of several
related sub-fields and focus on researching four mainstream deepfake fields:
popular face swap, face reenactment, talking face generation, and facial
attribute editing, as well as foreign detection. Subsequently, we
comprehensively benchmark representative methods on popular datasets for each
field, fully evaluating the latest and influential works published in top
conferences/journals. Finally, we analyze the challenges and future research
directions of the discussed fields. We closely follow the latest developments
in https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Latency Neural Stereo Streaming <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiqi Hou, Farzad Farhadzadeh, Amir Said, Guillaume Sautiere, Hoang Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of new video modalities like virtual reality or autonomous driving
has increased the demand for efficient multi-view video compression methods,
both in terms of rate-distortion (R-D) performance and in terms of delay and
runtime. While most recent stereo video compression approaches have shown
promising performance, they compress left and right views sequentially, leading
to poor parallelization and runtime performance. This work presents Low-Latency
neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video
coding method designed for fast and efficient low-latency stereo video
streaming. Instead of using a sequential cross-view motion compensation like
existing methods, LLSS introduces a bidirectional feature shifting module to
directly exploit mutual information among views and encode them effectively
with a joint cross-view prior model for entropy coding. Thanks to this design,
LLSS processes left and right views in parallel, minimizing latency; all while
substantially improving R-D performance compared to both existing neural and
conventional codecs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Diffusion Models with Moving Average Sampling in Frequency
  Domain <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently brought a powerful revolution in image
generation. Despite showing impressive generative capabilities, most of these
models rely on the current sample to denoise the next one, possibly resulting
in denoising instability. In this paper, we reinterpret the iterative denoising
process as model optimization and leverage a moving average mechanism to
ensemble all the prior samples. Instead of simply applying moving average to
the denoised samples at different timesteps, we first map the denoised samples
to data space and then perform moving average to avoid distribution shift
across timesteps. In view that diffusion models evolve the recovery from
low-frequency components to high-frequency details, we further decompose the
samples into different frequency components and execute moving average
separately on each component. We name the complete approach "Moving Average
Sampling in Frequency domain (MASF)". MASF could be seamlessly integrated into
mainstream pre-trained diffusion models and sampling schedules. Extensive
experiments on both unconditional and conditional diffusion models demonstrate
that our MASF leads to superior performances compared to the baselines, with
almost negligible additional complexity cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Supervise or Not to Supervise: Understanding and Addressing the Key
  Challenges of 3D Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souhail Hadgi, Lei Li, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning has long been a key factor in the advancement of many
fields including 2D image analysis. Unfortunately, its applicability in 3D data
processing has been relatively limited. While several approaches for 3D
transfer learning have been proposed in recent literature, with contrastive
learning gaining particular prominence, most existing methods in this domain
have only been studied and evaluated in limited scenarios. Most importantly,
there is currently a lack of principled understanding of both when and why 3D
transfer learning methods are applicable. Remarkably, even the applicability of
standard supervised pre-training is poorly understood. In this work, we conduct
the first in-depth quantitative and qualitative investigation of supervised and
contrastive pre-training strategies and their utility in downstream 3D tasks.
We demonstrate that layer-wise analysis of learned features provides
significant insight into the downstream utility of trained networks. Informed
by this analysis, we propose a simple geometric regularization strategy, which
improves the transferability of supervised pre-training. Our work thus sheds
light onto both the specific challenges of 3D transfer learning, as well as
strategies to overcome them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReMamber: Referring Image Segmentation with Mamba Twister 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhuan Yang, Chaofan Ma, Jiangchao Yao, Zhun Zhong, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Image Segmentation (RIS) leveraging transformers has achieved great
success on the interpretation of complex visual-language tasks. However, the
quadratic computation cost makes it resource-consuming in capturing long-range
visual-language dependencies. Fortunately, Mamba addresses this with efficient
linear complexity in processing. However, directly applying Mamba to
multi-modal interactions presents challenges, primarily due to inadequate
channel interactions for the effective fusion of multi-modal data. In this
paper, we propose ReMamber, a novel RIS architecture that integrates the power
of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly
models image-text interaction, and fuses textual and visual features through
its unique channel and spatial twisting mechanism. We achieve the
state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough
analyses of ReMamber and discuss other fusion designs using Mamba. These
provide valuable perspectives for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of datasets, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed dataset, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed dataset and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The dataset, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A foundation model utilizing chest CT volumes and radiology reports for
  supervised-level zero-shot detection of abnormalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Dogan, Muhammed Furkan Dasdelen, Bastian Wittmann, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, Anjany Sekuboyina, Berkan Lafci, Mehmet K. Ozdemir, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in computational research in 3D medical imaging is the lack
of comprehensive datasets. Addressing this issue, our study introduces CT-RATE,
the first 3D medical imaging dataset that pairs images with textual reports.
CT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188
through various reconstructions, from 21,304 unique patients, along with
corresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP,
a CT-focused contrastive language-image pre-training framework. As a versatile,
self-supervised model, CT-CLIP is designed for broad application and does not
require task-specific training. Remarkably, CT-CLIP outperforms
state-of-the-art, fully supervised methods in multi-abnormality detection
across all key metrics, thus eliminating the need for manual annotation. We
also demonstrate its utility in case retrieval, whether using imagery or
textual queries, thereby advancing knowledge dissemination. The open-source
release of CT-RATE and CT-CLIP marks a significant advancement in medical AI,
enhancing 3D imaging analysis and fostering innovation in healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessment of Multimodal Large Language Models in Alignment with Human
  Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu Sheng, Yu Qiao, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) aim to serve as versatile assistants aligned
with human values, as defined by the principles of being helpful, honest, and
harmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs),
despite their commendable performance in perception and reasoning tasks, their
alignment with human values remains largely unexplored, given the complexity of
defining hhh dimensions in the visual world and the difficulty in collecting
relevant data that accurately mirrors real-world situations. To address this
gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for
assessing alignment with human expectations. Ch3Ef dataset contains 1002
human-annotated data samples, covering 12 domains and 46 tasks based on the hhh
principle. We also present a unified evaluation strategy supporting assessment
across various scenarios and different perspectives. Based on the evaluation
results, we summarize over 10 key findings that deepen the understanding of
MLLM capabilities, limitations, and the dynamic relationships between
evaluation levels, guiding future advancements in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2311.02692</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from
  Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural hand-object interactions in 3D is challenging as the
resulting hand and object motions are expected to be physically plausible and
semantically meaningful. Furthermore, generalization to unseen objects is
hindered by the limited scale of available hand-object interaction datasets. We
propose DiffH2O, a novel method to synthesize realistic, one or two-handed
object interactions from provided text prompts and geometry of the object. The
method introduces three techniques that enable effective learning from limited
data. First, we decompose the task into a grasping stage and a text-based
interaction stage and use separate diffusion models for each. In the grasping
stage, the model only generates hand motions, whereas in the interaction phase
both hand and object poses are synthesized. Second, we propose a compact
representation that tightly couples hand and object poses. Third, we propose
two different guidance schemes to allow more control of the generated motions:
grasp guidance and detailed textual guidance. Grasp guidance takes a single
target grasping pose and guides the diffusion model to reach this grasp at the
end of the grasping stage, which provides control over the grasping pose. Given
a grasping motion from this stage, multiple different actions can be prompted
in the interaction phase. For textual guidance, we contribute comprehensive
text descriptions to the GRAB dataset and show that they enable our method to
have more fine-grained control over hand-object interactions. Our quantitative
and qualitative evaluation demonstrates that the proposed method outperforms
baseline methods and leads to natural hand-object motions. Moreover, we
demonstrate the practicality of our framework by utilizing a hand pose estimate
from an off-the-shelf pose estimator for guidance, and then sampling multiple
different actions in the interaction stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://diffh2o.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Image <span class="highlight-title">Pre-Train</span>ing with Siamese Cropped Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Eymaël, Renaud Vandeghen, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-training of image encoders is omnipresent in the
literature, particularly following the introduction of Masked autoencoders
(MAE). Current efforts attempt to learn object-centric representations from
motion in videos. In particular, SiamMAE recently introduced a Siamese network,
training a shared-weight encoder from two frames of a video with a high
asymmetric masking ratio (95%). In this work, we propose CropMAE, an
alternative approach to the Siamese pre-training introduced by SiamMAE. Our
method specifically differs by exclusively considering pairs of cropped images
sourced from the same image but cropped differently, deviating from the
conventional pairs of frames extracted from a video. CropMAE therefore
alleviates the need for video datasets, while maintaining competitive
performances and drastically reducing pre-training time. Furthermore, we
demonstrate that CropMAE learns similar object-centric representations without
explicit motion, showing that current self-supervised learning methods do not
learn objects from motion, but rather thanks to the Siamese architecture.
Finally, CropMAE achieves the highest masking ratio to date (98.5%), enabling
the reconstruction of images using only two visible patches. Our code is
available at https://github.com/alexandre-eymael/CropMAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 3 tables, 1 page of supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting, a novel differentiable rendering technique, has
achieved state-of-the-art novel view synthesis results with high rendering
speeds and relatively low training times. However, its performance on scenes
commonly seen in indoor datasets is poor due to the lack of geometric
constraints during optimization. We extend 3D Gaussian splatting with depth and
normal cues to tackle challenging indoor datasets and showcase techniques for
efficient mesh extraction, an important downstream application. Specifically,
we regularize the optimization procedure with depth information, enforce local
smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians
supervised by normal cues to achieve better alignment with the true scene
geometry. We improve depth estimation and novel view synthesis results over
baselines and show how this simple yet effective regularization technique can
be used to directly extract meshes from the Gaussian representation yielding
more physically accurate reconstructions on indoor scenes. Our code will be
released in https://github.com/maturk/dn-splatter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotated Biomedical Video Generation using Denoising Diffusion
  Probabilistic Models and Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rüveyda Yilmaz, Dennis Eschweiler, Johannes Stegmaier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation and tracking of living cells play a vital role within the
biomedical domain, particularly in cancer research, drug development, and
developmental biology. These are usually tedious and time-consuming tasks that
are traditionally done by biomedical experts. Recently, to automatize these
processes, deep learning based segmentation and tracking methods have been
proposed. These methods require large-scale datasets and their full potential
is constrained by the scarcity of annotated data in the biomedical imaging
domain. To address this limitation, we propose Biomedical Video Diffusion Model
(BVDM), capable of generating realistic-looking synthetic microscopy videos.
Trained only on a single real video, BVDM can generate videos of arbitrary
length with pixel-level annotations that can be used for training data-hungry
models. It is composed of a denoising diffusion probabilistic model (DDPM)
generating high-fidelity synthetic cell microscopy images and a flow prediction
model (FPM) predicting the non-rigid transformation between consecutive video
frames. During inference, initially, the DDPM imposes realistic cell textures
on synthetic cell masks which are generated based on real data statistics. The
flow prediction model predicts the flow field between consecutive masks and
applies that to the DDPM output from the previous time frame to create the next
one while keeping temporal consistency. BVDM outperforms state-of-the-art
synthetic live cell microscopy video generation models. Furthermore, we
demonstrate that a sufficiently large synthetic dataset enhances the
performance of cell segmentation and tracking models compared to using a
limited amount of available real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text-to-Image Consistency via Automatic <span class="highlight-title">Prompt</span> Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal Drozdzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive advances in text-to-image (T2I) generative models have yielded a
plethora of high performing models which are able to generate aesthetically
appealing, photorealistic images. Despite the progress, these models still
struggle to produce images that are consistent with the input prompt,
oftentimes failing to capture object quantities, relations and attributes
properly. Existing solutions to improve prompt-image consistency suffer from
the following challenges: (1) they oftentimes require model fine-tuning, (2)
they only focus on nearby prompt samples, and (3) they are affected by
unfavorable trade-offs among image quality, representation diversity, and
prompt-image consistency. In this paper, we address these challenges and
introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a
large language model (LLM) to improve prompt-image consistency in T2I models.
Our framework starts from a user prompt and iteratively generates revised
prompts with the goal of maximizing a consistency score. Our extensive
validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost
the initial consistency score by up to 24.9% in terms of DSG score while
preserving the FID and increasing the recall between generated and real data.
Our work paves the way toward building more reliable and robust T2I systems by
harnessing the power of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards 3D Vision with Low-Cost Single-Photon Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangzhou Mu, Carter Sifferman, Sacha Jungerman, Yiquan Li, Mark Han, Michael Gleicher, Mohit Gupta, Yin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for reconstructing 3D shape of arbitrary Lambertian
objects based on measurements by miniature, energy-efficient, low-cost
single-photon cameras. These cameras, operating as time resolved image sensors,
illuminate the scene with a very fast pulse of diffuse light and record the
shape of that pulse as it returns back from the scene at a high temporal
resolution. We propose to model this image formation process, account for its
non-idealities, and adapt neural rendering to reconstruct 3D geometry from a
set of spatially distributed sensors with known poses. We show that our
approach can successfully recover complex 3D shapes from simulated data. We
further demonstrate 3D object reconstruction from real-world captures,
utilizing measurements from a commodity proximity sensor. Our work draws a
connection between image-based modeling and active range scanning and is a step
towards 3D vision with single-photon cameras.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Efficacy of <span class="highlight-title">Prompt</span>-Engineered Large Multimodal Models
  Versus Fine-Tuned Vision <span class="highlight-title">Transformer</span>s in Image-Based Security Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouad Trad, Ali Chehab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Large Language Models (LLMs) has led to a parallel rise in the
development of Large Multimodal Models (LMMs), such as Gemini-pro, which have
begun to transform a variety of applications. These sophisticated multimodal
models are designed to interpret and analyze complex data, integrating both
textual and visual information on a scale previously unattainable, opening new
avenues for a range of applications. This paper investigates the applicability
and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision
Transformer (ViT) models in addressing critical security challenges. We focus
on two distinct tasks: a visually evident task of detecting simple triggers,
such as small squares in images, indicative of potential backdoors, and a
non-visually evident task of malware classification through visual
representations. Our results highlight a significant divergence in performance,
with Gemini-pro falling short in accuracy and reliability when compared to
fine-tuned ViT models. The ViT models, on the other hand, demonstrate
exceptional accuracy, achieving near-perfect performance on both tasks. This
study not only showcases the strengths and limitations of prompt-engineered
LMMs in cybersecurity applications but also emphasizes the unmatched efficacy
of fine-tuned ViT models for precise and dependable tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenesisTex: Adapting Image Denoising Diffusion to Texture Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenjian Gao, Boyan Jiang, Xinghui Li, Yingpeng Zhang, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GenesisTex, a novel method for synthesizing textures for 3D
geometries from text descriptions. GenesisTex adapts the pretrained image
diffusion model to texture space by texture space sampling. Specifically, we
maintain a latent texture map for each viewpoint, which is updated with
predicted noise on the rendering of the corresponding viewpoint. The sampled
latent texture maps are then decoded into a final texture map. During the
sampling process, we focus on both global and local consistency across multiple
viewpoints: global consistency is achieved through the integration of style
consistency mechanisms within the noise prediction network, and low-level
consistency is achieved by dynamically aligning latent textures. Finally, we
apply reference-based inpainting and img2img on denser views for texture
refinement. Our approach overcomes the limitations of slow optimization in
distillation-based methods and instability in inpainting-based methods.
Experiments on meshes from various sources demonstrate that our method
surpasses the baseline methods quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongrui Yu, Hanyu Chen, Zitian Zhang, Qiong Xiao, Wenhui Lei, Linrui Dai, Yu Fu, Hui Tan, Guan Wang, Peng Gao, Xiaofan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant success achieved by deep learning methods in medical
image segmentation, researchers still struggle in the computer-aided diagnosis
of abdominal lymph nodes due to the complex abdominal environment, small and
indistinguishable lesions, and limited annotated data. To address these
problems, we present a pipeline that integrates the conditional diffusion model
for lymph node generation and the nnU-Net model for lymph node segmentation to
improve the segmentation performance of abdominal lymph nodes through
synthesizing a diversity of realistic abdominal lymph node data. We propose
LN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph
node (LN) generation. LN-DDPM utilizes lymph node masks and anatomical
structure masks as model conditions. These conditions work in two conditioning
mechanisms: global structure conditioning and local detail conditioning, to
distinguish between lymph nodes and their surroundings and better capture lymph
node characteristics. The obtained paired abdominal lymph node images and masks
are used for the downstream segmentation task. Experimental results on the
abdominal lymph node datasets demonstrate that LN-DDPM outperforms other
generative methods in the abdominal lymph node image synthesis and better
assists the downstream abdominal lymph node segmentation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yan, Ruomin He, Zhenghua Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing
multiple tri-plane hash-encodings for efficient scene representation. MUTE-SLAM
effectively tracks camera positions and incrementally builds a scalable
multi-map representation for both small and large indoor environments. It
dynamically allocates sub-maps for newly observed local regions, enabling
constraint-free mapping without prior scene information. Unlike traditional
grid-based methods, we use three orthogonal axis-aligned planes for
hash-encoding scene properties, significantly reducing hash collisions and the
number of trainable parameters. This hybrid approach not only speeds up
convergence but also enhances the fidelity of surface reconstruction.
Furthermore, our optimization strategy concurrently optimizes all sub-maps
intersecting with the current camera frustum, ensuring global consistency.
Extensive testing on both real-world and synthetic datasets has shown that
MUTE-SLAM delivers state-of-the-art surface reconstruction quality and
competitive tracking performance across diverse indoor settings. The code will
be made public upon acceptance of the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Makeup Prior Models for 3D Facial Makeup Estimation and Applications <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchao Yang, Takafumi Taketomi, Yuki Endo, Yoshihiro Kanamori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce two types of makeup prior models to extend
existing 3D face prior models: PCA-based and StyleGAN2-based priors. The
PCA-based prior model is a linear model that is easy to construct and is
computationally efficient. However, it retains only low-frequency information.
Conversely, the StyleGAN2-based model can represent high-frequency information
with relatively higher computational cost than the PCA-based model. Although
there is a trade-off between the two models, both are applicable to 3D facial
makeup estimation and related applications. By leveraging makeup prior models
and designing a makeup consistency module, we effectively address the
challenges that previous methods faced in robustly estimating makeup,
particularly in the context of handling self-occluded faces. In experiments, we
demonstrate that our approach reduces computational costs by several orders of
magnitude, achieving speeds up to 180 times faster. In addition, by improving
the accuracy of the estimated makeup, we confirm that our methods are highly
advantageous for various 3D facial makeup applications such as 3D makeup face
reconstruction, user-friendly makeup editing, makeup transfer, and
interpolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024. Project: https://yangxingchao.github.io/makeup-priors-page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise2Noise Denoising of CRISM Hyperspectral Data <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Platt, Rossella Arcucci, Cédric John
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral data acquired by the Compact Reconnaissance Imaging
Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the
surface mineralogy of Mars. Due to sensor degradation over time, a significant
portion of the recently acquired data is considered unusable. Here a new
data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to
remove noise from CRISM images. Our model is self-supervised and does not
require zero-noise target data, making it well suited for use in Planetary
Science applications where high quality labelled data is scarce. We demonstrate
its strong performance on synthetic-noise data and CRISM images, and its impact
on downstream classification performance, outperforming benchmark methods on
most metrics. This allows for detailed analysis for critical sites of interest
on the Martian surface, including proposed lander sites.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024
  ML4RS Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DataCook: Crafting Anti-Adversarial Examples for Healthcare Data
  Copyright Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihan Shang, Jiancheng Yang, Zhenglong Sun, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of healthcare, the challenges of copyright protection and
unauthorized third-party misuse are increasingly significant. Traditional
methods for data copyright protection are applied prior to data distribution,
implying that models trained on these data become uncontrollable. This paper
introduces a novel approach, named DataCook, designed to safeguard the
copyright of healthcare data during the deployment phase. DataCook operates by
"cooking" the raw data before distribution, enabling the development of models
that perform normally on this processed data. However, during the deployment
phase, the original test data must be also "cooked" through DataCook to ensure
normal model performance. This process grants copyright holders control over
authorization during the deployment phase. The mechanism behind DataCook is by
crafting anti-adversarial examples (AntiAdv), which are designed to enhance
model confidence, as opposed to standard adversarial examples (Adv) that aim to
confuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,
ensuring that the data processed by DataCook remains easily understandable. We
conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D
data and the high-resolution variants. The outcomes indicate that DataCook
effectively meets its objectives, preventing models trained on AntiAdv from
analyzing unauthorized data effectively, without compromising the validity and
accuracy of the data in legitimate scenarios. Code and data are available at
https://github.com/MedMNIST/DataCook.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Dense Prediction via Mixture of Low-Rank Experts <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Yang, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous multi-task dense prediction methods based on the Mixture of Experts
(MoE) have received great performance but they neglect the importance of
explicitly modeling the global relations among all tasks. In this paper, we
present a novel decoder-focused method for multi-task dense prediction, called
Mixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships,
MLoRE adds a generic convolution path to the original MoE structure, where each
task feature can go through this path for explicit parameter sharing.
Furthermore, to control the parameters and computational cost brought by the
increase in the number of experts, we take inspiration from LoRA and propose to
leverage the low-rank format of a vanilla convolution in the expert network.
Since the low-rank experts have fewer parameters and can be dynamically
parameterized into the generic convolution, the parameters and computational
cost do not change much with the increase of experts. Benefiting from this
design, we increase the number of experts and its reception field to enlarge
the representation capacity, facilitating multiple dense tasks learning in a
unified network. Extensive experiments on the PASCAL-Context and NYUD-v2
benchmarks show that our MLoRE achieves superior performance compared to
previous state-of-the-art methods on all metrics. Our code is available at
https://github.com/YuqiYang213/MLoRE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation
  scans using Linked Denoising Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rowan Bradbury, Katherine A. Vallis, Bartlomiej W. Papiez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Artificial Intelligence (AI) in biomedical imaging
and radiotherapy is hindered by the limited availability of large imaging data
repositories. With recent research and improvements in denoising diffusion
probabilistic models (DDPM), high quality synthetic medical scans are now
possible. Despite this, there is currently no way of generating multiple
related images, such as a corresponding ground truth which can be used to train
models, so synthetic scans are often manually annotated before use. This
research introduces a novel architecture that is able to generate multiple,
related PET-CT-tumour mask pairs using paired networks and conditional
encoders. Our approach includes innovative, time step-controlled mechanisms and
a `noise-seeding' strategy to improve DDPM sampling consistency. While our
model requires a modified perceptual loss function to ensure accurate feature
alignment we show generation of clearly aligned synthetic images and
improvement in segmentation accuracy with generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in IEEE International Symposium on Biomedical Imaging
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastPerson: Enhancing Video Learning through Effective Video
  Summarization that Preserves Linguistic and Visual Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Kawamura, Jun Rekimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly understanding lengthy lecture videos is essential for learners with
limited time and interest in various topics to improve their learning
efficiency. To this end, video summarization has been actively researched to
enable users to view only important scenes from a video. However, these studies
focus on either the visual or audio information of a video and extract
important segments in the video. Therefore, there is a risk of missing
important information when both the teacher's speech and visual information on
the blackboard or slides are important, such as in a lecture video. To tackle
this issue, we propose FastPerson, a video summarization approach that
considers both the visual and auditory information in lecture videos.
FastPerson creates summary videos by utilizing audio transcriptions along with
on-screen images and text, minimizing the risk of overlooking crucial
information for learners. Further, it provides a feature that allows learners
to switch between the summary and original videos for each chapter of the
video, enabling them to adjust the pace of learning based on their interests
and level of understanding. We conducted an evaluation with 40 participants to
assess the effectiveness of our method and confirmed that it reduced viewing
time by 53\% at the same level of comprehension as that when using traditional
video playback methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Segmentation of Cracks in High-Resolution Images of
  Steel Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrii Kompanets, Gautam Pai, Remco Duits, Davide Leonetti, Bert Snijder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating the current bridge visual inspection practices using drones and
image processing techniques is a prominent way to make these inspections more
effective, robust, and less expensive. In this paper, we investigate the
development of a novel deep-learning method for the detection of fatigue cracks
in high-resolution images of steel bridges. First, we present a novel and
challenging dataset comprising of images of cracks in steel bridges. Secondly,
we integrate the ConvNext neural network with a previous state- of-the-art
encoder-decoder network for crack segmentation. We study and report, the
effects of the use of background patches on the network performance when
applied to high-resolution images of cracks in steel bridges. Finally, we
introduce a loss function that allows the use of more background patches for
the training process, which yields a significant reduction in false positive
rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A
  New Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jue Wang, Yuxiang Lin, Qi Zhao, Dong Luo, Shuaibao Chen, Wei Chen, Xiaojiang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of various chemical gases in industrial processes
necessitates effective measures to prevent their leakage during transportation
and storage, given their high toxicity. Thermal infrared-based computer vision
detection techniques provide a straightforward approach to identify gas leakage
areas. However, the development of high-quality algorithms has been challenging
due to the low texture in thermal images and the lack of open-source datasets.
In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN),
which employs an RGB-assisted two-stream network architecture to integrate
texture information from RGB images and gas area information from thermal
images. Additionally, to facilitate the research of invisible gas detection, we
introduce Gas-DB, an extensive open-source gas detection database including
about 1.3K well-annotated RGB-thermal images with eight variant collection
scenes. Experimental results demonstrate that our method successfully leverages
the advantages of both modalities, achieving state-of-the-art (SOTA)
performance among RGB-thermal methods, surpassing single-stream SOTA models in
terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%,
and 4.88%, respectively. The code and data will be made available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Groupwise Query Specialization and Quality-Aware Multi-Assignment for
  <span class="highlight-title">Transformer</span>-based Visual Relationship Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongha Kim, Jihwan Park, Jinyoung Park, Jinyoung Kim, Sehyung Kim, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Relationship Detection (VRD) has seen significant advancements with
Transformer-based architectures recently. However, we identify two key
limitations in a conventional label assignment for training Transformer-based
VRD models, which is a process of mapping a ground-truth (GT) to a prediction.
Under the conventional assignment, an unspecialized query is trained since a
query is expected to detect every relation, which makes it difficult for a
query to specialize in specific relations. Furthermore, a query is also
insufficiently trained since a GT is assigned only to a single prediction,
therefore near-correct or even correct predictions are suppressed by being
assigned no relation as a GT. To address these issues, we propose Groupwise
Query Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise
Query Specialization trains a specialized query by dividing queries and
relations into disjoint groups and directing a query in a specific query group
solely toward relations in the corresponding relation group. Quality-Aware
Multi-Assignment further facilitates the training by assigning a GT to multiple
predictions that are significantly close to a GT in terms of a subject, an
object, and the relation in between. Experimental results and analyses show
that SpeaQ effectively trains specialized queries, which better utilize the
capacity of a model, resulting in consistent performance gains with zero
additional inference cost across multiple VRD models and benchmarks. Code is
available at https://github.com/mlvlab/SpeaQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Panonut360: A Head and Eye Tracking <span class="highlight-title">Dataset</span> for Panoramic Video <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xu, Junhao Du, Jiahe Wang, Yuwei Ning, Sihan Zhou Yang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development and widespread application of VR/AR technology,
maximizing the quality of immersive panoramic video services that match users'
personal preferences and habits has become a long-standing challenge.
Understanding the saliency region where users focus, based on data collected
with HMDs, can promote multimedia encoding, transmission, and quality
assessment. At the same time, large-scale datasets are essential for
researchers and developers to explore short/long-term user behavior patterns
and train AI models related to panoramic videos. However, existing panoramic
video datasets often include low-frequency user head or eye movement data
through short-term videos only, lacking sufficient data for analyzing users'
Field of View (FoV) and generating video saliency regions.
  Driven by these practical factors, in this paper, we present a head and eye
tracking dataset involving 50 users (25 males and 25 females) watching 15
panoramic videos. The dataset provides details on the viewport and gaze
attention locations of users. Besides, we present some statistics samples
extracted from the dataset. For example, the deviation between head and eye
movements challenges the widely held assumption that gaze attention decreases
from the center of the FoV following a Gaussian distribution. Our analysis
reveals a consistent downward offset in gaze fixations relative to the FoV in
experimental settings involving multiple users and videos. That's why we name
the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also
provide a script that generates saliency distributions based on given head or
eye coordinates and pre-generated saliency distribution map sets of each video
from the collected eye tracking data.
  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,ACM MMSys'24 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Solution for the CVPR 2023 1st foundation model challenge-Track2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Xu, Yurui Huang, Sishun Pan, Zhihao Guan, Yi Xu, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a solution for cross-modal transportation
retrieval. Due to the cross-domain problem of traffic images, we divide the
problem into two sub-tasks of pedestrian retrieval and vehicle retrieval
through a simple strategy. In pedestrian retrieval tasks, we use IRRA as the
base model and specifically design an Attribute Classification to mine the
knowledge implied by attribute labels. More importantly, We use the strategy of
Inclusion Relation Matching to make the image-text pairs with inclusion
relation have similar representation in the feature space. For the vehicle
retrieval task, we use BLIP as the base model. Since aligning the color
attributes of vehicles is challenging, we introduce attribute-based object
detection techniques to add color patch blocks to vehicle images for color data
augmentation. This serves as strong prior information, helping the model
perform the image-text alignment. At the same time, we incorporate labeled
attributes into the image-text alignment loss to learn fine-grained alignment
and prevent similar images and texts from being incorrectly separated. Our
approach ranked first in the final B-board test with a score of 70.9.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Tang, Lianglun Cheng, Guoheng Huang, Zhengguang Tan, Junhao Lu, Kaihong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation holds a vital position in the realms of diagnosis and
treatment within the medical domain. Traditional convolutional neural networks
(CNNs) and Transformer models have made significant advancements in this realm,
but they still encounter challenges because of limited receptive field or high
computing complexity. Recently, State Space Models (SSMs), particularly Mamba
and its variants, have demonstrated notable performance in the field of vision.
However, their feature extraction methods may not be sufficiently effective and
retain some redundant structures, leaving room for parameter reduction.
Motivated by previous spatial and channel attention methods, we propose Triplet
Mamba-UNet. The method leverages residual VSS Blocks to extract intensive
contextual features, while Triplet SSM is employed to fuse features across
spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,
CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,
demonstrating the superior segmentation performance of our proposed TM-UNet.
Additionally, compared to the previous VM-UNet, our model achieves a one-third
reduction in parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PlainMamba: a simple non-hierarchical state space model (SSM)
designed for general visual recognition. The recent Mamba model has shown how
SSMs can be highly competitive with other architectures on sequential data and
initial attempts have been made to apply it to images. In this paper, we
further adapt the selective scanning process of Mamba to the visual domain,
enhancing its ability to learn features from two-dimensional images by (i) a
continuous 2D scanning process that improves spatial continuity by ensuring
adjacency of tokens in the scanning sequence, and (ii) direction-aware updating
which enables the model to discern the spatial relations of tokens by encoding
directional information. Our architecture is designed to be easy to use and
easy to scale, formed by stacking identical PlainMamba blocks, resulting in a
model with constant width throughout all layers. The architecture is further
simplified by removing the need for special tokens. We evaluate PlainMamba on a
variety of visual recognition tasks including image classification, semantic
segmentation, object detection, and instance segmentation. Our method achieves
performance gains over previous non-hierarchical models and is competitive with
hierarchical alternatives. For tasks requiring high-resolution inputs, in
particular, PlainMamba requires much less computing while maintaining high
performance. Code and models are available at
https://github.com/ChenhongyiYang/PlainMamba
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawei Wei, Zejun Yang, Zhisheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose AniPortrait, a novel framework for generating
high-quality animation driven by audio and a reference portrait image. Our
methodology is divided into two stages. Initially, we extract 3D intermediate
representations from audio and project them into a sequence of 2D facial
landmarks. Subsequently, we employ a robust diffusion model, coupled with a
motion module, to convert the landmark sequence into photorealistic and
temporally consistent portrait animation. Experimental results demonstrate the
superiority of AniPortrait in terms of facial naturalness, pose diversity, and
visual quality, thereby offering an enhanced perceptual experience. Moreover,
our methodology exhibits considerable potential in terms of flexibility and
controllability, which can be effectively applied in areas such as facial
motion editing or face reenactment. We release code and model weights at
https://github.com/scutzzj/AniPortrait
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifold-Guided Lyapunov Control with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Mukherjee, Thanin Quartz, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to generating stabilizing controllers
for a large class of dynamical systems using diffusion models. The core
objective is to develop stabilizing control functions by identifying the
closest asymptotically stable vector field relative to a predetermined manifold
and adjusting the control function based on this finding. To achieve this, we
employ a diffusion model trained on pairs consisting of asymptotically stable
vector fields and their corresponding Lyapunov functions. Our numerical results
demonstrate that this pre-trained model can achieve stabilization over
previously unseen systems efficiently and rapidly, showcasing the potential of
our approach in fast zero-shot control and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to
  Inform GenAI Copyright Disputes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Generative Artificial Intelligence (GenAI) models, including
GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content
creation, enabling non-professionals to produce high-quality content across
various domains. This transformative technology has led to a surge of synthetic
content and sparked legal disputes over copyright infringement. To address
these challenges, this paper introduces a novel approach that leverages the
learning capacity of GenAI models for copyright legal analysis, demonstrated
with GPT2 and Stable Diffusion models. Copyright law distinguishes between
original expressions and generic ones (Sc\`enes \`a faire), protecting the
former and permitting reproduction of the latter. However, this distinction has
historically been challenging to make consistently, leading to over-protection
of copyrighted works. GenAI offers an unprecedented opportunity to enhance this
legal analysis by revealing shared patterns in preexisting works. We propose a
data-driven approach to identify the genericity of works created by GenAI,
employing "data-driven bias" to assess the genericity of expressive
compositions. This approach aids in copyright scope determination by utilizing
the capabilities of GenAI to identify and prioritize expressive elements and
rank them according to their frequency in the model's dataset. The potential
implications of measuring expressive genericity for copyright law are profound.
Such scoring could assist courts in determining copyright scope during
litigation, inform the registration practices of Copyright Offices, allowing
registration of only highly original synthetic works, and help copyright owners
signal the value of their works and facilitate fairer licensing deals. More
generally, this approach offers valuable insights to policymakers grappling
with adapting copyright law to the challenges posed by the era of GenAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ACM CSLAW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Light <span class="highlight-title">Transformer</span> Ensembles for Multimodal Trajectory
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien Lafage, Mathieu Barbier, Gianni Franchi, David Filliat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate trajectory forecasting is crucial for the performance of various
systems, such as advanced driver-assistance systems and self-driving vehicles.
These forecasts allow to anticipate events leading to collisions and,
therefore, to mitigate them. Deep Neural Networks have excelled in motion
forecasting, but issues like overconfidence and uncertainty quantification
persist. Deep Ensembles address these concerns, yet applying them to multimodal
distributions remains challenging. In this paper, we propose a novel approach
named Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently
training an ensemble of Transformer architectures using a novel hierarchical
loss function. HLT-Ens leverages grouped fully connected layers, inspired by
grouped convolution techniques, to capture multimodal distributions,
effectively. Through extensive experimentation, we demonstrate that HLT-Ens
achieves state-of-the-art performance levels, offering a promising avenue for
improving trajectory forecasting techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Perceived Gloss: Do Weak Labels Suffice? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Guerrero-Viu, J. Daniel Subias, Ana Serrano, Katherine R. Storrs, Roland W. Fleming, Belen Masia, Diego Gutierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating perceptual attributes of materials directly from images is a
challenging task due to their complex, not fully-understood interactions with
external factors, such as geometry and lighting. Supervised deep learning
models have recently been shown to outperform traditional approaches, but rely
on large datasets of human-annotated images for accurate perception
predictions. Obtaining reliable annotations is a costly endeavor, aggravated by
the limited ability of these models to generalise to different aspects of
appearance. In this work, we show how a much smaller set of human annotations
("strong labels") can be effectively augmented with automatically derived "weak
labels" in the context of learning a low-dimensional image-computable gloss
metric. We evaluate three alternative weak labels for predicting human gloss
perception from limited annotated data. Incorporating weak labels enhances our
gloss prediction beyond the current state of the art. Moreover, it enables a
substantial reduction in human annotation costs without sacrificing accuracy,
whether working with rendered images or real photographs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Graphics Forum (Eurographics 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with
  Space-sensitive Customization and Semantic Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilin Wang, Jiangning Zhang, Chengming Xu, Weijian Cao, Ying Tai, Yue Han, Yanhao Ge, Hong Gu, Chengjie Wang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Appearance Editing (FAE) aims to modify physical attributes, such as
pose, expression and lighting, of human facial images while preserving
attributes like identity and background, showing great importance in
photograph. In spite of the great progress in this area, current researches
generally meet three challenges: low generation fidelity, poor attribute
preservation, and inefficient inference. To overcome above challenges, this
paper presents DiffFAE, a one-stage and highly-efficient diffusion-based
framework tailored for high-fidelity FAE. For high-fidelity query attributes
transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures
the fidelity and generalization ability by utilizing rendering texture derived
from 3D Morphable Model (3DMM). In order to preserve source attributes, we
introduce the Region-responsive Semantic Composition (RSC). This module is
guided to learn decoupled source-regarding features, thereby better preserving
the identity and alleviating artifacts from non-facial attributes such as hair,
clothes, and background. We further introduce a consistency regularization for
our pipeline to enhance editing controllability by leveraging prior knowledge
in the attention matrices of diffusion model. Extensive experiments demonstrate
the superiority of DiffFAE over existing methods, achieving state-of-the-art
performance in facial appearance editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Dynamic <span class="highlight-title">Transformer</span> for Efficient Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Zhu, Xin Chen, Haiwen Diao, Shuai Li, Jun-Yan He, Chenyang Li, Bin Luo, Dong Wang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The speed-precision trade-off is a critical problem for visual object
tracking which usually requires low latency and deployment on constrained
resources. Existing solutions for efficient tracking mainly focus on adopting
light-weight backbones or modules, which nevertheless come at the cost of a
sacrifice in precision. In this paper, inspired by dynamic network routing, we
propose DyTrack, a dynamic transformer framework for efficient tracking.
Real-world tracking scenarios exhibit diverse levels of complexity. We argue
that a simple network is sufficient for easy frames in video sequences, while
more computation could be assigned to difficult ones. DyTrack automatically
learns to configure proper reasoning routes for various inputs, gaining better
utilization of the available computational budget. Thus, it can achieve higher
performance with the same running speed. We formulate instance-specific
tracking as a sequential decision problem and attach terminating branches to
intermediate layers of the entire model. Especially, to fully utilize the
computations, we introduce the feature recycling mechanism to reuse the outputs
of predecessors. Furthermore, a target-aware self-distillation strategy is
designed to enhance the discriminating capabilities of early predictions by
effectively mimicking the representation pattern of the deep model. Extensive
experiments on multiple benchmarks demonstrate that DyTrack achieves promising
speed-precision trade-offs with only a single model. For instance, DyTrack
obtains 64.9% AUC on LaSOT with a speed of 256 fps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Resolution Image Translation Model Based on Grayscale Redefinition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xixian Wu, Dian Chao, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation is a technique that focuses on transferring images
from one domain to another while maintaining the essential content
representations. In recent years, image-to-image translation has gained
significant attention and achieved remarkable advancements due to its diverse
applications in computer vision and image processing tasks. In this work, we
propose an innovative method for image translation between different domains.
For high-resolution image translation tasks, we use a grayscale adjustment
method to achieve pixel-level translation. For other tasks, we utilize the
Pix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and
improved loss to enhance the image translation performance. On the other hand,
to tackle the issue of sparse training data, we adopt model weight
initialization from other task to optimize the performance of the current task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with
  Relative Geometric Consistency <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjie Xu, Bangzhen Liu, Hao Tang, Bailin Deng, Shengfeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a voxel-based optimization framework, ReVoRF, for few-shot
radiance fields that strategically address the unreliability in pseudo novel
view synthesis. Our method pivots on the insight that relative depth
relationships within neighboring regions are more reliable than the absolute
color values in disoccluded areas. Consequently, we devise a bilateral
geometric consistency loss that carefully navigates the trade-off between color
fidelity and geometric accuracy in the context of depth consistency for
uncertain regions. Moreover, we present a reliability-guided learning strategy
to discern and utilize the variable quality across synthesized views,
complemented by a reliability-aware voxel smoothing algorithm that smoothens
the transition between reliable and unreliable data patches. Our approach
allows for a more nuanced use of all available data, promoting enhanced
learning from regions previously considered unsuitable for high-quality
reconstruction. Extensive experiments across diverse datasets reveal that our
approach attains significant gains in efficiency and accuracy, delivering
rendering speeds of 3 FPS, 7 mins to train a $360^\circ$ scene, and a 5\%
improvement in PSNR over existing few-shot methods. Code is available at
https://github.com/HKCLynn/ReVoRF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 final version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object
  Detection with Sparse LiDAR and Large Domain Gaps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we address a gap in existing unsupervised domain adaptation
approaches on LiDAR-based 3D object detection, which have predominantly
concentrated on adapting between established, high-density autonomous driving
datasets. We focus on sparser point clouds, capturing scenarios from different
perspectives: not just from vehicles on the road but also from mobile robots on
sidewalks, which encounter significantly different environmental conditions and
sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation
for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source
models or teacher-student architectures. Instead, it uses an adversarial
approach to directly learn domain-invariant features. We demonstrate its
efficacy in various adaptation scenarios, showing significant improvements in
both self-driving car and mobile robot domains. Our code is open-source and
will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AniArtAvatar: Animatable 3D Art Avatar from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach for generating animatable 3D-aware art avatars
from a single image, with controllable facial expressions, head poses, and
shoulder movements. Unlike previous reenactment methods, our approach utilizes
a view-conditioned 2D diffusion model to synthesize multi-view images from a
single art portrait with a neutral expression. With the generated colors and
normals, we synthesize a static avatar using an SDF-based neural surface. For
avatar animation, we extract control points, transfer the motion with these
points, and deform the implicit canonical space. Firstly, we render the front
image of the avatar, extract the 2D landmarks, and project them to the 3D space
using a trained SDF network. We extract 3D driving landmarks using 3DMM and
transfer the motion to the avatar landmarks. To animate the avatar pose, we
manually set the body height and bound the head and torso of an avatar with two
cages. The head and torso can be animated by transforming the two cages. Our
approach is a one-shot pipeline that can be applied to various styles.
Experiments demonstrate that our method can generate high-quality 3D art
avatars with desired control over different motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles
  from 3D Cell Painting Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Gopalakrishnan, Jingzhe Ma, Zhiyong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their black-box nature, deep learning models are extensively used in
image-based drug discovery to extract feature vectors from single cells in
microscopy images. To better understand how these networks perform
representation learning, we employ visual explainability techniques (e.g.,
Grad-CAM). Our analyses reveal several mechanisms by which supervised models
cheat, exploiting biologically irrelevant pixels when extracting morphological
features from images, such as noise in the background. This raises doubts
regarding the fidelity of learned single-cell representations and their
relevance when investigating downstream biological questions. To address this
misalignment between researcher expectations and machine behavior, we introduce
Grad-CAMO, a novel single-cell interpretability score for supervised feature
extractors. Grad-CAMO measures the proportion of a model's attention that is
concentrated on the cell of interest versus the background. This metric can be
assessed per-cell or averaged across a validation set, offering a tool to audit
individual features vectors or guide the improved design of deep learning
architectures. Importantly, Grad-CAMO seamlessly integrates into existing
workflows, requiring no dataset or model modifications, and is compatible with
both 2D and 3D Cell Painting data. Additional results are available at
https://github.com/eigenvivek/Grad-CAMO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMVP: A Multimodal MoCap <span class="highlight-title">Dataset</span> with Vision and Pressure Sensors <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhang, Shenghao Ren, Haolei Yuan, Jianhui Zhao, Fan Li, Shuangpeng Sun, Zhenghao Liang, Tao Yu, Qiu Shen, Xun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foot contact is an important cue not only for human motion capture but also
for motion understanding and physically plausible motion generation. However,
most of the foot-contact annotations in existing datasets are estimated by
purely visual matching and distance thresholding, which results in low accuracy
and coarse granularity. Even though existing multimodal datasets
synergistically capture plantar pressure (foot contact) and visual signals,
they are specifically designed for small-range and slow motion such as Taiji
Quan and Yoga. Therefore, there is still a lack of a vision-pressure multimodal
dataset with large-range and fast human motion, as well as accurate and dense
foot-contact annotation. To fill this gap, we propose a Multimodal MoCap
Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate
and dense plantar pressure signals synchronized with RGBD observations, which
is especially useful for both plausible shape estimation, robust pose fitting
without foot drifting, and accurate global translation tracking. To validate
the dataset, we propose an RGBD-P SMPL fitting method and also a
monocular-video-based baseline framework, VP-MoCap, for human motion capture.
Experiments demonstrate that our RGBD-P SMPL Fitting results significantly
outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA
methods in foot-contact and global translation estimation accuracy. We believe
the configuration of the dataset and the baseline frameworks will stimulate the
research in this direction and also provide a good reference for MoCap
applications in various domains. Project page:
https://haolyuan.github.io/MMVP-Dataset/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake or JPEG? Revealing Common Biases in Generated Image Detection
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of generative image models has highlighted the urgent
need to detect artificial content, which is a crucial step in combating
widespread manipulation and misinformation. Consequently, numerous detectors
and associated datasets have emerged. However, many of these datasets
inadvertently introduce undesirable biases, thereby impacting the effectiveness
and evaluation of detectors. In this paper, we emphasize that many datasets for
AI-generated image detection contain biases related to JPEG compression and
image size. Using the GenImage dataset, we demonstrate that detectors indeed
learn from these undesired factors. Furthermore, we show that removing the
named biases substantially increases robustness to JPEG compression and
significantly alters the cross-generator performance of evaluated detectors.
Specifically, it leads to more than 11 percentage points increase in
cross-generator performance for ResNet50 and Swin-T detectors on the GenImage
dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous
website: https://www.unbiased-genimage.org
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Memory Networks: A Versatile Adaptation Approach for
  Vision-Language Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024; Codes are available at \url{https://github.com/YBZh/DMN}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been achieved in sensing real large-scale
outdoor 3D environments, particularly by using modern acquisition equipment
such as LiDAR sensors. Unfortunately, they are fundamentally limited in their
ability to produce dense, complete 3D scenes. To address this issue, recent
learning-based methods integrate neural implicit representations and
optimizable feature grids to approximate surfaces of 3D scenes. However,
naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results
due to the nature of sparse, conflicting LiDAR measurements. Instead, in this
work we depart from fitting LiDAR data exactly, instead letting the network
optimize a non-metric monotonic implicit field defined in 3D space. To fit our
field, we design a learning system integrating a monotonicity loss that enables
optimizing neural monotonic fields and leverages recent progress in large-scale
3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as
captured by multiple quantitative and perceptual measures and visual results
obtained for Mai City, Newer College, and KITTI benchmarks. The code of our
approach will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Applications of Advanced Cloud Services and Generative AI
  Systems in Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Xu, Binbin Wu, Jiaxin Huang, Yulu Gong, Yifan Zhang, Bo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The medical field is one of the important fields in the application of
artificial intelligence technology. With the explosive growth and
diversification of medical data, as well as the continuous improvement of
medical needs and challenges, artificial intelligence technology is playing an
increasingly important role in the medical field. Artificial intelligence
technologies represented by computer vision, natural language processing, and
machine learning have been widely penetrated into diverse scenarios such as
medical imaging, health management, medical information, and drug research and
development, and have become an important driving force for improving the level
and quality of medical services.The article explores the transformative
potential of generative AI in medical imaging, emphasizing its ability to
generate syntheticACM-2 data, enhance images, aid in anomaly detection, and
facilitate image-to-image translation. Despite challenges like model
complexity, the applications of generative models in healthcare, including
Med-PaLM 2 technology, show promising results. By addressing limitations in
dataset size and diversity, these models contribute to more accurate diagnoses
and improved patient outcomes. However, ethical considerations and
collaboration among stakeholders are essential for responsible implementation.
Through experiments leveraging GANs to augment brain tumor MRI datasets, the
study demonstrates how generative AI can enhance image quality and diversity,
ultimately advancing medical diagnostics and patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gaze-grounded Visual Question Answering <span class="highlight-title">Dataset</span> for Clarifying
  Ambiguous Japanese Questions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Inadumi, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi, Koichiro Yoshino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Situated conversations, which refer to visual information as visual question
answering (VQA), often contain ambiguities caused by reliance on directive
information. This problem is exacerbated because some languages, such as
Japanese, often omit subjective or objective terms. Such ambiguities in
questions are often clarified by the contexts in conversational situations,
such as joint attention with a user or user gaze information. In this study, we
propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous
questions using gaze information by focusing on a clarification process
complemented by gaze information. We also propose a method that utilizes gaze
target estimation results to improve the accuracy of GazeVQA tasks. Our
experimental results showed that the proposed method improved the performance
in some cases of a VQA system on GazeVQA and identified some typical problems
of GazeVQA tasks that need to be improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordRobe: Text-Guided Generation of Textured 3D Garments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Astitva Srivastava, Pranav Manu, Amit Raj, Varun Jampani, Avinash Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle a new and challenging problem of text-driven
generation of 3D garments with high-quality textures. We propose "WordRobe", a
novel framework for the generation of unposed & textured 3D garment meshes from
user-friendly text prompts. We achieve this by first learning a latent
representation of 3D garments using a novel coarse-to-fine training strategy
and a loss for latent disentanglement, promoting better latent interpolation.
Subsequently, we align the garment latent space to the CLIP embedding space in
a weakly supervised manner, enabling text-driven 3D garment generation and
editing. For appearance modeling, we leverage the zero-shot generation
capability of ControlNet to synthesize view-consistent texture maps in a single
feed-forward inference step, thereby drastically decreasing the generation time
as compared to existing methods. We demonstrate superior performance over
current SOTAs for learning 3D garment latent space, garment interpolation, and
text-driven texture synthesis, supported by quantitative evaluation and
qualitative user study. The unposed 3D garment meshes generated using WordRobe
can be directly fed to standard cloth simulation & animation pipelines without
any post-processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using
  Heuristics-Guided Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Field (NeRF) has been widely recognized for its excellence in
novel view synthesis and 3D scene reconstruction. However, their effectiveness
is inherently tied to the assumption of static scenes, rendering them
susceptible to undesirable artifacts when confronted with transient distractors
such as moving objects or shadows. In this work, we propose a novel paradigm,
namely "Heuristics-Guided Segmentation" (HuGS), which significantly enhances
the separation of static scenes from transient distractors by harmoniously
combining the strengths of hand-crafted heuristics and state-of-the-art
segmentation models, thus significantly transcending the limitations of
previous solutions. Furthermore, we delve into the meticulous design of
heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based
heuristics and color residual heuristics, catering to a diverse range of
texture profiles. Extensive experiments demonstrate the superiority and
robustness of our method in mitigating transient distractors for NeRFs trained
in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Few-Shot Learning with Disentangled <span class="highlight-title">Self-Supervised</span> Learning
  and Meta-Learning for Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Pachetti, Sotirios A. Tsaftaris, Sara Colantonio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background and objective: Employing deep learning models in critical domains
such as medical imaging poses challenges associated with the limited
availability of training data. We present a strategy for improving the
performance and generalization capabilities of models trained in low-data
regimes. Methods: The proposed method starts with a pre-training phase, where
features learned in a self-supervised learning setting are disentangled to
improve the robustness of the representations for downstream tasks. We then
introduce a meta-fine-tuning step, leveraging related classes between
meta-training and meta-testing phases but varying the granularity level. This
approach aims to enhance the model's generalization capabilities by exposing it
to more challenging classification tasks during meta-training and evaluating it
on easier tasks but holding greater clinical relevance during meta-testing. We
demonstrate the effectiveness of the proposed approach through a series of
experiments exploring several backbones, as well as diverse pre-training and
fine-tuning schemes, on two distinct medical tasks, i.e., classification of
prostate cancer aggressiveness from MRI data and classification of breast
cancer malignity from microscopic images. Results: Our results indicate that
the proposed approach consistently yields superior performance w.r.t. ablation
experiments, maintaining competitiveness even when a distribution shift between
training and evaluation data occurs. Conclusion: Extensive experiments
demonstrate the effectiveness and wide applicability of the proposed approach.
We hope that this work will add another solution to the arsenal of addressing
learning issues in data-scarce imaging domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures, 4 tables. Submitted to Elsevier on 25 March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equipping Sketch Patches with Context-Aware Positional Encoding for
  Graphic Sketch Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Zang, Zhijun Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The drawing order of a sketch records how it is created stroke-by-stroke by a
human being. For graphic sketch representation learning, recent studies have
injected sketch drawing orders into graph edge construction by linking each
patch to another in accordance to a temporal-based nearest neighboring
strategy. However, such constructed graph edges may be unreliable, since a
sketch could have variants of drawings. In this paper, we propose a
variant-drawing-protected method by equipping sketch patches with context-aware
positional encoding (PE) to make better use of drawing orders for learning
graphic sketch representation. Instead of injecting sketch drawings into graph
edges, we embed these sequential information into graph nodes only. More
specifically, each patch embedding is equipped with a sinusoidal absolute PE to
highlight the sequential position in the drawing order. And its neighboring
patches, ranked by the values of self-attention scores between patch
embeddings, are equipped with learnable relative PEs to restore the contextual
positions within a neighborhood. During message aggregation via graph
convolutional networks, a node receives both semantic contents from patch
embeddings and contextual patterns from PEs by its neighbors, arriving at
drawing-order-enhanced sketch representations. Experimental results indicate
that our method significantly improves sketch healing and controllable sketch
synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Adversarial Training via Fisher-Rao Norm-based Regularization <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Yin, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is extensively utilized to improve the adversarial
robustness of deep neural networks. Yet, mitigating the degradation of standard
generalization performance in adversarial-trained models remains an open
problem. This paper attempts to resolve this issue through the lens of model
complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant
metric for model complexity, to establish the non-trivial bounds of the
Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer
Perceptron. Then we generalize a complexity-related variable, which is
sensitive to the changes in model width and the trade-off factors in
adversarial training. Moreover, intensive empirical evidence validates that
this variable highly correlates with the generalization gap of Cross-Entropy
loss between adversarial-trained and standard-trained models, especially during
the initial and final phases of the training process. Building upon this
observation, we propose a novel regularization framework, called Logit-Oriented
Adversarial Training (LOAT), which can mitigate the trade-off between
robustness and accuracy while imposing only a negligible increase in
computational overhead. Our extensive experiments demonstrate that the proposed
regularization strategy can boost the performance of the prevalent adversarial
training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,
across various network architectures. Our code will be available at
https://github.com/TrustAI/LOAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random-coupled Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liu, Mingzhe Liu, Peng Li, Jiahui Wu, Xin Jiang, Zhuo Zuo, Bingqi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the efficiency of current neural networks and modeling them in
biological neural systems have become popular research directions in recent
years. Pulse-coupled neural network (PCNN) is a well applicated model for
imitating the computation characteristics of the human brain in computer vision
and neural network fields. However, differences between the PCNN and biological
neural systems remain: limited neural connection, high computational cost, and
lack of stochastic property. In this study, random-coupled neural network
(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic
computing via a random inactivation process. This process randomly closes some
neural connections in the RCNN model, realized by the random inactivation
weight matrix of link input. This releases the computational burden of PCNN,
making it affordable to achieve vast neural connections. Furthermore, the image
and video processing mechanisms of RCNN are researched. It encodes constant
stimuli as periodic spike trains and periodic stimuli as chaotic spike trains,
the same as biological neural information encoding characteristics. Finally,
the RCNN is applicated to image segmentation, fusion, and pulse shape
discrimination subtasks. It is demonstrated to be robust, efficient, and highly
anti-noised, with outstanding performance in all applications mentioned above.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free
  Class-Incremental Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Zhiping Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) under an exemplar-free constraint has
presented a significant challenge. Existing methods adhering to this constraint
are prone to catastrophic forgetting, far more so than replay-based techniques
that retain access to past samples. In this paper, to solve the exemplar-free
CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The
DS-AL contains a main stream offering an analytical (i.e., closed-form) linear
solution, and a compensation stream improving the inherent under-fitting
limitation due to adopting linear mapping. The main stream redefines the CIL
problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an
equivalence between the CIL and its joint-learning counterpart. The
compensation stream is governed by a Dual-Activation Compensation (DAC) module.
This module re-activates the embedding with a different activation function
from the main stream one, and seeks fitting compensation by projecting the
embedding to the null space of the main stream's linear mapping. Empirical
results demonstrate that the DS-AL, despite being an exemplar-free technique,
delivers performance comparable with or better than that of replay-based
methods across various datasets, including CIFAR-100, ImageNet-100 and
ImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to
execute CIL in a phase-invariant manner. This is evidenced by a
never-before-seen 500-phase CIL ImageNet task, which performs on a level
identical to a 5-phase one. Our codes are available at
https://github.com/ZHUANGHP/Analytic-continual-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dihan Zheng, Yihang Zou, Xiaowen Zhang, Chenglong Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The data bottleneck has emerged as a fundamental challenge in learning based
image restoration methods. Researchers have attempted to generate synthesized
training data using paired or unpaired samples to address this challenge. This
study proposes SeNM-VAE, a semi-supervised noise modeling method that leverages
both paired and unpaired datasets to generate realistic degraded data. Our
approach is based on modeling the conditional distribution of degraded and
clean images with a specially designed graphical model. Under the variational
inference framework, we develop an objective function for handling both paired
and unpaired data. We employ our method to generate paired training samples for
real-world image denoising and super-resolution tasks. Our approach excels in
the quality of synthetic degraded images compared to other unpaired and paired
noise modeling methods. Furthermore, our approach demonstrates remarkable
performance in downstream image restoration tasks, even with limited paired
data. With more paired data, our method achieves the best performance on the
SIDD dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharing the Cost of Success: A Game for Evaluating and Learning
  Collaborative Multi-Agent Instruction Giving and Following Policies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Sadler, Sherzod Hakimov, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In collaborative goal-oriented settings, the participants are not only
interested in achieving a successful outcome, but do also implicitly negotiate
the effort they put into the interaction (by adapting to each other). In this
work, we propose a challenging interactive reference game that requires two
players to coordinate on vision and language observations. The learning signal
in this game is a score (given after playing) that takes into account the
achieved goal and the players' assumed efforts during the interaction. We show
that a standard Proximal Policy Optimization (PPO) setup achieves a high
success rate when bootstrapped with heuristic partner behaviors that implement
insights from the analysis of human-human interactions. And we find that a
pairing of neural partners indeed reduces the measured joint effort when
playing together repeatedly. However, we observe that in comparison to a
reasonable heuristic pairing there is still room for improvement -- which
invites further research in the direction of cost-sharing in collaborative
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dr.Hair: Reconstructing Scalp-Connected Hair Strands without
  <span class="highlight-title">Pre-train</span>ing via Differentiable Rendering of Line Segments <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Takimoto, Hikari Takehara, Hiroyuki Sato, Zihao Zhu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the film and gaming industries, achieving a realistic hair appearance
typically involves the use of strands originating from the scalp. However,
reconstructing these strands from observed surface images of hair presents
significant challenges. The difficulty in acquiring Ground Truth (GT) data has
led state-of-the-art learning-based methods to rely on pre-training with
manually prepared synthetic CG data. This process is not only labor-intensive
and costly but also introduces complications due to the domain gap when
compared to real-world data. In this study, we propose an optimization-based
approach that eliminates the need for pre-training. Our method represents hair
strands as line segments growing from the scalp and optimizes them using a
novel differentiable rendering algorithm. To robustly optimize a substantial
number of slender explicit geometries, we introduce 3D orientation estimation
utilizing global optimization, strand initialization based on Laplace's
equation, and reparameterization that leverages geometric connectivity and
spatial proximity. Unlike existing optimization-based methods, our method is
capable of reconstructing internal hair flow in an absolute direction. Our
method exhibits robust and accurate inverse rendering, surpassing the quality
of existing methods and significantly improving processing speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on
  360° Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai Bâce, Zhiming Hu, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DiffGaze, a novel method for generating realistic and diverse
continuous human gaze sequences on 360{\deg} images based on a conditional
score-based denoising diffusion model. Generating human gaze on 360{\deg}
images is important for various human-computer interaction and computer
graphics applications, e.g. for creating large-scale eye tracking datasets or
for realistic animation of virtual humans. However, existing methods are
limited to predicting discrete fixation sequences or aggregated saliency maps,
thereby neglecting crucial parts of natural gaze behaviour. Our method uses
features extracted from 360{\deg} images as condition and uses two transformers
to model the temporal and spatial dependencies of continuous human gaze. We
evaluate DiffGaze on two 360{\deg} image benchmarks for gaze sequence
generation as well as scanpath prediction and saliency prediction. Our
evaluations show that DiffGaze outperforms state-of-the-art methods on all
tasks on both benchmarks. We also report a 21-participant user study showing
that our method generates gaze sequences that are indistinguishable from real
human sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated
  Image Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Diffusion Models has dramatically improved image generation
quality, making it increasingly difficult to differentiate between real and
generated images. This development, while impressive, also raises significant
privacy and security concerns. In response to this, we propose a novel Latent
REconstruction error guided feature REfinement method (LaRE^2) for detecting
the diffusion-generated images. We come up with the Latent Reconstruction Error
(LaRE), the first reconstruction-error based feature in the latent space for
generated image detection. LaRE surpasses existing methods in terms of feature
extraction efficiency while preserving crucial cues required to differentiate
between the real and the fake. To exploit LaRE, we propose an Error-Guided
feature REfinement module (EGRE), which can refine the image feature guided by
LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an
align-then-refine mechanism, which effectively refines the image feature for
generated-image detection from both spatial and channel perspectives. Extensive
experiments on the large-scale GenImage benchmark demonstrate the superiority
of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%
average ACC/AP across 8 different image generators. LaRE also surpasses
existing methods in terms of feature extraction cost, delivering an impressive
speed enhancement of 8 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Bridges across Spatial and Temporal Resolutions:
  Reference-Based Super-Resolution via Change Priors and Conditional Diffusion
  Model <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runmin Dong, Shuai Yuan, Bin Luo, Mengxuan Chen, Jinxiao Zhang, Lixian Zhang, Weijia Li, Juepeng Zheng, Haohuan Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference-based super-resolution (RefSR) has the potential to build bridges
across spatial and temporal resolutions of remote sensing images. However,
existing RefSR methods are limited by the faithfulness of content
reconstruction and the effectiveness of texture transfer in large scaling
factors. Conditional diffusion models have opened up new opportunities for
generating realistic high-resolution images, but effectively utilizing
reference images within these models remains an area for further exploration.
Furthermore, content fidelity is difficult to guarantee in areas without
relevant reference information. To solve these issues, we propose a
change-aware diffusion model named Ref-Diff for RefSR, using the land cover
change priors to guide the denoising process explicitly. Specifically, we
inject the priors into the denoising model to improve the utilization of
reference information in unchanged areas and regulate the reconstruction of
semantically relevant content in changed areas. With this powerful guidance, we
decouple the semantics-guided denoising and reference texture-guided denoising
processes to improve the model performance. Extensive experiments demonstrate
the superior effectiveness and robustness of the proposed method compared with
state-of-the-art RefSR methods in both quantitative and qualitative
evaluations. The code and data are available at
https://github.com/dongrunmin/RefDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain of Compression: A Systematic Approach to Combinationally Compress
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Shen, Minqing Sun, Jie Zhao, An Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have achieved significant popularity,
but their computational and memory intensity poses challenges for
resource-constrained computing systems, particularly with the prerequisite of
real-time performance. To release this burden, model compression has become an
important research focus. Many approaches like quantization, pruning, early
exit, and knowledge distillation have demonstrated the effect of reducing
redundancy in neural networks. Upon closer examination, it becomes apparent
that each approach capitalizes on its unique features to compress the neural
network, and they can also exhibit complementary behavior when combined. To
explore the interactions and reap the benefits from the complementary features,
we propose the Chain of Compression, which works on the combinational sequence
to apply these common techniques to compress the neural network. Validated on
the image-based regression and classification networks across different data
sets, our proposed Chain of Compression can significantly compress the
computation cost by 100-1000 times with ignorable accuracy loss compared with
the baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Mamba Sequence Model and Hierarchical Upsampling Network for
  Accurate Semantic Segmentation of Multiple Sclerosis Legion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazi Shahriar Sanjid, Md. Tanzim Hossain, Md. Shakib Shahariar Junayed, Dr. Mohammad Monir Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating components from convolutional neural networks and state space
models in medical image segmentation presents a compelling approach to enhance
accuracy and efficiency. We introduce Mamba HUNet, a novel architecture
tailored for robust and efficient segmentation tasks. Leveraging strengths from
Mamba UNet and the lighter version of Hierarchical Upsampling Network (HUNet),
Mamba HUNet combines convolutional neural networks local feature extraction
power with state space models long range dependency modeling capabilities. We
first converted HUNet into a lighter version, maintaining performance parity
and then integrated this lighter HUNet into Mamba HUNet, further enhancing its
efficiency. The architecture partitions input grayscale images into patches,
transforming them into 1D sequences for processing efficiency akin to Vision
Transformers and Mamba models. Through Visual State Space blocks and patch
merging layers, hierarchical features are extracted while preserving spatial
information. Experimental results on publicly available Magnetic Resonance
Imaging scans, notably in Multiple Sclerosis lesion segmentation, demonstrate
Mamba HUNet's effectiveness across diverse segmentation tasks. The model's
robustness and flexibility underscore its potential in handling complex
anatomical structures. These findings establish Mamba HUNet as a promising
solution in advancing medical image segmentation, with implications for
improving clinical decision making processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time Adaptation Meets Image Enhancement: Improving Accuracy via
  Uncertainty-aware Logit Switching <span class="chip">IJCNN2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shohei Enomoto, Naoya Hasegawa, Kazuki Adachi, Taku Sasaki, Shin'ya Yamaguchi, Satoshi Suzuki, Takeharu Eda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved remarkable success in a variety of
computer vision applications. However, there is a problem of degrading accuracy
when the data distribution shifts between training and testing. As a solution
of this problem, Test-time Adaptation~(TTA) has been well studied because of
its practicality. Although TTA methods increase accuracy under distribution
shift by updating the model at test time, using high-uncertainty predictions is
known to degrade accuracy. Since the input image is the root of the
distribution shift, we incorporate a new perspective on enhancing the input
image into TTA methods to reduce the prediction's uncertainty. We hypothesize
that enhancing the input image reduces prediction's uncertainty and increase
the accuracy of TTA methods. On the basis of our hypothesis, we propose a novel
method: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the
classification model is combined with the image enhancement model that
transforms input images into recognition-friendly ones, and these models are
updated by existing TTA methods. Furthermore, we found that the prediction from
the enhanced image does not always have lower uncertainty than the prediction
from the original image. Thus, we propose logit switching, which compares the
uncertainty measure of these predictions and outputs the lower one. In our
experiments, we evaluate TECA with various TTA methods and show that TECA
reduces prediction's uncertainty and increases accuracy of TTA methods despite
having no hyperparameters and little parameter overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse
  Diffusion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present InterHandGen, a novel framework that learns the generative prior
of two-hand interaction. Sampling from our model yields plausible and diverse
two-hand shapes in close interaction with or without an object. Our prior can
be incorporated into any optimization or learning methods to reduce ambiguity
in an ill-posed setup. Our key observation is that directly modeling the joint
distribution of multiple instances imposes high learning complexity due to its
combinatorial nature. Thus, we propose to decompose the modeling of joint
distribution into the modeling of factored unconditional and conditional single
instance distribution. In particular, we introduce a diffusion model that
learns the single-hand distribution unconditional and conditional to another
hand via conditioning dropout. For sampling, we combine anti-penetration and
classifier-free guidance to enable plausible generation. Furthermore, we
establish the rigorous evaluation protocol of two-hand synthesis, where our
method significantly outperforms baseline generative models in terms of
plausibility and diversity. We also demonstrate that our diffusion prior can
boost the performance of two-hand reconstruction from monocular in-the-wild
images, achieving new state-of-the-art accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024, project page:
  https://jyunlee.github.io/projects/interhandgen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Visually Localize Sound Sources from Mixtures without Prior
  Source Knowledge <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjin Kim, Sung Jin Um, Sangmin Lee, Jung Uk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of the multi-sound source localization task is to localize sound
sources from the mixture individually. While recent multi-sound source
localization methods have shown improved performance, they face challenges due
to their reliance on prior information about the number of objects to be
separated. In this paper, to overcome this limitation, we present a novel
multi-sound source localization method that can perform localization without
prior knowledge of the number of sound sources. To achieve this goal, we
propose an iterative object identification (IOI) module, which can recognize
sound-making objects in an iterative manner. After finding the regions of
sound-making objects, we devise object similarity-aware clustering (OSC) loss
to guide the IOI module to effectively combine regions of the same object but
also distinguish between different objects and backgrounds. It enables our
method to perform accurate localization of sound-making objects without any
prior knowledge. Extensive experimental results on the MUSIC and VGGSound
benchmarks show the significant performance improvements of the proposed method
over the existing methods for both single and multi-source. Our code is
available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Clustering based Visual Representation Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guikun Chen, Xia Li, Yi Yang, Wenguan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a fundamental aspect of machine vision: the measurement of
features, by revisiting clustering, one of the most classic approaches in
machine learning and data analysis. Existing visual feature extractors,
including ConvNets, ViTs, and MLPs, represent an image as rectangular regions.
Though prevalent, such a grid-style paradigm is built upon engineering practice
and lacks explicit modeling of data distribution. In this work, we propose
feature extraction with clustering (FEC), a conceptually elegant yet
surprisingly ad-hoc interpretable neural clustering framework, which views
feature extraction as a process of selecting representatives from data and thus
automatically captures the underlying data distribution. Given an image, FEC
alternates between grouping pixels into individual clusters to abstract
representatives and updating the deep features of pixels with current
representatives. Such an iterative working mechanism is implemented in the form
of several neural layers and the final representatives can be used for
downstream tasks. The cluster assignments across layers, which can be viewed
and inspected by humans, make the forward process of FEC fully transparent and
empower it with promising ad-hoc interpretability. Extensive experiments on
various visual recognition models and tasks verify the effectiveness,
generality, and interpretability of FEC. We expect this work will provoke a
rethink of the current de facto grid-style paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code: https://github.com/guikunchen/FEC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songbur Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SSF3D modified the semi-supervised 3D object detection (SS3DOD) framework,
which designed specifically for point cloud data. Leveraging the
characteristics of non-coincidence and weak correlation of target objects in
point cloud, we adopt a strategy of retaining only the truth-determining pseudo
labels and trimming the other fuzzy labels with points, instead of pursuing a
balance between the quantity and quality of pseudo labels. Besides, we notice
that changing the filter will make the model meet different distributed
targets, which is beneficial to break the training bottleneck. Two mechanism
are introduced to achieve above ideas: strict threshold and filter switching.
The experiments are conducted to analyze the effectiveness of above approaches
and their impact on the overall performance of the system. Evaluating on the
KITTI dataset, SSF3D exhibits superior performance compared to the current
state-of-the-art methods. The code will be released here.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object
  Detection <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We delve into pseudo-labeling for semi-supervised monocular 3D object
detection (SSM3OD) and discover two primary issues: a misalignment between the
prediction quality of 3D and 2D attributes and the tendency of depth
supervision derived from pseudo-labels to be noisy, leading to significant
optimization conflicts with other reliable forms of supervision. We introduce a
novel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach
features a Decoupled Pseudo-label Generation (DPG) module, designed to
efficiently generate pseudo-labels by separately processing 2D and 3D
attributes. This module incorporates a unique homography-based method for
identifying dependable pseudo-labels in BEV space, specifically for 3D
attributes. Additionally, we present a DepthGradient Projection (DGP) module to
mitigate optimization conflicts caused by noisy depth supervision of
pseudo-labels, effectively decoupling the depth gradient and removing
conflicting gradients. This dual decoupling strategy-at both the pseudo-label
generation and gradient levels-significantly improves the utilization of
pseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark
demonstrate the superiority of our method over existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated that diffusion models are capable of
generating high-quality samples, but their quality heavily depends on sampling
guidance techniques, such as classifier guidance (CG) and classifier-free
guidance (CFG). These techniques are often not applicable in unconditional
generation or in various downstream tasks such as image restoration. In this
paper, we propose a novel sampling guidance, called Perturbed-Attention
Guidance (PAG), which improves diffusion sample quality across both
unconditional and conditional settings, achieving this without requiring
additional training or the integration of external modules. PAG is designed to
progressively enhance the structure of samples throughout the denoising
process. It involves generating intermediate samples with degraded structure by
substituting selected self-attention maps in diffusion U-Net with an identity
matrix, by considering the self-attention mechanisms' ability to capture
structural information, and guiding the denoising process away from these
degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves
sample quality in conditional and even unconditional scenarios. Moreover, PAG
significantly improves the baseline performance in various downstream tasks
where existing guidances such as CG or CFG cannot be fully utilized, including
ControlNet with empty prompts and image restoration such as inpainting and
deblurring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at
  https://ku-cvlab.github.io/Perturbed-Attention-Guidance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiVa-360: The Dynamic Visual <span class="highlight-title">Dataset</span> for Immersive Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-You Lu, Peisen Zhou, Angela Xing, Chandradeep Pokhariya, Arnab Dey, Ishaan Shah, Rugved Mavidipalli, Dylan Hu, Andrew Comport, Kefan Chen, Srinath Sridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in neural fields are enabling high-fidelity capture of the shape and
appearance of dynamic 3D scenes. However, their capabilities lag behind those
offered by conventional representations such as 2D videos because of
algorithmic challenges and the lack of large-scale multi-view real-world
datasets. We address the dataset limitation with DiVa-360, a real-world 360
dynamic visual dataset that contains synchronized high-resolution and
long-duration multi-view video sequences of table-scale scenes captured using a
customized low-cost system with 53 cameras. It contains 21 object-centric
sequences categorized by different motion types, 25 intricate hand-object
interaction sequences, and 8 long-duration sequences for a total of 17.4 M
image frames. In addition, we provide foreground-background segmentation masks,
synchronized audio, and text descriptions. We benchmark the state-of-the-art
dynamic neural field methods on DiVa-360 and provide insights about existing
methods and future challenges on long-duration neural field capture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HoloVIC: Large-scale <span class="highlight-title">Dataset</span> and Benchmark for Multi-Sensor Holographic
  Intersection and Vehicle-Infrastructure Cooperative <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Ma, Lei Qiao, Chengkai Zhu, Kai Liu, Zelong Kong, Qing Li, Xueqi Zhou, Yuheng Kan, Wei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous
Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one
of the important research area. Due to the complexity of traffic conditions
such as blind spots and occlusion, it greatly limits the perception
capabilities of single-view roadside sensing systems. To further enhance the
accuracy of roadside perception and provide better information to the vehicle
side, in this paper, we constructed holographic intersections with various
layouts to build a large-scale multi-sensor holographic vehicle-infrastructure
cooperation dataset, called HoloVIC. Our dataset includes 3 different types of
sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the
different intersections. Each intersection is equipped with 6-18 sensors to
capture synchronous data. While autonomous vehicles pass through these
intersections for collecting VIC data. HoloVIC contains in total on 100k+
synchronous frames from different sensors. Additionally, we annotated 3D
bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs
of the same objects across different devices and consecutive frames in
sequence. Based on HoloVIC, we formulated four tasks to facilitate the
development of related research. We also provide benchmarks for these tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CVPR 2024, Benchmark Website: https://holovic.net</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-based radiance field rendering has demonstrated impressive results for
novel view synthesis, offering a compelling blend of rendering quality and
computational efficiency. However, also latest approaches in this domain are
not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.
2023] struggles when tasked with rendering highly detailed scenes, due to
blurring and cloudy artifacts. On the other hand, ADOP [R\"uckert et al. 2022]
can accommodate crisper images, but the neural reconstruction network decreases
performance, it grapples with temporal instability and it is unable to
effectively address large gaps in the point cloud.
  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that
combines ideas from both Gaussian Splatting and ADOP. The fundamental concept
behind our novel technique involves rasterizing points into a screen-space
image pyramid, with the selection of the pyramid layer determined by the
projected point size. This approach allows rendering arbitrarily large points
using a single trilinear write. A lightweight neural network is then used to
reconstruct a hole-free image including detail beyond splat resolution.
Importantly, our render pipeline is entirely differentiable, allowing for
automatic optimization of both point sizes and positions.
  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art
methods in terms of rendering quality while maintaining a real-time frame rate
of 60 frames per second on readily available hardware. This performance extends
to challenging scenarios, such as scenes featuring intricate geometry,
expansive landscapes, and auto-exposed footage.
  The project page is located at: https://lfranke.github.io/trips/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Crowd Counting from Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Duan, Fan Wan, Rui Sun, Zeyu Wang, Varun Ojha, Yu Guan, Hubert P. H. Shum, Bingzhang Hu, Yang Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Crowd behavior analysis can be applied to effectively help the
daily transportation statistics and planning, which helps the smart city
construction. As one of the most important keys, crowd counting has drawn
increasing attention. Recent works achieved promising performance but relied on
the supervised paradigm with expensive crowd annotations. To alleviate the
annotation cost in real-world transportation scenarios, in this work we
proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can
leverage both unlabeled/labeled data for robust crowd counting. In the
unsupervised pathway, two \textit{self-supervised losses} were proposed to
simulate the crowd variations such as scale, illumination, based on which
supervised information pseudo labels were generated and gradually refined. We
also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit
(GCRU)}, which can preserve discriminant crowd information by extracting
second-order statistics, yielding pseudo labels with improved quality. A joint
loss including both unsupervised/supervised information was proposed, and a
dynamic weighting strategy was employed to balance the importance of the
unsupervised loss and supervised loss at different training stages. We
conducted extensive experiments on four popular crowd counting datasets in
semi-supervised settings. Experimental results supported the effectiveness of
each proposed component in our $S^{4}$Crowd framework. Our method achieved
competitive performance in semi-supervised learning approaches on these crowd
counting datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimLVSeg: Simplifying Left Ventricular Segmentation in 2D+Time
  Echocardiograms with Self- and Weakly-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadillah Maani, Asim Ukaye, Nada Saadi, Numan Saeed, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Echocardiography has become an indispensable clinical imaging modality for
general heart health assessment. From calculating biomarkers such as ejection
fraction to the probability of a patient's heart failure, accurate segmentation
of the heart structures allows doctors to assess the heart's condition and
devise treatments with greater precision and accuracy. However, achieving
accurate and reliable left ventricle segmentation is time-consuming and
challenging due to different reasons. Hence, clinicians often rely on
segmenting the left ventricular (LV) in two specific echocardiogram frames to
make a diagnosis. This limited coverage in manual LV segmentation poses a
challenge for developing automatic LV segmentation with high temporal
consistency, as the resulting dataset is typically annotated sparsely. In
response to this challenge, this work introduces SimLVSeg, a novel paradigm
that enables video-based networks for consistent LV segmentation from sparsely
annotated echocardiogram videos. SimLVSeg consists of self-supervised
pre-training with temporal masking, followed by weakly supervised learning
tailored for LV segmentation from sparse annotations. We demonstrate how
SimLVSeg outperforms the state-of-the-art solutions by achieving a 93.32%
(95%CI 93.21-93.43%) dice score on the largest 2D+time echocardiography dataset
(EchoNet-Dynamic) while being more efficient. SimLVSeg is compatible with two
types of video segmentation networks: 2D super image and 3D segmentation. To
show the effectiveness of our approach, we provide extensive ablation studies,
including pre-training settings and various deep learning backbones. We further
conduct an out-of-distribution test to showcase SimLVSeg's generalizability on
unseen distribution (CAMUS dataset). The code is publicly available at
https://github.com/fadamsyah/SimLVSeg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map
  Construction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhou, Hui Zhang, Jiaqian Yu, Yifan Yang, Sangil Jung, Seung-In Park, ByungIn Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vectorized High-Definition (HD) map construction requires predictions of the
category and point coordinates of map elements (e.g. road boundary, lane
divider, pedestrian crossing, etc.). State-of-the-art methods are mainly based
on point-level representation learning for regressing accurate point
coordinates. However, this pipeline has limitations in obtaining element-level
information and handling element-level failures, e.g. erroneous element shape
or entanglement between elements. To tackle the above issues, we propose a
simple yet effective HybrId framework named HIMap to sufficiently learn and
interact both point-level and element-level information. Concretely, we
introduce a hybrid representation called HIQuery to represent all map elements,
and propose a point-element interactor to interactively extract and encode the
hybrid information of elements, e.g. point position and element shape, into the
HIQuery. Additionally, we present a point-element consistency constraint to
enhance the consistency between the point-level and element-level information.
Finally, the output point-element integrated HIQuery can be directly converted
into map elements' class, point coordinates, and mask. We conduct extensive
experiments and consistently outperform previous methods on both nuScenes and
Argoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes
dataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pushing Auto-regressive Models for 3D Shape Generation at Capacity and
  Scalability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelin Qian, Yu Wang, Simian Luo, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang, Xiangyang Xue, Bo Zhao, Tiejun Huang, Yunsheng Wu, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-regressive models have achieved impressive results in 2D image
generation by modeling joint distributions in grid space. In this paper, we
extend auto-regressive models to 3D domains, and seek a stronger ability of 3D
shape generation by improving auto-regressive models at capacity and
scalability simultaneously. Firstly, we leverage an ensemble of publicly
available 3D datasets to facilitate the training of large-scale models. It
consists of a comprehensive collection of approximately 900,000 objects, with
multiple properties of meshes, points, voxels, rendered images, and text
captions. This diverse labeled dataset, termed Objaverse-Mix, empowers our
model to learn from a wide range of object variations. However, directly
applying 3D auto-regression encounters critical challenges of high
computational demands on volumetric grids and ambiguous auto-regressive order
along grid dimensions, resulting in inferior quality of 3D shapes. To this end,
we then present a novel framework Argus3D in terms of capacity. Concretely, our
approach introduces discrete representation learning based on a latent vector
instead of volumetric grids, which not only reduces computational costs but
also preserves essential geometric details by learning the joint distributions
in a more tractable order. The capacity of conditional generation can thus be
realized by simply concatenating various conditioning inputs to the latent
vector, such as point clouds, categories, images, and texts. In addition,
thanks to the simplicity of our model architecture, we naturally scale up our
approach to a larger model with an impressive 3.6 billion parameters, further
enhancing the quality of versatile 3D generation. Extensive experiments on four
generation tasks demonstrate that Argus3D can synthesize diverse and faithful
shapes across multiple categories, achieving remarkable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://argus-3d.github.io/ . Datasets:
  https://huggingface.co/datasets/BAAI/Objaverse-MIX. arXiv admin note:
  substantial text overlap with arXiv:2303.14700</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches for 3D human motion synthesis generate high-quality
animations of digital humans performing a wide variety of actions and gestures.
However, a notable technological gap exists in addressing the complex dynamics
of multi-human interactions within this paradigm. In this work, we present
ReMoS, a denoising diffusion-based model that synthesizes full-body reactive
motion of a person in a two-person interaction scenario. Assuming the motion of
one person is given, we employ a combined spatio-temporal cross-attention
mechanism to synthesize the reactive body and hand motion of the second person,
thereby completing the interactions between the two. We demonstrate ReMoS
across challenging two-person scenarios such as pair-dancing, Ninjutsu,
kickboxing, and acrobatics, where one person's movements have complex and
diverse influences on the other. We also contribute the ReMoCap dataset for
two-person interactions containing full-body and finger motions. We evaluate
ReMoS through multiple quantitative metrics, qualitative visualizations, and a
user study, and also indicate usability in interactive motion editing
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Med<span class="highlight-title">Prompt</span>X: Grounded Multimodal <span class="highlight-title">Prompt</span>ing for Chest X-ray Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mai A. Shaaban, Adnan Khan, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-ray images are commonly used for predicting acute and chronic
cardiopulmonary conditions, but efforts to integrate them with structured
clinical data face challenges due to incomplete electronic health records
(EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate
multimodal large language models (MLLMs), few-shot prompting (FP) and visual
grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A
pre-trained MLLM is utilized to complement the missing EHR information,
providing a comprehensive understanding of patients' medical history.
Additionally, FP reduces the necessity for extensive training of MLLMs while
effectively tackling the issue of hallucination. Nevertheless, the process of
determining the optimal number of few-shot examples and selecting high-quality
candidates can be burdensome, yet it profoundly influences model performance.
Hence, we propose a new technique that dynamically refines few-shot data for
real-time adjustment to new patient scenarios. Moreover, VG aids in focusing
the model's attention on relevant regions of interest in X-ray images,
enhancing the identification of abnormalities. We release MedPromptX-VQA, a new
in-context visual question answering dataset encompassing interleaved image and
EHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the
SOTA performance of MedPromptX, achieving an 11% improvement in F1-score
compared to the baselines. Code and data are available at
https://github.com/BioMedIA-MBZUAI/MedPromptX
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Guided Variational Image Generation for Industrial Anomaly
  Detection and Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Lee, Jongwon Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a text-guided variational image generation method to address the
challenge of getting clean data for anomaly detection in industrial
manufacturing. Our method utilizes text information about the target object,
learned from extensive text library documents, to generate non-defective data
images resembling the input image. The proposed framework ensures that the
generated non-defective images align with anticipated distributions derived
from textual and image-based knowledge, ensuring stability and generality.
Experimental results demonstrate the effectiveness of our approach, surpassing
previous methods even with limited non-defective data. Our approach is
validated through generalization tests across four baseline models and three
distinct datasets. We present an additional analysis to enhance the
effectiveness of anomaly detection models by utilizing the generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identity-aware Dual-constraint Network for Cloth-Changing Person
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peini Guo, Mengyuan Liu, Hong Liu, Ruijia Fan, Guoquan Wang, Bin He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify
the target person in more realistic surveillance scenarios, where pedestrians
usually change their clothing. Despite great progress, limited cloth-changing
training samples in existing CC-ReID datasets still prevent the model from
adequately learning cloth-irrelevant features. In addition, due to the absence
of explicit supervision to keep the model constantly focused on
cloth-irrelevant areas, existing methods are still hampered by the disruption
of clothing variations. To solve the above issues, we propose an Identity-aware
Dual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the
model extract cloth-irrelevant clues, we propose a Clothes Diversity
Augmentation (CDA), which generates more realistic cloth-changing samples by
enriching the clothing color while preserving the texture. In addition, a
Multi-scale Constraint Block (MCB) is designed, which extracts fine-grained
identity-related features and effectively transfers cloth-irrelevant knowledge.
Moreover, a Counterfactual-guided Attention Module (CAM) is presented, which
learns cloth-irrelevant features from channel and space dimensions and utilizes
the counterfactual intervention for supervising the attention map to highlight
identity-related regions. Finally, a Semantic Alignment Constraint (SAC) is
designed to facilitate high-level semantic feature interaction. Comprehensive
experiments on four CC-ReID datasets indicate that our method outperforms prior
state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative 3D part assembly involves understanding part relationships and
predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work
often focus on the geometry of individual parts, neglecting part-whole
hierarchies of objects. Leveraging two key observations: 1) super-part poses
provide strong hints about part poses, and 2) predicting super-part poses is
easier due to fewer superparts, we propose a part-whole-hierarchy message
passing network for efficient 3D part assembly. We first introduce super-parts
by grouping geometrically similar parts without any semantic labels. Then we
employ a part-whole hierarchical encoder, wherein a super-part encoder predicts
latent super-part poses based on input parts. Subsequently, we transform the
point cloud using the latent poses, feeding it to the part encoder for
aggregating super-part information and reasoning about part relationships to
predict all part poses. In training, only ground-truth part poses are required.
During inference, the predicted latent poses of super-parts enhance
interpretability. Experimental results on the PartNet dataset show that our
method achieves state-of-the-art performance in part and connectivity accuracy
and enables an interpretable hierarchical part assembly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree
  Neural Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqing Wang, Tong Zhang, Alaa Abboud, Sabine Süsstrunk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose InNeRF360, an automatic system that accurately removes
text-specified objects from 360-degree Neural Radiance Fields (NeRF). The
challenge is to effectively remove objects while inpainting perceptually
consistent content for the missing regions, which is particularly demanding for
existing NeRF models due to their implicit volumetric representation. Moreover,
unbounded scenes are more prone to floater artifacts in the inpainted region
than frontal-facing scenes, as the change of object appearance and background
across views is more sensitive to inaccurate segmentations and inconsistent
inpainting. With a trained NeRF and a text description, our method efficiently
removes specified objects and inpaints visually consistent content without
artifacts. We apply depth-space warping to enforce consistency across multiview
text-encoded segmentations, and then refine the inpainted NeRF model using
perceptual priors and 3D diffusion-based geometric priors to ensure visual
plausibility. Through extensive experiments in segmentation and inpainting on
360-degree and frontal-facing NeRFs, we show that our approach is effective and
enhances NeRF's editability. Project page: https://ivrl.github.io/InNeRF360.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Passive Non-Line-of-Sight Imaging with Light Transport Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Zhang, Ruixu Geng, Xiaolong Du, Yan Chen, Houqiang Li, Yang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in
recent years, due to its ability to image objects that are out of sight. The
light transport condition plays an important role in this task since changing
the conditions will lead to different imaging models. Existing learning-based
NLOS methods usually train independent models for different light transport
conditions, which is computationally inefficient and impairs the practicality
of the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging
method that effectively handles multiple light transport conditions with a
single network. We achieve this by inferring a latent light transport
representation from the projection image and using this representation to
modulate the network that reconstructs the hidden image from the projection
image. We train a light transport encoder together with a vector quantizer to
obtain the light transport representation. To further regulate this
representation, we jointly learn both the reconstruction network and the
reprojection network during training. A set of light transport modulation
blocks is used to modulate the two jointly trained networks in a multi-scale
way. Extensive experiments on a large-scale passive NLOS dataset demonstrate
the superiority of the proposed method. The code is available at
https://github.com/JerryOctopus/NLOS-LTM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViT-Lens: Towards Omni-modal Representations <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixian Lei, Yixiao Ge, Kun Yi, Jianfeng Zhang, Difei Gao, Dylan Sun, Yuying Ge, Ying Shan, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to advance AI agents, large foundation models significantly improve
reasoning and instruction execution, yet the current focus on vision and
language neglects the potential of perceiving diverse modalities in open-world
environments. However, the success of data-driven vision and language models is
costly or even infeasible to be reproduced for rare modalities. In this paper,
we present ViT-Lens-2 that facilitates efficient omni-modal representation
learning by perceiving novel modalities with a pretrained ViT and aligning them
to a pre-defined space. Specifically, the modality-specific lens is tuned to
project any-modal signals to an intermediate embedding space, which are then
processed by a strong ViT with pre-trained visual knowledge. The encoded
representations are optimized toward aligning with the modal-independent space,
pre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified
solution for representation learning of increasing modalities with two
appealing advantages: (i) Unlocking the great potential of pretrained ViTs to
novel modalities effectively with efficient data regime; (ii) Enabling emergent
downstream capabilities through modality alignment and shared ViT parameters.
We tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio,
tactile and EEG, and set new state-of-the-art results across various
understanding tasks, such as zero-shot classification. By seamlessly
integrating ViT-Lens-2 into Multimodal Foundation Models, we enable
Any-modality to Text and Image Generation in a zero-shot manner. Code and
models are available at https://github.com/TencentARC/ViT-Lens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is a follow-up of arXiv:2308.10185. Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Discriminative Knowledge Learning for Visible-Infrared Person
  Re-Identification <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11708v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11708v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijie Ren, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-Infrared Person Re-identification (VI-ReID) is a challenging
cross-modal pedestrian retrieval task, due to significant intra-class
variations and cross-modal discrepancies among different cameras. Existing
works mainly focus on embedding images of different modalities into a unified
space to mine modality-shared features. They only seek distinctive information
within these shared features, while ignoring the identity-aware useful
information that is implicit in the modality-specific features. To address this
issue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)
network to uncover and leverage the implicit discriminative information
contained within the modality-specific. First, we extract modality-specific and
modality-shared features using a novel dual-stream network. Then, the
modality-specific features undergo purification to reduce their modality style
discrepancies while preserving identity-aware discriminative knowledge.
Subsequently, this kind of implicit knowledge is distilled into the
modality-shared feature to enhance its distinctiveness. Finally, an alignment
loss is proposed to minimize modality discrepancy on enhanced modality-shared
features. Extensive experiments on multiple public datasets demonstrate the
superiority of IDKL network over the state-of-the-art methods. Code is
available at https://github.com/1KK077/IDKL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Search of a Data Transformation That Accelerates Neural Field
  Training <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Seo, Sangyoon Lee, Kwang In Kim, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural field is an emerging paradigm in data representation that trains a
neural network to approximate the given signal. A key obstacle that prevents
its widespread adoption is the encoding speed-generating neural fields requires
an overfitting of a neural network, which can take a significant number of SGD
steps to reach the desired fidelity level. In this paper, we delve into the
impacts of data transformations on the speed of neural field training,
specifically focusing on how permuting pixel locations affect the convergence
speed of SGD. Counterintuitively, we find that randomly permuting the pixel
locations can considerably accelerate the training. To explain this phenomenon,
we examine the neural field training through the lens of PSNR curves, loss
landscapes, and error patterns. Our analyses suggest that the random pixel
permutations remove the easy-to-fit patterns, which facilitate easy
optimization in the early stage but hinder capturing fine details of the
signal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation
  with Unified Audio-Visual Speech Representation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongsoo Choi, Se Jin Park, Minsu Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech
Translation (AV2AV) framework, where the input and output of the system are
multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key
advantages can be brought: 1) We can perform real-like conversations with
individuals worldwide in a virtual meeting by utilizing our own primary
languages. In contrast to Speech-to-Speech Translation (A2A), which solely
translates between audio modalities, the proposed AV2AV directly translates
between audio-visual speech. This capability enhances the dialogue experience
by presenting synchronized lip movements along with the translated speech. 2)
We can improve the robustness of the spoken language translation system. By
employing the complementary information of audio-visual speech, the system can
effectively translate spoken language even in the presence of acoustic noise,
showcasing robust performance. To mitigate the problem of the absence of a
parallel AV2AV translation dataset, we propose to train our spoken language
translation system with the audio-only dataset of A2A. This is done by learning
unified audio-visual speech representations through self-supervised learning in
advance to train the translation system. Moreover, we propose an AV-Renderer
that can generate raw audio and video in parallel. It is designed with
zero-shot speaker modeling, thus the speaker in source audio-visual speech can
be maintained at the target translated audio-visual speech. The effectiveness
of AV2AV is evaluated with extensive experiments in a many-to-many language
translation setting. Demo page is available on
https://choijeongsoo.github.io/av2av.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SINC: Spatial Composition of 3D Human Motions for Simultaneous Action
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10417v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10417v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikos Athanasiou, Mathis Petrovich, Michael J. Black, Gül Varol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our goal is to synthesize 3D human motions given textual inputs describing
simultaneous actions, for example 'waving hand' while 'walking' at the same
time. We refer to generating such simultaneous movements as performing 'spatial
compositions'. In contrast to temporal compositions that seek to transition
from one action to another, spatial compositing requires understanding which
body parts are involved in which action, to be able to move them
simultaneously. Motivated by the observation that the correspondence between
actions and body parts is encoded in powerful language models, we extract this
knowledge by prompting GPT-3 with text such as "what are the body parts
involved in the action <action name>?", while also providing the parts list and
few-shot examples. Given this action-part mapping, we combine body parts from
two motions together and establish the first automated method to spatially
compose two actions. However, training data with compositional actions is
always limited by the combinatorics. Hence, we further create synthetic data
with this approach, and use it to train a new state-of-the-art text-to-motion
generation model, called SINC ("SImultaneous actioN Compositions for 3D human
motions"). In our experiments, that training with such GPT-guided synthetic
data improves spatial composition generation over baselines. Our code is
publicly available at https://sinc.is.tue.mpg.de/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Teaser Fixed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Powerful Lossy Compression for Noisy Images <span class="chip">ICME 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilv Cai, Xiaoguo Liang, Shuning Cao, Luxin Yan, Sheng Zhong, Liqun Chen, Xu Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression and denoising represent fundamental challenges in image
processing with many real-world applications. To address practical demands,
current solutions can be categorized into two main strategies: 1) sequential
method; and 2) joint method. However, sequential methods have the disadvantage
of error accumulation as there is information loss between multiple individual
models. Recently, the academic community began to make some attempts to tackle
this problem through end-to-end joint methods. Most of them ignore that
different regions of noisy images have different characteristics. To solve
these problems, in this paper, our proposed signal-to-noise ratio~(SNR) aware
joint solution exploits local and non-local features for image compression and
denoising simultaneously. We design an end-to-end trainable network, which
includes the main encoder branch, the guidance branch, and the signal-to-noise
ratio~(SNR) aware branch. We conducted extensive experiments on both synthetic
and real-world datasets, demonstrating that our joint solution outperforms
existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViT-Lens: Initiating Omni-Modal Exploration through 3D Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixian Lei, Yixiao Ge, Jianfeng Zhang, Dylan Sun, Kun Yi, Ying Shan, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though the success of CLIP-based training recipes in vision-language models,
their scalability to more modalities (e.g., 3D, audio, etc.) is limited to
large-scale data, which is expensive or even inapplicable for rare modalities.
In this paper, we present ViT-Lens that facilitates efficient omni-modal
representation learning by perceiving novel modalities with a pretrained ViT
and aligning to a pre-defined space. Specifically, the modality-specific lens
is tuned to project multimodal signals to the shared embedding space, which are
then processed by a strong ViT that carries pre-trained image knowledge. The
encoded multimodal representations are optimized toward aligning with the
modal-independent space, pre-defined by off-the-shelf foundation models. A
well-trained lens with a ViT backbone has the potential to serve as one of
these foundation models, supervising the learning of subsequent modalities.
ViT-Lens provides a unified solution for representation learning of increasing
modalities with two appealing benefits: (i) Exploiting the pretrained ViT
across tasks and domains effectively with efficient data regime; (ii) Emergent
downstream capabilities of novel modalities are demonstrated due to the
modality alignment space. We evaluate ViT-Lens in the context of 3D as an
initial verification. In zero-shot 3D classification, ViT-Lens achieves
substantial improvements over previous state-of-the-art, showing 52.0% accuracy
on Objaverse-LVIS, 87.4% on ModelNet40, and 60.6% on ScanObjectNN. Furthermore,
we enable zero-shot 3D question-answering by simply integrating the trained 3D
lens into the InstructBLIP model without any adaptation. We will release the
results of ViT-Lens on more modalities in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures and 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TP2O: Creative Text Pair-to-Object Generation using Balance
  Swap-Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01819v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01819v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Li, Zedong Zhang, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating creative combinatorial objects from two seemingly unrelated object
texts is a challenging task in text-to-image synthesis, often hindered by a
focus on emulating existing data distributions. In this paper, we develop a
straightforward yet highly effective method, called \textbf{balance
swap-sampling}. First, we propose a swapping mechanism that generates a novel
combinatorial object image set by randomly exchanging intrinsic elements of two
text embeddings through a cutting-edge diffusion model. Second, we introduce a
balance swapping region to efficiently sample a small subset from the newly
generated image set by balancing CLIP distances between the new images and
their original generations, increasing the likelihood of accepting the
high-quality combinations. Last, we employ a segmentation method to compare
CLIP distances among the segmented components, ultimately selecting the most
promising object from the sampled subset. Extensive experiments demonstrate
that our approach outperforms recent SOTA T2I methods. Surprisingly, our
results even rival those of human artists, such as frog-broccoli.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://tp2o.github.io/anon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segment and Caption Anything <span class="chip">CVPR 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, Zicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to efficiently equip the Segment Anything Model (SAM)
with the ability to generate regional captions. SAM presents strong
generalizability to segment anything while is short for semantic understanding.
By introducing a lightweight query-based feature mixer, we align the
region-specific features with the embedding space of language models for later
caption generation. As the number of trainable parameters is small (typically
in the order of tens of millions), it costs less computation, less memory
usage, and less communication bandwidth, resulting in both fast and scalable
training. To address the scarcity problem of regional caption data, we propose
to first pre-train our model on objection detection and segmentation tasks. We
call this step weak supervision pretraining since the pre-training data only
contains category names instead of full-sentence descriptions. The weak
supervision pretraining allows us to leverage many publicly available object
detection and segmentation datasets. We conduct extensive experiments to
demonstrate the superiority of our method and validate each design choice. This
work serves as a stepping stone towards scaling up regional captioning data and
sheds light on exploring efficient ways to augment SAM with regional semantics.
The project page, along with the associated code, can be accessed via
https://xk-huang.github.io/segment-caption-anything/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page, along with the associated code, can be accessed via
  https://xk-huang.github.io/segment-caption-anything/; Update author
  information; Accepted by CVPR 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TagAlign: Improving Vision-Language Alignment with Multi-Tag
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14149v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14149v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinying Liu, Wei Wu, Kecheng Zheng, Zhan Tong, Jiawei Liu, Yu Liu, Wei Chen, Zilei Wang, Yujun Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The crux of learning vision-language models is to extract semantically
aligned information from visual and linguistic data. Existing attempts usually
face the problem of coarse alignment, e.g., the vision encoder struggles in
localizing an attribute-specified object. In this work, we propose an
embarrassingly simple approach to better align image and text features with no
need of additional data formats other than image-text pairs. Concretely, given
an image and its paired text, we manage to parse objects (e.g., cat) and
attributes (e.g., black) from the description, which are highly likely to exist
in the image. It is noteworthy that the parsing pipeline is fully automatic and
thus enjoys good scalability. With these parsed semantics as supervision
signals, we can complement the commonly used image-text contrastive loss with
the multi-tag classification loss. Extensive experimental results on a broad
suite of semantic segmentation datasets substantiate the average 5.2\%
improvement of our framework over existing alternatives. Furthermore, the
visualization results indicate that attribute supervision makes vision-language
models accurately localize attribute-specified objects. Project page can be
found at https://qinying-liu.github.io/Tag-Align.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03246v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03246v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Tianchen Deng, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian
Splatting. It incorporates appearance, geometry, and semantic features through
multi-channel optimization, addressing the oversmoothing limitations of neural
implicit SLAM systems in high-quality rendering, scene understanding, and
object-level geometry. We introduce a unique semantic feature loss that
effectively compensates for the shortcomings of traditional depth and color
losses in object optimization. Through a semantic-guided keyframe selection
strategy, we prevent erroneous reconstructions caused by cumulative errors.
Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art
performance in camera pose estimation, map reconstruction, precise semantic
segmentation, and object-level geometric accuracy, while ensuring real-time
rendering capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder
  and Explicit Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dar-Yen Chen, Hamish Tennent, Ching-Wen Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces ArtAdapter, a transformative text-to-image (T2I) style
transfer framework that transcends traditional limitations of color,
brushstrokes, and object shape, capturing high-level style elements such as
composition and distinctive artistic expression. The integration of a
multi-level style encoder with our proposed explicit adaptation mechanism
enables ArtAdapter to achieve unprecedented fidelity in style transfer,
ensuring close alignment with textual descriptions. Additionally, the
incorporation of an Auxiliary Content Adapter (ACA) effectively separates
content from style, alleviating the borrowing of content from style references.
Moreover, our novel fast finetuning approach could further enhance zero-shot
style representation while mitigating the risk of overfitting. Comprehensive
evaluations confirm that ArtAdapter surpasses current state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clean-image Backdoor Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhong Rong, Guoyao Yu, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To gather a significant quantity of annotated training data for
high-performance image classification models, numerous companies opt to enlist
third-party providers to label their unlabeled data. This practice is widely
regarded as secure, even in cases where some annotated errors occur, as the
impact of these minor inaccuracies on the final performance of the models is
negligible and existing backdoor attacks require attacker's ability to poison
the training images. Nevertheless, in this paper, we propose clean-image
backdoor attacks which uncover that backdoors can still be injected via a
fraction of incorrect labels without modifying the training images.
Specifically, in our attacks, the attacker first seeks a trigger feature to
divide the training images into two parts: those with the feature and those
without it. Subsequently, the attacker falsifies the labels of the former part
to a backdoor class. The backdoor will be finally implanted into the target
model after it is trained on the poisoned data. During the inference phase, the
attacker can activate the backdoor in two ways: slightly modifying the input
image to obtain the trigger feature, or taking an image that naturally has the
trigger feature as input. We conduct extensive experiments to demonstrate the
effectiveness and practicality of our attacks. According to the experimental
results, we conclude that our attacks seriously jeopardize the fairness and
robustness of image classification models, and it is necessary to be vigilant
about the incorrect labels in outsourced labeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferring Relative Monocular Depth to Surgical Vision with Temporal
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Budd, Tom Vercauteren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relative monocular depth, inferring depth up to shift and scale from a single
image, is an active research topic. Recent deep learning models, trained on
large and varied meta-datasets, now provide excellent performance in the domain
of natural images. However, few datasets exist which provide ground truth depth
for endoscopic images, making training such models from scratch unfeasible.
This work investigates the transfer of these models into the surgical domain,
and presents an effective and simple way to improve on standard supervision
through the use of temporal consistency self-supervision. We show temporal
consistency significantly improves supervised training alone when transferring
to the low-data regime of endoscopy, and outperforms the prevalent
self-supervision technique for this task. In addition we show our method
drastically outperforms the state-of-the-art method from within the domain of
endoscopy. We also release our code, model and ensembled meta-dataset,
Meta-MED, establishing a strong benchmark for future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Source-free Domain Adaptive Semantic Segmentation via
  Importance-aware and Prototype-contrast Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Cao, Hui Zhang, Xiao Lu, Zheng Xiao, Kailun Yang, Yaonan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive semantic segmentation enables robust pixel-wise understanding
in real-world driving scenes. Source-free domain adaptation, as a more
practical technique, addresses the concerns of data privacy and storage
limitations in typical unsupervised domain adaptation methods, making it
especially relevant in the context of intelligent vehicles. It utilizes a
well-trained source model and unlabeled target data to achieve adaptation in
the target domain. However, in the absence of source data and target labels,
current solutions cannot sufficiently reduce the impact of domain shift and
fully leverage the information from the target data. In this paper, we propose
an end-to-end source-free domain adaptation semantic segmentation method via
Importance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC
framework effectively extracts domain-invariant knowledge from the well-trained
source model and learns domain-specific knowledge from the unlabeled target
domain. Specifically, considering the problem of domain shift in the prediction
of the target domain by the source model, we put forward an importance-aware
mechanism for the biased target prediction probability distribution to extract
domain-invariant knowledge from the source model. We further introduce a
prototype-contrast strategy, which includes a prototype-symmetric cross-entropy
loss and a prototype-enhanced cross-entropy loss, to learn target intra-domain
knowledge without relying on labels. A comprehensive variety of experiments on
two domain adaptive semantic segmentation benchmarks demonstrates that the
proposed end-to-end IAPC solution outperforms existing state-of-the-art
methods. The source code is publicly available at
https://github.com/yihong-97/Source-free-IAPC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The
  source code is publicly available at
  https://github.com/yihong-97/Source-free-IAPC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SD4Match: Learning to <span class="highlight-title">Prompt</span> Stable Diffusion Model for Semantic
  Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of matching semantically similar
keypoints across image pairs. Existing research indicates that the intermediate
output of the UNet within the Stable Diffusion (SD) can serve as robust image
feature maps for such a matching task. We demonstrate that by employing a basic
prompt tuning technique, the inherent potential of Stable Diffusion can be
harnessed, resulting in a significant enhancement in accuracy over previous
approaches. We further introduce a novel conditional prompting module that
conditions the prompt on the local details of the input image pairs, leading to
a further improvement in performance. We designate our approach as SD4Match,
short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of
SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets
new benchmarks in accuracy across all these datasets. Particularly, SD4Match
outperforms the previous state-of-the-art by a margin of 12 percentage points
on the challenging SPair-71k dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project website:
  https://sd4match.active.vision/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ObjectCompose: Evaluating Resilience of Vision-Based Models on
  Object-to-Background Compositional Changes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the large-scale multi-modal training of recent vision-based models and
their generalization capabilities, understanding the extent of their robustness
is critical for their real-world deployment. In this work, we evaluate the
resilience of current vision-based models against diverse object-to-background
context variations. The majority of robustness evaluation methods have
introduced synthetic datasets to induce changes to object characteristics
(viewpoints, scale, color) or utilized image transformation techniques
(adversarial changes, common corruptions) on real images to simulate shifts in
distributions. Recent works have explored leveraging large language models and
diffusion models to generate changes in the background. However, these methods
either lack in offering control over the changes to be made or distort the
object semantics, making them unsuitable for the task. Our method, on the other
hand, can induce diverse object-to-background changes while preserving the
original semantics and appearance of the object. To achieve this goal, we
harness the generative capabilities of text-to-image, image-to-text, and
image-to-segment models to automatically generate a broad spectrum of
object-to-background changes. We induce both natural and adversarial background
changes by either modifying the textual prompts or optimizing the latents and
textual embedding of text-to-image models. We produce various versions of
standard vision datasets (ImageNet, COCO), incorporating either diverse and
realistic backgrounds into the images or introducing color, texture, and
adversarial changes in the background. We conduct extensive experiment to
analyze the robustness of vision-based models against object-to-background
context variations across diverse tasks. Code
https://github.com/Muhammad-Huzaifaa/ObjectCompose.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate human motion sequences from given
textual descriptions, where the model explores diverse mappings from natural
language instructions to human body movements. While most existing works are
confined to coarse-grained motion descriptions, e.g., "A man squats.",
fine-grained descriptions specifying movements of relevant body parts are
barely explored. Models trained with coarse-grained texts may not be able to
learn mappings from fine-grained motion-related words to motion primitives,
resulting in the failure to generate motions from unseen descriptions. In this
paper, we build a large-scale language-motion dataset specializing in
fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with
step-by-step instructions with pseudo-code compulsory checks. Accordingly, we
design a new text2motion model, FineMotionDiffuse, making full use of
fine-grained textual information. Our quantitative evaluation shows that
FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of
0.38, compared with competitive baselines. According to the qualitative
evaluation and case study, our model outperforms MotionDiffuse in generating
spatially or chronologically composite motions, by learning the implicit
mappings from fine-grained descriptions to the corresponding basic motions. We
release our data at https://github.com/KunhangL/finemotiondiffuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The personalization of machine learning (ML) models to address data drift is
a significant challenge in the context of Internet of Things (IoT)
applications. Presently, most approaches focus on fine-tuning either the full
base model or its last few layers to adapt to new data, while often neglecting
energy costs. However, various types of data drift exist, and fine-tuning the
full base model or the last few layers may not result in optimal performance in
certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy
adaptive personalization framework designed for resource-constrained devices.
We categorize data drift and personalization into three types: input-level,
feature-level, and output-level. For each type, we fine-tune different blocks
of the model to achieve optimal performance with reduced energy costs.
Specifically, input-, feature-, and output-level correspond to fine-tuning the
front, middle, and rear blocks of the model. We evaluate TBFT on a ResNet
model, three datasets, three different training sizes, and a Raspberry Pi.
Compared with the $Block Avg$, where each block is fine-tuned individually and
their performance improvements are averaged, TBFT exhibits an improvement in
model accuracy by an average of 15.30% whilst saving 41.57% energy consumption
on average compared with full fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FPT: Fine-grained <span class="highlight-title">Prompt</span> Tuning for Parameter and Memory Efficient Fine
  Tuning in High-resolution Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to
transfer pre-trained models to downstream tasks, avoiding the high cost of
updating entire large-scale pre-trained models (LPMs). In this work, we present
Fine-grained Prompt Tuning (FPT), a novel PEFT method for medical image
classification. FPT significantly reduces memory consumption compared to other
PEFT methods, especially in high-resolution contexts. To achieve this, we first
freeze the weights of the LPM and construct a learnable lightweight side
network. The frozen LPM takes high-resolution images as input to extract
fine-grained features, while the side network is fed low-resolution images to
reduce memory usage. To allow the side network to access pre-trained knowledge,
we introduce fine-grained prompts that summarize information from the LPM
through a fusion module. Important tokens selection and preloading techniques
are employed to further reduce training cost and memory requirements. We
evaluate FPT on four medical datasets with varying sizes, modalities, and
complexities. Experimental results demonstrate that FPT achieves comparable
performance to fine-tuning the entire LPM while using only 1.8% of the
learnable parameters and 13% of the memory costs of an encoder ViT-B model with
a 512 x 512 input resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegVol: Universal and Interactive Volumetric Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13385v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13385v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise image segmentation provides clinical study with instructive
information. Despite the remarkable progress achieved in medical image
segmentation, there is still an absence of 3D foundation segmentation model
that can segment a wide range of anatomical categories with easy user
interaction. In this paper, we propose a 3D foundation segmentation model,
named SegVol, supporting universal and interactive volumetric medical image
segmentation. By scaling up training data to 90K unlabeled Computed Tomography
(CT) volumes and 6K labeled CT volumes, this foundation model supports the
segmentation of over 200 anatomical categories using semantic and spatial
prompts. Extensive experiments on 10 internal validation tasks and 18 external
validation tasks verify that SegVol outperforms the state of the art by a large
margin. Through its capacity to provide precise volumetric segmentation across
various anatomical categories, SegVol has the potential to accelerate
advancements in medical imaging diagnosis and facilitate treatment
optimization. The model and code are publicly available at:
https://github.com/BAAI-DCAI/SegVol.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamComposer: Controllable 3D Object Generation via Multi-View
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yhyang-myron.github.io/DreamComposer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularizing <span class="highlight-title">Self-supervised</span> 3D Scene Flows with Surface Awareness and
  Cyclic Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrik Vacek, David Hurych, Karel Zimmermann, Patrick Perez, Tomas Svoboda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning without supervision how to predict 3D scene flows from point clouds
is essential to many perception systems. We propose a novel learning framework
for this task which improves the necessary regularization. Relying on the
assumption that scene elements are mostly rigid, current smoothness losses are
built on the definition of ``rigid clusters" in the input point clouds. The
definition of these clusters is challenging and has a significant impact on the
quality of predicted flows. We introduce two new consistency losses that
enlarge clusters while preventing them from spreading over distinct objects. In
particular, we enforce \emph{temporal} consistency with a forward-backward
cyclic loss and \emph{spatial} consistency by considering surface orientation
similarity in addition to spatial proximity. The proposed losses are
model-independent and can thus be used in a plug-and-play fashion to
significantly improve the performance of existing models, as demonstrated on
two most widely used architectures. We also showcase the effectiveness and
generalization capability of our framework on four standard sensor-unique
driving datasets, achieving state-of-the-art performance in 3D scene flow
estimation. Our codes are available on https://github.com/ctu-vras/sac-flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P2ANet: A <span class="highlight-title">Dataset</span> and Benchmark for Dense Action Detection from Table
  Tennis Match Broadcasting Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Bian, Xuhong Li, Tao Wang, Qingzhong Wang, Jun Huang, Chen Liu, Jun Zhao, Feixiang Lu, Dejing Dou, Haoyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has been widely used for video analytics, such as video
classification and action detection, dense action detection with fast-moving
subjects from sports videos is still challenging. In this work, we release yet
another sports video benchmark \TheName{} for \emph{\underline{P}}ing
\emph{\underline{P}}ong-\emph{\underline{A}}ction detection, which consists of
2,721 video clips collected from the broadcasting videos of professional table
tennis matches in World Table Tennis Championships and Olympiads. We work with
a crew of table tennis professionals and referees on a specially designed
annotation toolbox to obtain fine-grained action labels (in 14 classes) for
every ping-pong action that appeared in the dataset, and formulate two sets of
action detection problems -- \emph{action localization} and \emph{action
recognition}. We evaluate a number of commonly-seen action recognition (e.g.,
TSM, TSN, Video SwinTransformer, and Slowfast) and action localization models
(e.g., BSN, BSN++, BMN, TCANet), using \TheName{} for both problems, under
various settings. These models can only achieve 48\% area under the AR-AN curve
for localization and 82\% top-one accuracy for recognition since the ping-pong
actions are dense with fast-moving subjects but broadcasting videos are with
only 25 FPS. The results confirm that \TheName{} is still a challenging task
and can be used as a special benchmark for dense action detection from videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Semantic Segmentation Through Depth-Guided Feature
  Correlation and Sampling <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, training neural networks to perform semantic segmentation
required expensive human-made annotations. But more recently, advances in the
field of unsupervised learning have made significant progress on this issue and
towards closing the gap to supervised algorithms. To achieve this, semantic
knowledge is distilled by learning to correlate randomly sampled features from
images across an entire dataset. In this work, we build upon these advances by
incorporating information about the structure of the scene into the training
process through the use of depth information. We achieve this by (1) learning
depth-feature correlation by spatially correlate the feature maps with the
depth maps to induce knowledge about the structure of the scene and (2)
implementing farthest-point sampling to more effectively select relevant
features by utilizing 3D sampling techniques on depth information of the scene.
Finally, we demonstrate the effectiveness of our technical contributions
through extensive experimentation and present significant improvements in
performance across multiple benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of
  Illumination and Reflectance <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Enyo, Ko Nishino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reflectance bounds the frequency spectrum of illumination in the object
appearance. In this paper, we introduce the first stochastic inverse rendering
method, which recovers the attenuated frequency spectrum of an illumination
jointly with the reflectance of an object of known geometry from a single
image. Our key idea is to solve this blind inverse problem in the reflectance
map, an appearance representation invariant to the underlying geometry, by
learning to reverse the image formation with a novel diffusion model which we
refer to as the Diffusion Reflectance Map Network (DRMNet). Given an observed
reflectance map converted and completed from the single input image, DRMNet
generates a reflectance map corresponding to a perfect mirror sphere while
jointly estimating the reflectance. The forward process can be understood as
gradually filtering a natural illumination with lower and lower frequency
reflectance and additive Gaussian noise. DRMNet learns to invert this process
with two subnetworks, IllNet and RefNet, which work in concert towards this
joint estimation. The network is trained on an extensive synthetic dataset and
is demonstrated to generalize to real images, showing state-of-the-art accuracy
on established datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProMamba: <span class="highlight-title">Prompt</span>-Mamba for polyp segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Xie, Ruofan Liao, Ziang Zhang, Sida Yi, Yuesheng Zhu, Guibo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting polyps through colonoscopy is an important task in medical image
segmentation, which provides significant assistance and reference value for
clinical surgery. However, accurate segmentation of polyps is a challenging
task due to two main reasons. Firstly, polyps exhibit various shapes and
colors. Secondly, the boundaries between polyps and their normal surroundings
are often unclear. Additionally, significant differences between different
datasets lead to limited generalization capabilities of existing methods. To
address these issues, we propose a segmentation model based on Prompt-Mamba,
which incorporates the latest Vision-Mamba and prompt technologies. Compared to
previous models trained on the same dataset, our model not only maintains high
segmentation accuracy on the validation part of the same dataset but also
demonstrates superior accuracy on unseen datasets, exhibiting excellent
generalization capabilities. Notably, we are the first to apply the
Vision-Mamba architecture to polyp segmentation and the first to utilize prompt
technology in a polyp segmentation model. Our model efficiently accomplishes
segmentation tasks, surpassing previous state-of-the-art methods by an average
of 5% across six datasets. Furthermore, we have developed multiple versions of
our model with scaled parameter counts, achieving better performance than
previous models even with fewer parameters. Our code and trained weights will
be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures,3 tabels</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SocialCircle: Learning the Angle-based Social Interaction Representation
  for Pedestrian Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conghao Wong, Beihao Xia, Ziqian Zou, Yulong Wang, Xinge You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing and forecasting trajectories of agents like pedestrians and cars in
complex scenes has become more and more significant in many intelligent systems
and applications. The diversity and uncertainty in socially interactive
behaviors among a rich variety of agents make this task more challenging than
other deterministic computer vision tasks. Researchers have made a lot of
efforts to quantify the effects of these interactions on future trajectories
through different mathematical models and network structures, but this problem
has not been well solved. Inspired by marine animals that localize the
positions of their companions underwater through echoes, we build a new
anglebased trainable social interaction representation, named SocialCircle, for
continuously reflecting the context of social interactions at different angular
orientations relative to the target agent. We validate the effect of the
proposed SocialCircle by training it along with several newly released
trajectory prediction models, and experiments show that the SocialCircle not
only quantitatively improves the prediction performance, but also qualitatively
helps better simulate social interactions when forecasting pedestrian
trajectories in a way that is consistent with human intuitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotic Masked Autoencoder with Attention Fusion for Facial Expression
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bach Nguyen-Xuan, Thien Nguyen-Hoang, Nhu Tai-Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Expression Recognition (FER) is a critical task within computer vision
with diverse applications across various domains. Addressing the challenge of
limited FER datasets, which hampers the generalization capability of expression
recognition models, is imperative for enhancing performance. Our paper presents
an innovative approach integrating the MAE-Face self-supervised learning (SSL)
method and Fusion Attention mechanism for expression classification,
particularly showcased in the 6th Affective Behavior 32 pages harvmac; added
references for section 5Analysis in-the-wild (ABAW) competition. Additionally,
we propose preprocessing techniques to emphasize essential facial features,
thereby enhancing model performance on both training and validation sets,
notably demonstrated on the Aff-wild2 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages; added references for section 1; corrected typo for email
  author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning User Embeddings from Human Gaze for Personalised Saliency
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Strohm, Mihai Bâce, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reusable embeddings of user behaviour have shown significant performance
improvements for the personalised saliency prediction task. However, prior
works require explicit user characteristics and preferences as input, which are
often difficult to obtain. We present a novel method to extract user embeddings
from pairs of natural images and corresponding saliency maps generated from a
small amount of user-specific eye tracking data. At the core of our method is a
Siamese convolutional neural encoder that learns the user embeddings by
contrasting the image and personal saliency map pairs of different users.
Evaluations on two public saliency datasets show that the generated embeddings
have high discriminative power, are effective at refining universal saliency
maps to the individual users, and generalise well across users and images.
Finally, based on our model's ability to encode individual user
characteristics, our work points towards other applications that can benefit
from reusable embeddings of gaze behaviour.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VRP-SAM: SAM with Visual Reference <span class="highlight-title">Prompt</span> <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, Zechao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that
empowers the Segment Anything Model (SAM) to utilize annotated reference images
as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM
can utilize annotated reference images to comprehend specific objects and
perform segmentation of specific objects in target image. It is note that the
VRP encoder can support a variety of annotation formats for reference images,
including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}.
VRP-SAM achieves a breakthrough within the SAM framework by extending its
versatility and applicability while preserving SAM's inherent strengths, thus
enhancing user-friendliness. To enhance the generalization ability of VRP-SAM,
the VRP encoder adopts a meta-learning strategy. To validate the effectiveness
of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO
datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual
reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM
demonstrates strong generalization capabilities, allowing it to perform
segmentation of unseen objects and enabling cross-domain segmentation. The
source code and models will be available at
\url{https://github.com/syp2ysy/VRP-SAM}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024; The camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Strohm, Mihai Bâce, Markus Kaltenecker, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Semantic Facial Feature Control (SeFFeC) - a novel method for
fine-grained face shape editing. Our method enables the manipulation of
human-understandable, semantic face features, such as nose length or mouth
width, which are defined by different groups of facial landmarks. In contrast
to existing methods, the use of facial landmarks enables precise measurement of
the facial features, which then enables training SeFFeC without any manually
annotated labels. SeFFeC consists of a transformer-based encoder network that
takes a latent vector of a pre-trained generative model and a facial feature
embedding as input, and learns to modify the latent vector to perform the
desired face edit operation. To ensure that the desired feature measurement is
changed towards the target value without altering uncorrelated features, we
introduced a novel semantic face feature loss. Qualitative and quantitative
results show that SeFFeC enables precise and fine-grained control of 23 facial
features, some of which could not previously be controlled by other methods,
without requiring manual annotations. Unlike existing methods, SeFFeC also
provides deterministic control over the exact values of the facial features and
more localised and disentangled face edits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Prototype Attention for Unsupervised Video Object Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12036v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12036v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhwan Cho, Minhyeok Lee, Seunghoon Lee, Dogyoon Lee, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised video object segmentation (VOS) aims to detect and segment the
most salient object in videos. The primary techniques used in unsupervised VOS
are 1) the collaboration of appearance and motion information; and 2) temporal
fusion between different frames. This paper proposes two novel prototype-based
attention mechanisms, inter-modality attention (IMA) and inter-frame attention
(IFA), to incorporate these techniques via dense propagation across different
modalities and frames. IMA densely integrates context information from
different modalities based on a mutual refinement. IFA injects global context
of a video to the query frame, enabling a full utilization of useful properties
from multiple frames. Experimental results on public benchmark datasets
demonstrate that our proposed approach outperforms all existing methods by a
substantial margin. The proposed two components are also thoroughly validated
via ablative study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Pretext to Purpose: Batch-Adaptive <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiansong Zhang, Linlin Shen, Peizhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, self-supervised contrastive learning has emerged as a
distinguished paradigm in the artificial intelligence landscape. It facilitates
unsupervised feature learning through contrastive delineations at the instance
level. However, crafting an effective self-supervised paradigm remains a
pivotal challenge within this field. This paper delves into two crucial factors
impacting self-supervised contrastive learning-bach size and pretext tasks, and
from a data processing standpoint, proposes an adaptive technique of batch
fusion. The proposed method, via dimensionality reduction and reconstruction of
batch data, enables formerly isolated individual data to partake in intra-batch
communication through the Embedding Layer. Moreover, it adaptively amplifies
the self-supervised feature encoding capability as the training progresses. We
conducted a linear classification test of this method based on the classic
contrastive learning framework on ImageNet-1k. The empirical findings
illustrate that our approach achieves state-of-the-art performance under
equitable comparisons. Benefiting from its "plug-and-play" characteristics, we
further explored other contrastive learning methods. On the ImageNet-100,
compared to the original performance, the top1 has seen a maximum increase of
1.25%. We suggest that the proposed method may contribute to the advancement of
data-driven self-supervised learning research, bringing a fresh perspective to
this community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, the code of this paper will be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaFS: When Large Language Models Meet Few-Shot Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16926v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16926v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanyun Zhu, Tianrun Chen, Deyi Ji, Jieping Ye, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes LLaFS, the first attempt to leverage large language
models (LLMs) in few-shot segmentation. In contrast to the conventional
few-shot segmentation methods that only rely on the limited and biased
information from the annotated support images, LLaFS leverages the vast prior
knowledge gained by LLM as an effective supplement and directly uses the LLM to
segment images in a few-shot manner. To enable the text-based LLM to handle
image-related tasks, we carefully design an input instruction that allows the
LLM to produce segmentation results represented as polygons, and propose a
region-attribute table to simulate the human visual mechanism and provide
multi-modal guidance. We also synthesize pseudo samples and use curriculum
learning for pretraining to augment data and achieve better optimization. LLaFS
achieves state-of-the-art results on multiple datasets, showing the potential
of using LLMs for few-shot computer vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship
  Detection through Edge-Cloud Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Huang, Hanning Chen, Yang Ni, Arghavan Rezvani, Sanggeon Yun, Sungheon Jeon, Eric Pedley, Mohsen Imani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting marine objects inshore presents challenges owing to algorithmic
intricacies and complexities in system deployment. We propose a
difficulty-aware edge-cloud collaborative sensing system that splits the task
into object localization and fine-grained classification. Objects are
classified either at the edge or within the cloud, based on their estimated
difficulty. The framework comprises a low-power device-tailored front-end model
for object localization, classification, and difficulty estimation, along with
a transformer-graph convolutional network-based back-end model for fine-grained
classification. Our system demonstrates superior performance (mAP@0.5 +4.3%})
on widely used marine object detection datasets, significantly reducing both
data transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the
system level. We validate the proposed system across various embedded system
platforms and in real-world scenarios involving drone deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision <span class="highlight-title">Transformer</span>s with Hierarchical Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.03180v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.03180v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Liu, Yu-Huan Wu, Guolei Sun, Le Zhang, Ajad Chhatkuli, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the high computational/space complexity associated with
Multi-Head Self-Attention (MHSA) in vanilla vision transformers. To this end,
we propose Hierarchical MHSA (H-MHSA), a novel approach that computes
self-attention in a hierarchical fashion. Specifically, we first divide the
input image into patches as commonly done, and each patch is viewed as a token.
Then, the proposed H-MHSA learns token relationships within local patches,
serving as local relationship modeling. Then, the small patches are merged into
larger ones, and H-MHSA models the global dependencies for the small number of
the merged tokens. At last, the local and global attentive features are
aggregated to obtain features with powerful representation capacity. Since we
only calculate attention for a limited number of tokens at each step, the
computational load is reduced dramatically. Hence, H-MHSA can efficiently model
global relationships among tokens without sacrificing fine-grained information.
With the H-MHSA module incorporated, we build a family of
Hierarchical-Attention-based Transformer Networks, namely HAT-Net. To
demonstrate the superiority of HAT-Net in scene understanding, we conduct
extensive experiments on fundamental vision tasks, including image
classification, semantic segmentation, object detection, and instance
segmentation. Therefore, HAT-Net provides a new perspective for vision
transformers. Code and pretrained models are available at
https://github.com/yun-liu/HAT-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Machine Intelligence Research (MIR), DOI: 10.1007/s11633-024-1393-8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07728v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07728v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Mao, Chenming Wu, Zhelun Shen, Yifan Wang, Dayan Wu, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method, namely NeuS-PIR, for recovering relightable
neural surfaces using pre-integrated rendering from multi-view images or video.
Unlike methods based on NeRF and discrete meshes, our method utilizes implicit
neural surface representation to reconstruct high-quality geometry, which
facilitates the factorization of the radiance field into two components: a
spatially varying material field and an all-frequency lighting representation.
This factorization, jointly optimized using an adapted differentiable
pre-integrated rendering framework with material encoding regularization, in
turn addresses the ambiguity of geometry reconstruction and leads to better
disentanglement and refinement of each scene property. Additionally, we
introduced a method to distil indirect illumination fields from the learned
representations, further recovering the complex illumination effect like
inter-reflection. Consequently, our method enables advanced applications such
as relighting, which can be seamlessly integrated with modern graphics engines.
Qualitative and quantitative experiments have shown that NeuS-PIR outperforms
existing methods across various tasks on both synthetic and real datasets.
Source code is available at https://github.com/Sheldonmao/NeuSPIR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Tian, Lingxiao Yang, Ran Ji, Yuexin Ma, Lan Xu, Jingyi Yu, Ye Shi, Jingya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaze plays a crucial role in revealing human attention and intention,
shedding light on the cognitive processes behind human actions. The integration
of gaze guidance with the dynamics of hand-object interactions boosts the
accuracy of human motion prediction. However, the lack of datasets that capture
the intricate relationship and consistency among gaze, hand, and object
movements remains a substantial hurdle. In this paper, we introduce the first
Gaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task
for synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI,
features simultaneous 3D modeling of gaze, hand, and object interactions,
comprising 479 sequences with an average duration of 19.1 seconds, 812
sub-sequences, and 33 objects of various sizes. We propose a hierarchical
framework centered on a gaze-guided hand-object interaction diffusion model,
named GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions
into spatial-temporal features and goal pose conditions at different levels of
information granularity. During the diffusion phase, two gaze-conditioned
diffusion models are stacked to simplify the complex synthesis of hand-object
motions. Here, the object motion diffusion model generates sequences of object
motions based on gaze conditions, while the hand motion diffusion model
produces hand motions based on the generated object motion. To improve
fine-grained goal pose alignment, we introduce a Spherical Gaussian constraint
to guide the denoising step. In the subsequent post-diffusion phase, we
optimize the generated hand motions using contact consistency. Our extensive
experiments highlight the uniqueness of our dataset and the effectiveness of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-based Axial Video Motion Magnification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwon Byung-Ki, Oh Hyun-Bin, Kim Jun-Seong, Hyunwoo Ha, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video motion magnification amplifies invisible small motions to be
perceptible, which provides humans with a spatially dense and holistic
understanding of small motions in the scene of interest. This is based on the
premise that magnifying small motions enhances the legibility of motions. In
the real world, however, vibrating objects often possess convoluted systems
that have complex natural frequencies, modes, and directions. Existing motion
magnification often fails to improve legibility since the intricate motions
still retain complex characteristics even after being magnified, which may
distract us from analyzing them. In this work, we focus on improving legibility
by proposing a new concept, axial motion magnification, which magnifies
decomposed motions along the user-specified direction. Axial motion
magnification can be applied to various applications where motions of specific
axes are critical, by providing simplified and easily readable motion
information. To achieve this, we propose a novel Motion Separation Module that
enables to disentangle and magnify the motion representation along axes of
interest. Furthermore, we build a new synthetic training dataset for the axial
motion magnification task. Our proposed method improves the legibility of
resulting motions along certain axes by adding a new feature: user
controllability. Axial motion magnification is a more generalized concept;
thus, our method can be directly adapted to the generic motion magnification
and achieves favorable performance against competing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper: 12 pages, supplementary: 10 pages, 20 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search and Society: Reimagining Information Access for Radical Futures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval (IR) technologies and research are undergoing
transformative changes. It is our perspective that the community should accept
this opportunity to re-center our research agendas on societal needs while
dismantling the artificial separation between the work on fairness,
accountability, transparency, and ethics in IR and the rest of IR research.
Instead of adopting a reactionary strategy of trying to mitigate potential
social harms from emerging technologies, the community should aim to
proactively set the research agenda for the kinds of systems we should build
inspired by diverse explicitly stated sociotechnical imaginaries. The
sociotechnical imaginaries that underpin the design and development of
information access technologies needs to be explicitly articulated, and we need
to develop theories of change in context of these diverse perspectives. Our
guiding future imaginaries must be informed by other academic fields, such as
democratic theory and critical theory, and should be co-developed with social
science scholars, legal scholars, civil rights and social justice activists,
and artists, among others. In this perspective paper, we motivate why the
community must consider this radical shift in how we do research and what we
work on, and sketch a path forward towards this transformation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIND Your Language: A Multilingual <span class="highlight-title">Dataset</span> for Cross-lingual News
  Recommendation <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreea Iana, Goran Glavaš, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital news platforms use news recommenders as the main instrument to cater
to the individual information needs of readers. Despite an increasingly
language-diverse online community, in which many Internet users consume news in
multiple languages, the majority of news recommendation focuses on major,
resource-rich languages, and English in particular. Moreover, nearly all news
recommendation efforts assume monolingual news consumption, whereas more and
more users tend to consume information in at least two languages. Accordingly,
the existing body of work on news recommendation suffers from a lack of
publicly available multilingual benchmarks that would catalyze development of
news recommenders effective in multilingual settings and for low-resource
languages. Aiming to fill this gap, we introduce xMIND, an open, multilingual
news recommendation dataset derived from the English MIND dataset using machine
translation, covering a set of 14 linguistically and geographically diverse
languages, with digital footprints of varying sizes. Using xMIND, we
systematically benchmark several state-of-the-art content-based neural news
recommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT)
cross-lingual transfer scenarios, considering both monolingual and bilingual
news consumption patterns. Our findings reveal that (i) current NNRs, even when
based on a multilingual language model, suffer from substantial performance
losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT
training has limited benefits, particularly when combined with a bilingual news
consumption. Our findings thus warrant a broader research effort in
multilingual and cross-lingual news recommendation. The xMIND dataset is
available at https://github.com/andreeaiana/xMIND.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 47th International ACM SIGIR Conference on Research
  and Development in Information Retrieval (SIGIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArabicaQA: A Comprehensive <span class="highlight-title">Dataset</span> for Arabic Question Answering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaseLink: Inductive Graph Learning for Legal Case Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanran Tang, Ruihong Qiu, Hongzhi Yin, Xue Li, Zi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In case law, the precedents are the relevant cases that are used to support
the decisions made by the judges and the opinions of lawyers towards a given
case. This relevance is referred to as the case-to-case reference relation. To
efficiently find relevant cases from a large case pool, retrieval tools are
widely used by legal practitioners. Existing legal case retrieval models mainly
work by comparing the text representations of individual cases. Although they
obtain a decent retrieval accuracy, the intrinsic case connectivity
relationships among cases have not been well exploited for case encoding,
therefore limiting the further improvement of retrieval performance. In a case
pool, there are three types of case connectivity relationships: the case
reference relationship, the case semantic relationship, and the case legal
charge relationship. Due to the inductive manner in the task of legal case
retrieval, using case reference as input is not applicable for testing. Thus,
in this paper, a CaseLink model based on inductive graph learning is proposed
to utilise the intrinsic case connectivity for legal case retrieval, a novel
Global Case Graph is incorporated to represent both the case semantic
relationship and the case legal charge relationship. A novel contrastive
objective with a regularisation on the degree of case nodes is proposed to
leverage the information carried by the case reference relationship to optimise
the model. Extensive experiments have been conducted on two benchmark datasets,
which demonstrate the state-of-the-art performance of CaseLink. The code has
been released on https://github.com/yanran-tang/CaseLink.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TWOLAR: a TWO-step LLM-Augmented distillation method for passage
  Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Baldelli, Junfeng Jiang, Akiko Aizawa, Paolo Torroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present TWOLAR: a two-stage pipeline for passage reranking
based on the distillation of knowledge from Large Language Models (LLM). TWOLAR
introduces a new scoring strategy and a distillation process consisting in the
creation of a novel and diverse training dataset. The dataset consists of 20K
queries, each associated with a set of documents retrieved via four distinct
retrieval methods to ensure diversity, and then reranked by exploiting the
zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate
the contribution of each new component we introduced. Our experimental results
show that TWOLAR significantly enhances the document reranking ability of the
underlying model, matching and in some cases even outperforming
state-of-the-art models with three orders of magnitude more parameters on the
TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate
future work we release our data set, finetuned models, and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuheng Fang, Kangfei Zhao, Yu Rong, Zhixun Li, Jeffrey Xu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cold-start rating prediction is a fundamental problem in recommender systems
that has been extensively studied. Many methods have been proposed that exploit
explicit relations among existing data, such as collaborative filtering, social
recommendations and heterogeneous information network, to alleviate the data
insufficiency issue for cold-start users and items. However, the explicit
relations constructed based on data between different roles may be unreliable
and irrelevant, which limits the performance ceiling of the specific
recommendation task. Motivated by this, in this paper, we propose a flexible
framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not
solely rely on the pre-defined interaction pattern or the manually constructed
heterogeneous information network. Instead, we devise a Heterogeneous
Interaction Module (HIM) to jointly model the heterogeneous interactions and
directly infer the important interactions via the observed data. In the
experiments, we evaluate our model under three cold-start settings on three
real-world datasets. The experimental results show that HIRE outperforms other
baselines by a large margin. Furthermore, we visualize the inferred
interactions of HIRE to confirm the contribution of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EulerFormer: Sequential User Behavior Modeling with Complex Vector
  Attention <span class="chip">SIGIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To capture user preference, transformer models have been widely applied to
model sequential user behavior data. The core of transformer architecture lies
in the self-attention mechanism, which computes the pairwise attention scores
in a sequence. Due to the permutation-equivariant nature, positional encoding
is used to enhance the attention between token representations. In this
setting, the pairwise attention scores can be derived by both semantic
difference and positional difference. However, prior studies often model the
two kinds of difference measurements in different ways, which potentially
limits the expressive capacity of sequence modeling. To address this issue,
this paper proposes a novel transformer variant with complex vector attention,
named EulerFormer, which provides a unified theoretical framework to formulate
both semantic difference and positional difference. The EulerFormer involves
two key technical improvements. First, it employs a new transformation function
for efficiently transforming the sequence tokens into polar-form complex
vectors using Euler's formula, enabling the unified modeling of both semantic
and positional information in a complex rotation form.Secondly, it develops a
differential rotation mechanism, where the semantic rotation angles can be
controlled by an adaptation function, enabling the adaptive integration of the
semantic and positional information according to the semantic
contexts.Furthermore, a phase contrastive learning task is proposed to improve
the anisotropy of contextual representations in EulerFormer. Our theoretical
framework possesses a high degree of completeness and generality. It is more
robust to semantic variations and possesses moresuperior theoretical properties
in principle. Extensive experiments conducted on four public datasets
demonstrate the effectiveness and efficiency of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in SIGIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Enhanced Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have attracted
considerable interest among researchers to leverage these models to enhance
Recommender Systems (RSs). Existing work predominantly utilizes LLMs to
generate knowledge-rich texts or utilizes LLM-derived embeddings as features to
improve RSs. Al- though the extensive world knowledge embedded in LLMs
generally benefits RSs, the application can only take limited number of users
and items as inputs, without adequately exploiting collaborative filtering
information. Considering its crucial role in RSs, one key challenge in
enhancing RSs with LLMs lies in providing better collaborative filtering
information through LLMs. In this paper, drawing inspiration from the
in-context learning and chain of thought reasoning in LLMs, we propose the
Large Language Models enhanced Collaborative Filtering (LLM-CF) framework,
which distils the world knowledge and reasoning capabilities of LLMs into
collaborative filtering. We also explored a concise and efficient
instruction-tuning method, which improves the recommendation capabilities of
LLMs while preserving their general functionalities (e.g., not decreasing on
the LLM benchmark). Comprehensive experiments on three real-world datasets
demonstrate that LLM-CF significantly enhances several backbone recommendation
models and consistently outperforms competitive baselines, showcasing its
effectiveness in distilling the world knowledge and reasoning capabilities of
LLM into collaborative filtering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S+t-SNE - Bringing dimensionality reduction to data streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro C. Vieira, João P. Montrezol, João T. Vieira, João Gama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle
infinite data streams. The core idea behind S+t-SNE is to update the t-SNE
embedding incrementally as new data arrives, ensuring scalability and
adaptability to handle streaming scenarios. By selecting the most important
points at each step, the algorithm ensures scalability while keeping
informative visualisations. Employing a blind method for drift management
adjusts the embedding space, facilitating continuous visualisation of evolving
data dynamics. Our experimental evaluations demonstrate the effectiveness and
efficiency of S+t-SNE. The results highlight its ability to capture patterns in
a streaming scenario. We hope our approach offers researchers and practitioners
a real-time tool for understanding and interpreting high-dimensional data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. We will soon add a link to the final version of
  this contribution that underwent peer-review and post-acceptance improvements
  and was presented at IDA2024 (https://ida2024.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retentive Decision <span class="highlight-title">Transformer</span> with Adaptive Masking for Reinforcement
  Learning based Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Wang, Xiaocong Chen, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise
across a spectrum of applications, from e-commerce platforms to streaming
services. Yet, they grapple with challenges, notably in crafting reward
functions and harnessing large pre-existing datasets within the RL framework.
Recent advancements in offline RLRS provide a solution for how to address these
two challenges. However, existing methods mainly rely on the transformer
architecture, which, as sequence lengths increase, can introduce challenges
associated with computational resources and training costs. Additionally, the
prevalent methods employ fixed-length input trajectories, restricting their
capacity to capture evolving user preferences. In this study, we introduce a
new offline RLRS method to deal with the above problems. We reinterpret the
RLRS challenge by modeling sequential decision-making as an inference task,
leveraging adaptive masking configurations. This adaptive approach selectively
masks input tokens, transforming the recommendation task into an inference
challenge based on varying token subsets, thereby enhancing the agent's ability
to infer across diverse trajectory lengths. Furthermore, we incorporate a
multi-scale segmented retention mechanism that facilitates efficient modeling
of long sequences, significantly enhancing computational efficiency. Our
experimental analysis, conducted on both online simulator and offline datasets,
clearly demonstrates the advantages of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, users frequently engage in multiple types of
behaviors, such as clicking, adding to a cart, and purchasing. However, with
diversified behavior data, user behavior sequences will become very long in the
short term, which brings challenges to the efficiency of the sequence
recommendation model. Meanwhile, some behavior data will also bring inevitable
noise to the modeling of user interests. To address the aforementioned issues,
firstly, we develop the Efficient Behavior Sequence Miner (EBM) that
efficiently captures intricate patterns in user behavior while maintaining low
time complexity and parameter count. Secondly, we design hard and soft
denoising modules for different noise types and fully explore the relationship
between behaviors and noise. Finally, we introduce a contrastive loss function
along with a guided training strategy to compare the valid information in the
data with the noisy signal, and seamlessly integrate the two denoising
processes to achieve a high degree of decoupling of the noisy signal.
Sufficient experiments on real-world datasets demonstrate the effectiveness and
efficiency of our approach in dealing with multi-behavior sequential
recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document Set Expansion with Positive-Unlabelled Learning Using
  Intractable Density Estimation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Zhang, Qiuyi Chen, Yuanjie Zou, Yushan Pan, Jia Wang, Mark Stevenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Document Set Expansion (DSE) task involves identifying relevant documents
from large collections based on a limited set of example documents. Previous
research has highlighted Positive and Unlabeled (PU) learning as a promising
approach for this task. However, most PU methods rely on the unrealistic
assumption of knowing the class prior for positive samples in the collection.
To address this limitation, this paper introduces a novel PU learning framework
that utilizes intractable density estimation models. Experiments conducted on
PubMed and Covid datasets in a transductive setting showcase the effectiveness
of the proposed method for DSE. Code is available from
https://github.com/Beautifuldog01/Document-set-expansion-puDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024. arXiv admin note: text overlap with
  arXiv:2401.11145</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Touch the Core: Exploring Task Dependence Among Hybrid Targets for
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Tang, Yang Qiao, Fuyuan Lyu, Dugang Liu, Xiuqiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As user behaviors become complicated on business platforms, online
recommendations focus more on how to touch the core conversions, which are
highly related to the interests of platforms. These core conversions are
usually continuous targets, such as \textit{watch time}, \textit{revenue}, and
so on, whose predictions can be enhanced by previous discrete conversion
actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to
learn these hybrid targets. However, existing works mainly emphasize
investigating the sequential dependence among discrete conversion actions,
which neglects the complexity of dependence between discrete conversions and
the final continuous conversion. Moreover, simultaneously optimizing hybrid
tasks with stronger task dependence will suffer from volatile issues where the
core regression task might have a larger influence on other tasks. In this
paper, we study the MTL problem with hybrid targets for the first time and
propose the model named Hybrid Targets Learning Network (HTLNet) to explore
task dependence and enhance optimization. Specifically, we introduce label
embedding for each task to explicitly transfer the label information among
these tasks, which can effectively explore logical task dependence. We also
further design the gradient adjustment regime between the final regression task
and other classification tasks to enhance the optimization. Extensive
experiments on two offline public datasets and one real-world industrial
dataset are conducted to validate the effectiveness of HTLNet. Moreover, online
A/B tests on the financial recommender system also show our model has superior
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion
  Rate Prediction with a Single Model <span class="chip">CIKM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ouyang, Xiuwu Zhang, Chaofeng Guo, Shukui Ren, Yupei Sui, Kun Zhang, Jinmei Luo, Yunfeng Chen, Dongbo Xu, Xiangzheng Liu, Yanlong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world advertising systems, conversions have different types in nature
and ads can be shown in different display scenarios, both of which highly
impact the actual conversion rate (CVR). This results in the multi-type and
multi-scenario CVR prediction problem. A desired model for this problem should
satisfy the following requirements: 1) Accuracy: the model should achieve
fine-grained accuracy with respect to any conversion type in any display
scenario. 2) Scalability: the model parameter size should be affordable. 3)
Convenience: the model should not require a large amount of effort in data
partitioning, subset processing and separate storage. Existing approaches
cannot simultaneously satisfy these requirements. For example, building a
separate model for each (conversion type, display scenario) pair is neither
scalable nor convenient. Building a unified model trained on all the data with
conversion type and display scenario included as two features is not accurate
enough. In this paper, we propose the Masked Multi-domain Network (MMN) to
solve this problem. To achieve the accuracy requirement, we model
domain-specific parameters and propose a dynamically weighted loss to account
for the loss scale imbalance issue within each mini-batch. To achieve the
scalability requirement, we propose a parameter sharing and composition
strategy to reduce model parameters from a product space to a sum space. To
achieve the convenience requirement, we propose an auto-masking strategy which
can take mixed data from all the domains as input. It avoids the overhead
caused by data partitioning, individual processing and separate storage. Both
offline and online experimental results validate the superiority of MMN for
multi-type and multi-scenario CVR prediction. MMN is now the serving model for
real-time CVR prediction in UC Toutiao.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2023 (larger figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MA4DIV: Multi-Agent Reinforcement Learning for Search Result
  Diversification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong MA, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of search result diversification (SRD) is to ensure that
selected documents cover as many different subtopics as possible. Existing
methods primarily utilize a paradigm of "greedy selection", i.e., selecting one
document with the highest diversity score at a time. These approaches tend to
be inefficient and are easily trapped in a suboptimal state. In addition, some
other methods aim to approximately optimize the diversity metric, such as
$\alpha$-NDCG, but the results still remain suboptimal. To address these
challenges, we introduce Multi-Agent reinforcement learning (MARL) for search
result DIVersity, which called MA4DIV. In this approach, each document is an
agent and the search result diversification is modeled as a cooperative task
among multiple agents. This approach allows for directly optimizing the
diversity metrics, such as $\alpha$-NDCG, while achieving high training
efficiency. We conducted preliminary experiments on public TREC datasets to
demonstrate the effectiveness and potential of MA4DIV. Considering the limited
number of queries in public TREC datasets, we construct a large-scale dataset
from industry sources and show that MA4DIV achieves substantial improvements in
both effectiveness and efficiency than existing baselines on a industrial scale
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering
  for Recommendations <span class="chip">SIGIR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Chao Wang, Dazhong Shen, Chuan Qin, Liyi Chen, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering methods based on graph neural networks (GNNs) have
witnessed significant success in recommender systems (RS), capitalizing on
their ability to capture collaborative signals within intricate user-item
relationships via message-passing mechanisms. However, these GNN-based RS
inadvertently introduce excess linear correlation between user and item
embeddings, contradicting the goal of providing personalized recommendations.
While existing research predominantly ascribes this flaw to the over-smoothing
problem, this paper underscores the critical, often overlooked role of the
over-correlation issue in diminishing the effectiveness of GNN representations
and subsequent recommendation performance. Up to now, the over-correlation
issue remains unexplored in RS. Meanwhile, how to mitigate the impact of
over-correlation while preserving collaborative filtering signals is a
significant challenge. To this end, this paper aims to address the
aforementioned gap by undertaking a comprehensive study of the over-correlation
issue in graph collaborative filtering models. Firstly, we present empirical
evidence to demonstrate the widespread prevalence of over-correlation in these
models. Subsequently, we dive into a theoretical analysis which establishes a
pivotal connection between the over-correlation and over-smoothing issues.
Leveraging these insights, we introduce the Adaptive Feature De-correlation
Graph Collaborative Filtering (AFDGCF) framework, which dynamically applies
correlation penalties to the feature dimensions of the representation matrix,
effectively alleviating both over-correlation and over-smoothing issues. The
efficacy of the proposed framework is corroborated through extensive
experiments conducted with four representative graph collaborative filtering
models across four publicly available datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Domain Recommendation to Attract Users via Domain Preference
  Modeling <span class="chip">AAAI'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyuunjun Ju, SeongKu Kang, Dongha Lee, Junyoung Hwang, Sanghwan Jang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, web platforms have been operating various service domains
simultaneously. Targeting a platform that operates multiple service domains, we
introduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU),
which recommends items from multiple ``unseen'' domains with which each user
has not interacted yet, by using knowledge from the user's ``seen'' domains. In
this paper, we point out two challenges of MDRAU task. First, there are
numerous possible combinations of mappings from seen to unseen domains because
users have usually interacted with a different subset of service domains.
Second, a user might have different preferences for each of the target unseen
domains, which requires that recommendations reflect the user's preferences on
domains as well as items. To tackle these challenges, we propose DRIP framework
that models users' preferences at two levels (i.e., domain and item) and learns
various seen-unseen domain mappings in a unified way with masked domain
modeling. Our extensive experiments demonstrate the effectiveness of DRIP in
MDRAU task and its ability to capture users' domain-level preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Training ID-Agnostic Multi-modal Sequential
  Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youhua Li, Hanwen Du, Yongxin Ni, Yuanqi He, Junchen Fu, Xiangyan Liu, Qi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) aims to predict future user-item interactions
based on historical interactions. While many SR approaches concentrate on user
IDs and item IDs, the human perception of the world through multi-modal
signals, like text and images, has inspired researchers to delve into
constructing SR from multi-modal information without using IDs. However, the
complexity of multi-modal learning manifests in diverse feature extractors,
fusion methods, and pre-trained models. Consequently, designing a simple and
universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential
\textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable
challenge. We systematically summarize the existing multi-modal related SR
methods and distill the essence into four core components: visual encoder, text
encoder, multimodal fusion module, and sequential architecture. Along these
dimensions, we dissect the model designs, and answer the following
sub-questions: First, we explore how to construct MMSR from scratch, ensuring
its performance either on par with or exceeds existing SR methods without
complex techniques. Second, we examine if MMSR can benefit from existing
multi-modal pre-training paradigms. Third, we assess MMSR's capability in
tackling common challenges like cold start and domain transferring. Our
experiment results across four real-world recommendation scenarios demonstrate
the great potential ID-agnostic multi-modal sequential recommendation. Our
framework can be found at: https://github.com/MMSR23/MMSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An Empirical Study of Training ID-Agnostic Multi-modal Sequential
  Recommenders</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cognitively Biased Users Interacting with Algorithmically Biased Results
  in Whole-Session Search on Controversial Topics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Wang, Jiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When interacting with information retrieval (IR) systems, users, affected by
confirmation biases, tend to select search results that confirm their existing
beliefs on socially significant contentious issues. To understand the judgments
and attitude changes of users searching online, our study examined how
cognitively biased users interact with algorithmically biased search engine
result pages (SERPs). We designed three-query search sessions on debated topics
under various bias conditions. We recruited 1,321 crowdsourcing participants
and explored their attitude changes, search interactions, and the effects of
confirmation bias. Three key findings emerged: 1) most attitude changes occur
in the initial query of a search session; 2) confirmation bias and result
presentation on SERPs affect search behaviors in the current query and
perceived familiarity with clicked results in subsequent queries. The bias
position also affect attitude changes of users with lower perceived openness to
conflicting opinions; 3) Interactions in the first query and and dwell time
throughout the session are associated with users' attitude changes in different
forms. Our study goes beyond traditional simulation-based evaluation settings
and simulated rational users, sheds light on the mixed effects of human biases
and algorithmic biases in controversial information retrieval tasks, and can
inform the design of bias-aware user models, human-centered bias mitigation
techniques, and socially responsible intelligent IR systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Pre-train</span>ed Language Model Sensitivity via Mask Specific
  losses: A case study on Biomedical NER <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micheal Abaho, Danushka Bollegala, Gary Leeming, Dan Joyce, Iain E Buchan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting language models (LMs) to novel domains is often achieved through
fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning
introduces new knowledge into an LM, enabling it to comprehend and efficiently
perform a target domain task. Fine-tuning can however be inadvertently
insensitive if it ignores the wide array of disparities (e.g in word meaning)
between source and target domains. For instance, words such as chronic and
pressure may be treated lightly in social conversations, however, clinically,
these words are usually an expression of concern. To address insensitive
fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach
that efficiently acquires target domain knowledge by appropriately weighting
the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM
jointly masks DS-terms and generic words, then learns mask-specific losses by
ensuring LMs incur larger penalties for inaccurately predicting DS-terms
compared to generic words. Results of our analysis show that MSLM improves LMs
sensitivity and detection of DS-terms. We empirically show that an optimal
masking rate not only depends on the LM, but also on the dataset and the length
of sequences. Our proposed masking strategy outperforms advanced masking
strategies such as span- and PMI-based masking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper alrerady accepted for publishing by the NAACL 2024 conference
  (main conference paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Enhanced Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have attracted
considerable interest among researchers to leverage these models to enhance
Recommender Systems (RSs). Existing work predominantly utilizes LLMs to
generate knowledge-rich texts or utilizes LLM-derived embeddings as features to
improve RSs. Although the extensive world knowledge embedded in LLMs generally
benefits RSs, the application can only take limited number of users and items
as inputs, without adequately exploiting collaborative filtering information.
Considering its crucial role in RSs, one key challenge in enhancing RSs with
LLMs lies in providing better collaborative filtering information through LLMs.
In this paper, drawing inspiration from the in-context learning and chain of
thought reasoning in LLMs, we propose the Large Language Models enhanced
Collaborative Filtering (LLM-CF) framework, which distils the world knowledge
and reasoning capabilities of LLMs into collaborative filtering. We also
explored a concise and efficient instruction-tuning method, which improves the
recommendation capabilities of LLMs while preserving their general
functionalities (e.g., not decreasing on the LLM benchmark). Comprehensive
experiments on three real-world datasets demonstrate that LLM-CF significantly
enhances several backbone recommendation models and consistently outperforms
competitive baselines, showcasing its effectiveness in distilling the world
knowledge and reasoning capabilities of LLM into collaborative filtering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S+t-SNE -- Bringing dimensionality reduction to data streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro C. Vieira, João P. Montrezol, João T. Vieira, João Gama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle
infinite data streams. The core idea behind S+t-SNE is to update the t-SNE
embedding incrementally as new data arrives, ensuring scalability and
adaptability to handle streaming scenarios. By selecting the most important
points at each step, the algorithm ensures scalability while keeping
informative visualisations. Employing a blind method for drift management
adjusts the embedding space, facilitating continuous visualisation of evolving
data dynamics. Our experimental evaluations demonstrate the effectiveness and
efficiency of S+t-SNE. The results highlight its ability to capture patterns in
a streaming scenario. We hope our approach offers researchers and practitioners
a real-time tool for understanding and interpreting high-dimensional data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. We will soon add a link to the final version of
  this contribution that underwent peer-review and post-acceptance improvements
  and was presented at IDA2024 (https://ida2024.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Decade of Scholarly Research on Open Knowledge Graphs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houcemeddine Turki, Abraham Toluwase Owodunni, Mohamed Ali Hadj Taieb, René Fabrice Bile, Mohamed Ben Aouicha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of open knowledge graphs has led to a surge in scholarly
research on the topic over the past decade. This paper presents a bibliometric
analysis of the scholarly literature on open knowledge graphs published between
2013 and 2023. The study aims to identify the trends, patterns, and impact of
research in this field, as well as the key topics and research questions that
have emerged. The work uses bibliometric techniques to analyze a sample of 4445
scholarly articles retrieved from Scopus. The findings reveal an
ever-increasing number of publications on open knowledge graphs published every
year, particularly in developed countries (+50 per year). These outputs are
published in highly-referred scholarly journals and conferences. The study
identifies three main research themes: (1) knowledge graph construction and
enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into
NLP systems. Within these themes, the study identifies specific tasks that have
received considerable attention, including entity linking, knowledge graph
embedding, and graph neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready edition for LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Large Language Model Alignment for Information Retrieval
  via Contrastive Feedback <span class="chip">SIGIR24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu, Shuaiqiang Wang, Dawei Yin, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities across
various research domains, including the field of Information Retrieval (IR).
However, the responses generated by off-the-shelf LLMs tend to be generic,
i.e., cannot capture the distinctiveness of each document with similar content.
This limits the performance of LLMs in IR because finding and distinguishing
relevant documents from substantial similar documents is a typical problem in
many IR tasks. To address this issue, we propose an unsupervised alignment
method, namely Reinforcement Learning from Contrastive Feedback (RLCF),
empowering LLMs to generate both high-quality and context-specific responses.
Our approach constructs unsupervised contrastive feedback signals based on
similar document groups, and adopts a reward function, named group-wise
reciprocal rank, to optimize LLMs within a standard Proximal Policy
Optimization. We conduct extensive experiments to evaluate the effectiveness of
RLCF on LLMs built with different languages and parameter sizes on multiple
downstream IR applications. RLCF significantly outperforms existing alignment
methods, and RLCF-optimized LLMs demonstrate considerable improvement in
generating responses with distinctiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Feature Set for Click-Through Rate Prediction <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyuan Lyu, Xing Tang, Dugang Liu, Liang Chen, Xiuqiang He, Xue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through prediction (CTR) models transform features into latent vectors
and enumerate possible feature interactions to improve performance based on the
input feature set. Therefore, when selecting an optimal feature set, we should
consider the influence of both feature and its interaction. However, most
previous works focus on either feature field selection or only select feature
interaction based on the fixed feature set to produce the feature set. The
former restricts search space to the feature field, which is too coarse to
determine subtle features. They also do not filter useless feature
interactions, leading to higher computation costs and degraded model
performance. The latter identifies useful feature interaction from all
available features, resulting in many redundant features in the feature set. In
this paper, we propose a novel method named OptFS to address these problems. To
unify the selection of feature and its interaction, we decompose the selection
of each feature interaction into the selection of two correlated features. Such
a decomposition makes the model end-to-end trainable given various feature
interaction operations. By adopting feature-level search space, we set a
learnable gate to determine whether each feature should be within the feature
set. Because of the large-scale search space, we develop a
learning-by-continuation training scheme to learn such gates. Hence, OptFS
generates the feature set only containing features which improve the final
prediction results. Experimentally, we evaluate OptFS on three public datasets,
demonstrating OptFS can optimize feature sets which enhance the model
performance and further reduce both the storage and computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2023 Research Tracks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Signal Diffusion Model for Collaborative Filtering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqin Zhu, Chao Wang, Qi Zhang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is a critical technique in recommender systems. Among
various methods, an increasingly popular paradigm is to reconstruct user-item
interactions based on the historical observations. This can be viewed as a
conditional generative task, where recently developed diffusion model
demonstrates great potential. However, existing studies on diffusion models
lack effective solutions for modeling implicit feedback data. Particularly, the
isotropic nature of the standard diffusion process fails to account for the
heterogeneous dependencies among items, leading to a misalignment with the
graphical structure of the interaction space. Meanwhile, random noise
destroying personalized information in interaction vectors, causing difficulty
in reverse reconstruction. In this paper, we make novel adaptions of diffusion
model and propose Graph Signal Diffusion Model for Collaborative Filtering
(named GiffCF). To better represent the high-dimensional and sparse
distribution of implicit feedback, we define a generalized form of denoising
diffusion using heat equation on the item-item similarity graph. Our forward
process smooths interaction signals with an advanced family of graph filters.
Hence, instead of losing information, it involves item-item similarities as
beneficial prior knowledge for recommendation. To reconstruct high-quality
interactions, our reverse process iteratively refines and sharpens preference
signals in a deterministic manner, where the update direction is conditioned on
the user history and computed from a carefully designed two-stage denoiser.
Finally, through extensive experiments, we show that GiffCF effectively
leverages the advantages of both diffusion model and graph signal processing,
and achieves state-of-the-art performance on three benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Performance of Long-Document Ranking Models through
  Comprehensive Evaluation and Leaderboarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Boytsov, David Akinpelu, Tianyi Lin, Fangwei Gao, Yutian Zhao, Jeffrey Huang, Eric Nyberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluated 20+ Transformer models for ranking of long documents (including
recent LongP models trained with FlashAttention) and compared them with simple
FirstP baselines (applying the same model to input truncated to the first 512
tokens). We used MS MARCO Documents v1 as a primary training set and evaluated
models in the zero-shot scenario as well as after fine-tuning on other
collections.
  In our initial experiments with standard collections we found that
long-document models underperformed FirstP or outperformed it by at most 5% on
average in terms of MRR or NDCG. We then conjectured that this was not due to
models inability to process long context but rather due to a positional bias of
relevant passages, which tended to be among the first 512 document tokens. We
found evidence that this bias was, indeed, present in at least two test sets,
which motivated us to create a new collection MS MARCO FarRelevant where the
relevant passages were not present among the first 512 tokens.
  Unlike standard collections where we observed both little benefit from
incorporating longer contexts and limited variability in model performance
(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic
differences among models. FirstP models performed roughly at the
random-baseline level in both zero-shot and fine-tuning scenarios. Simple
aggregation models (e.g., MaxP) had good zero-shot accuracy but benefited
little from fine-tuning. Most other models had poor zero-shot performance
(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\%
after finetuning. Thus, positional bias not only diminishes benefits of
processing longer document contexts but also leads to model overfitting to this
bias and performing poorly in a zero-shot setting when a distribution of
relevant passages changes substantially.
  We make our software and MS MARCO FarRelevant available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Take Care of Your <span class="highlight-title">Prompt</span> Bias! Investigating and Mitigating <span class="highlight-title">Prompt</span> Bias
  in Factual Knowledge Extraction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that pre-trained language models (PLMs) suffer from
"prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce
biases toward specific labels. Prompt bias presents a significant challenge in
assessing the factual knowledge within PLMs. Therefore, this paper aims to
improve the reliability of existing benchmarks by thoroughly investigating and
mitigating prompt bias. We show that: 1) all prompts in the experiments exhibit
non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt
displaying significantly higher levels of bias; 2) prompt bias can amplify
benchmark accuracy unreasonably by overfitting the test datasets, especially on
imbalanced datasets like LAMA. Based on these findings, we propose a
representation-based approach to mitigate the prompt bias during inference
time. Specifically, we first estimate the biased representation using
prompt-only querying, and then remove it from the model's internal
representations to generate the debiased representations, which are used to
produce the final debiased outputs. Experiments across various prompts, PLMs,
and benchmarks show that our approach can not only correct the overfitted
performance caused by prompt bias, but also significantly improve the prompt
retrieval capability (up to 10% absolute performance gain). These results
indicate that our approach effectively alleviates prompt bias in knowledge
evaluation, thereby enhancing the reliability of benchmark assessments.
Hopefully, our plug-and-play approach can be a golden standard to strengthen
PLMs toward reliable knowledge bases. Code and data are released in
https://github.com/FelliYang/PromptBias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Sequential Recommendations via Bidirectional Temporal Data
  Augmentation with <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06460v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06460v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyong Jiang, Peiyan Zhang, Yingtao Luo, Chaozhuo Li, Jaeboum Kim, Kai Zhang, Senzhang Wang, Sunghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems are integral to discerning temporal user
preferences. Yet, the task of learning from abbreviated user interaction
sequences poses a notable challenge. Data augmentation has been identified as a
potent strategy to enhance the informational richness of these sequences.
Traditional augmentation techniques, such as item randomization, may disrupt
the inherent temporal dynamics. Although recent advancements in reverse
chronological pseudo-item generation have shown promise, they can introduce
temporal discrepancies when assessed in a natural chronological context. In
response, we introduce a sophisticated approach, Bidirectional temporal data
Augmentation with pre-training (BARec). Our approach leverages bidirectional
temporal augmentation and knowledge-enhanced fine-tuning to synthesize
authentic pseudo-prior items that \emph{retain user preferences and capture
deeper item semantic correlations}, thus boosting the model's expressive power.
Our comprehensive experimental analysis confirms the superiority of BARec
across both short and elongated sequence contexts. Moreover, theoretical
examination and visual representation of item embeddings offer further insight
into the model's logical processes and interpretability. The source code for
our study is available at
\textcolor{blue}{\href{https://github.com/juyongjiang/BARec}{https://github.com/juyongjiang/BARec}}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Need for Speed: Pruning <span class="highlight-title">Transformer</span>s with One Recipe <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Khaki, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique
for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework
as a tool to increase the efficiency of pre-trained transformer architectures
$\textit{without requiring re-training}$. Recent works have explored improving
transformer efficiency, however often incur computationally expensive
re-training procedures or depend on architecture-specific characteristics, thus
impeding practical wide-scale adoption. To address these shortcomings, the
OPTIN framework leverages intermediate feature distillation, capturing the
long-range dependencies of model parameters (coined $\textit{trajectory}$), to
produce state-of-the-art results on natural language, image classification,
transfer learning, and semantic segmentation tasks $\textit{without
re-training}$. Given a FLOP constraint, the OPTIN framework will compress the
network while maintaining competitive accuracy performance and improved
throughput. Particularly, we show a $\leq 2$% accuracy degradation from NLP
baselines and a $0.5$% improvement from state-of-the-art methods on image
classification at competitive FLOPs reductions. We further demonstrate the
generalization of tasks and architecture with comparative performance using
Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents
one of the first one-shot efficient frameworks for compressing transformer
architectures that generalizes well across different class domains, in
particular: natural language and image-related tasks, without
$\textit{re-training}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the International Conference on Learning Representations
  (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yiwei, Tang Chao, Aghabiglou Amir, Chu Chung San, Wiaux Yves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serpent: Scalable and Efficient Image Restoration via Multi-scale
  Structured State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shahab Sepehri, Zalan Fabian, Mahdi Soltanolkotabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of computational building blocks of efficient image restoration
architectures is dominated by a combination of convolutional processing and
various attention mechanisms. However, convolutional filters are inherently
local and therefore struggle at modeling long-range dependencies in images. On
the other hand, attention excels at capturing global interactions between
arbitrary image regions, however at a quadratic cost in image dimension. In
this work, we propose Serpent, an architecture that leverages recent advances
in state space models (SSMs) in its core computational block. SSMs, originally
introduced for sequence modeling, can maintain a global receptive field with a
favorable linear scaling in input size. Our preliminary results demonstrate
that Serpent can achieve reconstruction quality on par with state-of-the-art
techniques, while requiring orders of magnitude less compute (up to $150$ fold
reduction in FLOPS) and a factor of up to $5\times$ less GPU memory while
maintaining a compact model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, preliminary workshop submission of a
  comprehensive work to be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-based Novel Fault Detection with Deep Learning Classifiers using
  Hierarchical Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurettin Sergin, Jiayu Huang, Tzyy-Shuh Chang, Hao Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One important characteristic of modern fault classification systems is the
ability to flag the system when faced with previously unseen fault types. This
work considers the unknown fault detection capabilities of deep neural
network-based fault classifiers. Specifically, we propose a methodology on how,
when available, labels regarding the fault taxonomy can be used to increase
unknown fault detection performance without sacrificing model performance. To
achieve this, we propose to utilize soft label techniques to improve the
state-of-the-art deep novel fault detection techniques during the training
process and novel hierarchically consistent detection statistics for online
novel fault detection. Finally, we demonstrated increased detection performance
on novel fault detection in inspection images from the hot steel rolling
process, with results well replicated across multiple scenarios and baseline
detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IISE Transaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large scale paired antibody language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Kenlay, Frédéric A. Dreyer, Aleksandr Kovaltsuk, Dom Miketa, Douglas Pires, Charlotte M. Deane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibodies are proteins produced by the immune system that can identify and
neutralise a wide variety of antigens with high specificity and affinity, and
constitute the most successful class of biotherapeutics. With the advent of
next-generation sequencing, billions of antibody sequences have been collected
in recent years, though their application in the design of better therapeutics
has been constrained by the sheer volume and complexity of the data. To address
this challenge, we present IgBert and IgT5, the best performing
antibody-specific language models developed to date which can consistently
handle both paired and unpaired variable region sequences as input. These
models are trained comprehensively using the more than two billion unpaired
sequences and two million paired sequences of light and heavy chains present in
the Observed Antibody Space dataset. We show that our models outperform
existing antibody and protein language models on a diverse range of design and
regression tasks relevant to antibody engineering. This advancement marks a
significant leap forward in leveraging machine learning, large scale data sets
and high-performance computing for enhancing antibody design for therapeutic
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, 6 tables, model weights available at
  https://zenodo.org/doi/10.5281/zenodo.10876908</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressed Multi-task embeddings for Data-Efficient Downstream training
  and inference in Earth Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Gomes, Thomas Brunschwiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As repositories of large scale data in earth observation (EO) have grown, so
have transfer and storage costs for model training and inference, expending
significant resources. We introduce Neural Embedding Compression (NEC), based
on the transfer of compressed embeddings to data consumers instead of raw data.
We adapt foundation models (FM) through learned neural compression to generate
multi-task embeddings while navigating the tradeoff between compression rate
and embedding utility. We update only a small fraction of the FM parameters
(10%) for a short training period (1% of the iterations of pre-training). We
evaluate NEC on two EO tasks: scene classification and semantic segmentation.
Compared with applying traditional compression to the raw data, NEC achieves
similar accuracy with a 75% to 90% reduction in data. Even at 99.7%
compression, performance drops by only 5% on the scene classification task.
Overall, NEC is a data-efficient yet performant approach for multi-task EO
modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IGARSS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Data Mesh with Federated Learning <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of ACM Knowledge Discovery and Data Mining, Barcelona,
  Spain, 25th - 29th August, 2024 (Conference acronym KDD), 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample complexity of quantum hypothesis testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Chung Cheng, Nilanjana Datta, Nana Liu, Theshani Nuradha, Robert Salzmann, Mark M. Wilde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum hypothesis testing has been traditionally studied from the
information-theoretic perspective, wherein one is interested in the optimal
decay rate of error probabilities as a function of the number of samples of an
unknown state. In this paper, we study the sample complexity of quantum
hypothesis testing, wherein the goal is to determine the minimum number of
samples needed to reach a desired error probability. By making use of the
wealth of knowledge that already exists in the literature on quantum hypothesis
testing, we characterize the sample complexity of binary quantum hypothesis
testing in the symmetric and asymmetric settings, and we provide bounds on the
sample complexity of multiple quantum hypothesis testing. In more detail, we
prove that the sample complexity of symmetric binary quantum hypothesis testing
depends logarithmically on the inverse error probability and inversely on the
negative logarithm of the fidelity. As a counterpart of the quantum Stein's
lemma, we also find that the sample complexity of asymmetric binary quantum
hypothesis testing depends logarithmically on the inverse type~II error
probability and inversely on the quantum relative entropy. Finally, we provide
lower and upper bounds on the sample complexity of multiple quantum hypothesis
testing, with it remaining an intriguing open question to improve these bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 1 figure, preliminary version; see independent and
  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Fairness through Transforming Data Orthogonal to Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Chen, Shixiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have shown exceptional prowess in solving complex
issues across various domains. Nonetheless, these models can sometimes exhibit
biased decision-making, leading to disparities in treatment across different
groups. Despite the extensive research on fairness, the nuanced effects of
multivariate and continuous sensitive variables on decision-making outcomes
remain insufficiently studied. We introduce a novel data pre-processing
algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group
of continuous sensitive variables, thereby facilitating counterfactual fairness
in machine learning applications. Our approach is grounded in the assumption of
a jointly normal distribution within a structural causal model (SCM), proving
that counterfactual fairness can be achieved by ensuring the data is
uncorrelated with sensitive variables. The OB algorithm is model-agnostic,
catering to a wide array of machine learning models and tasks, and includes a
sparse variant to enhance numerical stability through regularization. Through
empirical evaluation on simulated and real-world datasets - including the adult
income and the COMPAS recidivism datasets - our methodology demonstrates its
capacity to enable fairer outcomes without compromising accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Climate Downscaling: A Deep-Learning Based Super-resolution Model of
  Precipitation Data with Attention Block and Skip Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Chiang, Zheng-Han Huang, Liwen Liu, Hsin-Chien Liang, Yi-Chi Wang, Wan-Ling Tseng, Chao Wang, Che-Ta Chen, Ko-Chih Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activities accelerate consumption of fossil fuels and produce
greenhouse gases, resulting in urgent issues today: global warming and the
climate change. These indirectly cause severe natural disasters, plenty of
lives suffering and huge losses of agricultural properties. To mitigate impacts
on our lands, scientists are developing renewable, reusable, and clean energies
and climatologists are trying to predict the extremes. Meanwhile, governments
are publicizing resource-saving policies for a more eco-friendly society and
arousing environment awareness. One of the most influencing factors is the
precipitation, bringing condensed water vapor onto lands. Water resources are
the most significant but basic needs in society, not only supporting our
livings, but also economics. In Taiwan, although the average annual
precipitation is up to 2,500 millimeter (mm), the water allocation for each
person is lower than the global average due to drastically geographical
elevation changes and uneven distribution through the year. Thus, it is crucial
to track and predict the rainfall to make the most use of it and to prevent the
floods. However, climate models have limited resolution and require intensive
computational power for local-scale use. Therefore, we proposed a deep
convolutional neural network with skip connections, attention blocks, and
auxiliary data concatenation, in order to downscale the low-resolution
precipitation data into high-resolution one. Eventually, we compare with other
climate downscaling methods and show better performance in metrics of Mean
Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,
structural similarity index (SSIM), and forecast indicators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TractOracle: towards an anatomically-informed reward function for
  RL-based tractography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Théberge, Maxime Descoteaux, Pierre-Marc Jodoin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL)-based tractography is a competitive alternative
to machine learning and classical tractography algorithms due to its high
anatomical accuracy obtained without the need for any annotated data. However,
the reward functions so far used to train RL agents do not encapsulate
anatomical knowledge which causes agents to generate spurious false positives
tracts. In this paper, we propose a new RL tractography system, TractOracle,
which relies on a reward network trained for streamline classification. This
network is used both as a reward function during training as well as a mean for
stopping the tracking process early and thus reduce the number of false
positive streamlines. This makes our system a unique method that evaluates and
reconstructs WM streamlines at the same time. We report an improvement of true
positive ratios by almost 20\% and a reduction of 3x of false positive ratios
on one dataset and an increase between 2x and 7x in the number true positive
streamlines on another dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanistic Design and Scaling of Hybrid Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, Ce Zhang, Stefano Massaroli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of deep learning architectures is a resource-demanding
process, due to a vast design space, long prototyping times, and high compute
costs associated with at-scale model training and evaluation. We set out to
simplify this process by grounding it in an end-to-end mechanistic architecture
design (MAD) pipeline, encompassing small-scale capability unit tests
predictive of scaling laws. Through a suite of synthetic token manipulation
tasks such as compression and recall, designed to probe capabilities, we
identify and test new hybrid architectures constructed from a variety of
computational primitives. We experimentally validate the resulting
architectures via an extensive compute-optimal and a new state-optimal scaling
law analysis, training over 500 language models between 70M to 7B parameters.
Surprisingly, we find MAD synthetics to correlate with compute-optimal
perplexity, enabling accurate evaluation of new architectures via isolated
proxy tasks. The new architectures found via MAD, based on simple ideas such as
hybridization and sparsity, outperform state-of-the-art Transformer,
convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in
scaling, both at compute-optimal budgets and in overtrained regimes. Overall,
these results provide evidence that performance on curated synthetic tasks can
be predictive of scaling laws, and that an optimal architecture should leverage
specialized layers via a hybrid topology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of datasets, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed dataset, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed dataset and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The dataset, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPFL: A Gradient Projection-Based Client Selection Framework for
  Efficient Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Na, Yuzhi Liang, Siu-Ming Yiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning client selection is crucial for determining participant
clients while balancing model accuracy and communication efficiency. Existing
methods have limitations in handling data heterogeneity, computational burdens,
and independent client treatment. To address these challenges, we propose GPFL,
which measures client value by comparing local and global descent directions.
We also employ an Exploit-Explore mechanism to enhance performance.
Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL
outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in
FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times
through pre-selection and parameter reuse in federated learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Optimal Power Flow: Environment Design Matters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wolgast, Astrid Nieße
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)
emerges as a promising new approach. However, the RL-OPF literature is strongly
divided regarding the exact formulation of the OPF problem as an RL
environment. In this work, we collect and implement diverse environment design
decisions from the literature regarding training data, observation space,
episode definition, and reward function choice. In an experimental analysis, we
show the significant impact of these environment design options on RL-OPF
training performance. Further, we derive some first recommendations regarding
the choice of these design decisions. The created environment framework is
fully open-source and can serve as a benchmark for future research in the
RL-OPF field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from
  Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural hand-object interactions in 3D is challenging as the
resulting hand and object motions are expected to be physically plausible and
semantically meaningful. Furthermore, generalization to unseen objects is
hindered by the limited scale of available hand-object interaction datasets. We
propose DiffH2O, a novel method to synthesize realistic, one or two-handed
object interactions from provided text prompts and geometry of the object. The
method introduces three techniques that enable effective learning from limited
data. First, we decompose the task into a grasping stage and a text-based
interaction stage and use separate diffusion models for each. In the grasping
stage, the model only generates hand motions, whereas in the interaction phase
both hand and object poses are synthesized. Second, we propose a compact
representation that tightly couples hand and object poses. Third, we propose
two different guidance schemes to allow more control of the generated motions:
grasp guidance and detailed textual guidance. Grasp guidance takes a single
target grasping pose and guides the diffusion model to reach this grasp at the
end of the grasping stage, which provides control over the grasping pose. Given
a grasping motion from this stage, multiple different actions can be prompted
in the interaction phase. For textual guidance, we contribute comprehensive
text descriptions to the GRAB dataset and show that they enable our method to
have more fine-grained control over hand-object interactions. Our quantitative
and qualitative evaluation demonstrates that the proposed method outperforms
baseline methods and leads to natural hand-object motions. Moreover, we
demonstrate the practicality of our framework by utilizing a hand pose estimate
from an off-the-shelf pose estimator for guidance, and then sampling multiple
different actions in the interaction stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://diffh2o.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Compressed Language Models Less Subgroup Robust? <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Gee, Andrea Zugarini, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotated Biomedical Video Generation using Denoising Diffusion
  Probabilistic Models and Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rüveyda Yilmaz, Dennis Eschweiler, Johannes Stegmaier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation and tracking of living cells play a vital role within the
biomedical domain, particularly in cancer research, drug development, and
developmental biology. These are usually tedious and time-consuming tasks that
are traditionally done by biomedical experts. Recently, to automatize these
processes, deep learning based segmentation and tracking methods have been
proposed. These methods require large-scale datasets and their full potential
is constrained by the scarcity of annotated data in the biomedical imaging
domain. To address this limitation, we propose Biomedical Video Diffusion Model
(BVDM), capable of generating realistic-looking synthetic microscopy videos.
Trained only on a single real video, BVDM can generate videos of arbitrary
length with pixel-level annotations that can be used for training data-hungry
models. It is composed of a denoising diffusion probabilistic model (DDPM)
generating high-fidelity synthetic cell microscopy images and a flow prediction
model (FPM) predicting the non-rigid transformation between consecutive video
frames. During inference, initially, the DDPM imposes realistic cell textures
on synthetic cell masks which are generated based on real data statistics. The
flow prediction model predicts the flow field between consecutive masks and
applies that to the DDPM output from the previous time frame to create the next
one while keeping temporal consistency. BVDM outperforms state-of-the-art
synthetic live cell microscopy video generation models. Furthermore, we
demonstrate that a sufficiently large synthetic dataset enhances the
performance of cell segmentation and tracking models compared to using a
limited amount of available real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding
  Model Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hanna, Sandro Pezzelle, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language model (LM) interpretability studies have adopted the
circuits framework, which aims to find the minimal computational subgraph, or
circuit, that explains LM behavior on a given task. Most studies determine
which edges belong in a LM's circuit by performing causal interventions on each
edge independently, but this scales poorly with model size. Edge attribution
patching (EAP), gradient-based approximation to interventions, has emerged as a
scalable but imperfect solution to this problem. In this paper, we introduce a
new method - EAP with integrated gradients (EAP-IG) - that aims to better
maintain a core property of circuits: faithfulness. A circuit is faithful if
all model edges outside the circuit can be ablated without changing the model's
performance on the task; faithfulness is what justifies studying circuits,
rather than the full model. Our experiments demonstrate that circuits found
using EAP are less faithful than those found using EAP-IG, even though both
have high node overlap with circuits found previously using causal
interventions. We conclude more generally that when using circuits to compare
the mechanisms models use to solve tasks, faithfulness, not overlap, is what
should be measured.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Brunnbauer, Luigi Berducci, Peter Priller, Dejan Nickovic, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated generation of diverse and complex training scenarios has been
an important ingredient in many complex learning tasks. Especially in
real-world application domains, such as autonomous driving, auto-curriculum
generation is considered vital for obtaining robust and general policies.
However, crafting traffic scenarios with multiple, heterogeneous agents is
typically considered as a tedious and time-consuming task, especially in more
complex simulation environments. In our work, we introduce MATS-Gym, a
Multi-Agent Traffic Scenario framework to train agents in CARLA, a
high-fidelity driving simulator. MATS-Gym is a multi-agent training framework
for autonomous driving that uses partial scenario specifications to generate
traffic scenarios with variable numbers of agents. This paper unifies various
existing approaches to traffic scenario description into a single training
framework and demonstrates how it can be integrated with techniques from
unsupervised environment design to automate the generation of adaptive
auto-curricula. The code is available at
https://github.com/AutonomousDrivingExaminer/mats-gym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Aggregation is Not Private Against Membership Inference Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khac-Hoang Ngo, Johan Östman, Giuseppe Durisi, Alexandre Graell i Amat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in
federated learning, affording the server access only to the aggregate of model
updates while safeguarding the confidentiality of individual updates. Despite
widespread claims regarding SecAgg's privacy-preserving capabilities, a formal
analysis of its privacy is lacking, making such presumptions unjustified. In
this paper, we delve into the privacy implications of SecAgg by treating it as
a local differential privacy (LDP) mechanism for each local update. We design a
simple attack wherein an adversarial server seeks to discern which update
vector a client submitted, out of two possible ones, in a single training round
of federated learning under SecAgg. By conducting privacy auditing, we assess
the success probability of this attack and quantify the LDP guarantees provided
by SecAgg. Our numerical results unveil that, contrary to prevailing claims,
SecAgg offers weak privacy against membership inference attacks even in a
single training round. Indeed, it is difficult to hide a local update by adding
other independent local updates when the updates are of high dimension. Our
findings underscore the imperative for additional privacy-enhancing mechanisms,
such as noise injection, in federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNews: From Scholarly Complexities to Public Narratives -- A <span class="highlight-title">Dataset</span>
  for Scientific News Report Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 Main Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotic Bayes risk of semi-supervised learning with uncertain
  labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Leger, Romain Couillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article considers a semi-supervised classification setting on a Gaussian
mixture model, where the data is not labeled strictly as usual, but instead
with uncertain labels. Our main aim is to compute the Bayes risk for this
model. We compare the behavior of the Bayes risk and the best known algorithm
for this model. This comparison eventually gives new insights over the
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise2Noise Denoising of CRISM Hyperspectral Data <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Platt, Rossella Arcucci, Cédric John
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral data acquired by the Compact Reconnaissance Imaging
Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the
surface mineralogy of Mars. Due to sensor degradation over time, a significant
portion of the recently acquired data is considered unusable. Here a new
data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to
remove noise from CRISM images. Our model is self-supervised and does not
require zero-noise target data, making it well suited for use in Planetary
Science applications where high quality labelled data is scarce. We demonstrate
its strong performance on synthetic-noise data and CRISM images, and its impact
on downstream classification performance, outperforming benchmark methods on
most metrics. This allows for detailed analysis for critical sites of interest
on the Martian surface, including proposed lander sites.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024
  ML4RS Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream
  Enhanced Rectified <span class="highlight-title">Transformer</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Shao, Michael G. H. Bell, Ze Wang, D. Glenn Geers, Xusheng Yao, Junbin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate, and effective traffic forecasting is vital for smart traffic
systems, crucial in urban traffic planning and management. Current
Spatio-Temporal Transformer models, despite their prediction capabilities,
struggle with balancing computational efficiency and accuracy, favoring global
over local information, and handling spatial and temporal data separately,
limiting insight into complex interactions. We introduce the Criss-Crossed
Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes
three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA),
Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified
Temporal Self-attention (ReTSA). These modules aim to lower computational needs
via sparse attention, focus on local information for better traffic dynamics
understanding, and merge spatial and temporal insights through a unique
learning method. Extensive tests on six real-world datasets highlight
CCDSReFormer's superior performance. An ablation study also confirms the
significant impact of each component on the model's predictive accuracy,
showcasing our model's ability to forecast traffic flow effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leave No Patient Behind: Enhancing Medication Recommendation for Rare
  Disease Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Zhao, Yi Jing, Fuli Feng, Jiancan Wu, Chongming Gao, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medication recommendation systems have gained significant attention in
healthcare as a means of providing tailored and effective drug combinations
based on patients' clinical information. However, existing approaches often
suffer from fairness issues, as recommendations tend to be more accurate for
patients with common diseases compared to those with rare conditions. In this
paper, we propose a novel model called Robust and Accurate REcommendations for
Medication (RAREMed), which leverages the pretrain-finetune learning paradigm
to enhance accuracy for rare diseases. RAREMed employs a transformer encoder
with a unified input sequence approach to capture complex relationships among
disease and procedure codes. Additionally, it introduces two self-supervised
pre-training tasks, namely Sequence Matching Prediction (SMP) and Self
Reconstruction (SR), to learn specialized medication needs and interrelations
among clinical codes. Experimental results on two real-world datasets
demonstrate that RAREMed provides accurate drug sets for both rare and common
disease patients, thereby mitigating unfairness in medication recommendation
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EulerFormer: Sequential User Behavior Modeling with Complex Vector
  Attention <span class="chip">SIGIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To capture user preference, transformer models have been widely applied to
model sequential user behavior data. The core of transformer architecture lies
in the self-attention mechanism, which computes the pairwise attention scores
in a sequence. Due to the permutation-equivariant nature, positional encoding
is used to enhance the attention between token representations. In this
setting, the pairwise attention scores can be derived by both semantic
difference and positional difference. However, prior studies often model the
two kinds of difference measurements in different ways, which potentially
limits the expressive capacity of sequence modeling. To address this issue,
this paper proposes a novel transformer variant with complex vector attention,
named EulerFormer, which provides a unified theoretical framework to formulate
both semantic difference and positional difference. The EulerFormer involves
two key technical improvements. First, it employs a new transformation function
for efficiently transforming the sequence tokens into polar-form complex
vectors using Euler's formula, enabling the unified modeling of both semantic
and positional information in a complex rotation form.Secondly, it develops a
differential rotation mechanism, where the semantic rotation angles can be
controlled by an adaptation function, enabling the adaptive integration of the
semantic and positional information according to the semantic
contexts.Furthermore, a phase contrastive learning task is proposed to improve
the anisotropy of contextual representations in EulerFormer. Our theoretical
framework possesses a high degree of completeness and generality. It is more
robust to semantic variations and possesses moresuperior theoretical properties
in principle. Extensive experiments conducted on four public datasets
demonstrate the effectiveness and efficiency of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in SIGIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Autoencoders are PDE Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Zhou, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural solvers for partial differential equations (PDEs) have great
potential, yet their practicality is currently limited by their
generalizability. PDEs evolve over broad scales and exhibit diverse behaviors;
predicting these phenomena will require learning representations across a wide
variety of inputs, which may encompass different coefficients, geometries, or
equations. As a step towards generalizable PDE modeling, we adapt masked
pretraining for PDEs. Through self-supervised learning across PDEs, masked
autoencoders can learn useful latent representations for downstream tasks. In
particular, masked pretraining can improve coefficient regression and
timestepping performance of neural solvers on unseen equations. We hope that
masked pretraining can emerge as a unifying method across large, unlabeled, and
heterogeneous datasets to learn latent physics at scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Tang, Lianglun Cheng, Guoheng Huang, Zhengguang Tan, Junhao Lu, Kaihong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation holds a vital position in the realms of diagnosis and
treatment within the medical domain. Traditional convolutional neural networks
(CNNs) and Transformer models have made significant advancements in this realm,
but they still encounter challenges because of limited receptive field or high
computing complexity. Recently, State Space Models (SSMs), particularly Mamba
and its variants, have demonstrated notable performance in the field of vision.
However, their feature extraction methods may not be sufficiently effective and
retain some redundant structures, leaving room for parameter reduction.
Motivated by previous spatial and channel attention methods, we propose Triplet
Mamba-UNet. The method leverages residual VSS Blocks to extract intensive
contextual features, while Triplet SSM is employed to fuse features across
spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,
CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,
demonstrating the superior segmentation performance of our proposed TM-UNet.
Additionally, compared to the previous VM-UNet, our model achieves a one-third
reduction in parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding
  Length Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiguo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When the predicted sequence length exceeds the length seen during training,
the transformer's inference accuracy diminishes. Existing relative position
encoding methods, such as those based on the ALiBi technique, address the
length extrapolation challenge exclusively through the implementation of a
single kernel function, which introduces a constant bias to every post-softmax
attention scores according to their distance. These approaches do not
investigate or employ multiple kernel functions to address the extrapolation
challenge. Drawing on the ALiBi approach, this study proposes a novel relative
positional encoding method, called MEP, which employs a weighted average to
combine distinct kernel functions(such as the exponential kernel and the
Gaussian kernel) to generate a bias that is applied to post-softmax attention
scores. Initially, the framework utilizes various kernel functions to construct
multiple kernel functions. Each kernel function adheres to a consistent mean
weight coefficient, harnessing the synergistic advantages of different kernels
to formulate an innovative bias function. Subsequently, specific slopes are
tailored for each kernel function, applying penalties at varying rates, to
enhance the model's extrapolation capabilities. Finally, this bias is
seamlessly incorporated as a penalty to the post-softmax scores. We present two
distinct versions of our method: a parameter-free variant that requires no new
learnable parameters, which enhances length extrapolation capabilities without
compromising training efficiency, and a parameterized variant capable of
integrating state-of-the-art techniques. Empirical evaluations across diverse
datasets have demonstrated that both variants of our method achieve
state-of-the-art performance, outperforming traditional parameter-free and
parameterized approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PlainMamba: a simple non-hierarchical state space model (SSM)
designed for general visual recognition. The recent Mamba model has shown how
SSMs can be highly competitive with other architectures on sequential data and
initial attempts have been made to apply it to images. In this paper, we
further adapt the selective scanning process of Mamba to the visual domain,
enhancing its ability to learn features from two-dimensional images by (i) a
continuous 2D scanning process that improves spatial continuity by ensuring
adjacency of tokens in the scanning sequence, and (ii) direction-aware updating
which enables the model to discern the spatial relations of tokens by encoding
directional information. Our architecture is designed to be easy to use and
easy to scale, formed by stacking identical PlainMamba blocks, resulting in a
model with constant width throughout all layers. The architecture is further
simplified by removing the need for special tokens. We evaluate PlainMamba on a
variety of visual recognition tasks including image classification, semantic
segmentation, object detection, and instance segmentation. Our method achieves
performance gains over previous non-hierarchical models and is competitive with
hierarchical alternatives. For tasks requiring high-resolution inputs, in
particular, PlainMamba requires much less computing while maintaining high
performance. Code and models are available at
https://github.com/ChenhongyiYang/PlainMamba
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifold-Guided Lyapunov Control with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Mukherjee, Thanin Quartz, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to generating stabilizing controllers
for a large class of dynamical systems using diffusion models. The core
objective is to develop stabilizing control functions by identifying the
closest asymptotically stable vector field relative to a predetermined manifold
and adjusting the control function based on this finding. To achieve this, we
employ a diffusion model trained on pairs consisting of asymptotically stable
vector fields and their corresponding Lyapunov functions. Our numerical results
demonstrate that this pre-trained model can achieve stabilization over
previously unseen systems efficiently and rapidly, showcasing the potential of
our approach in fast zero-shot control and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Private is DP-SGD? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate a substantial gap between the privacy guarantees of the
Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch
sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of
Differentially Private Stochastic Gradient Descent (DP-SGD) follows by
interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is
more commonly used in practical implementations, it is neither analytically nor
numerically amenable to easy privacy analysis. On the other hand, Poisson
subsampling based DP-SGD is challenging to scalably implement, but has a
well-understood privacy analysis, with multiple open-source numerically tight
privacy accountants available. This has led to a common practice of using
shuffling based DP-SGD in practice, but using the privacy analysis for the
corresponding Poisson subsampling version. Our result shows that there can be a
substantial gap between the privacy analysis when using the two types of batch
sampling, and thus advises caution in reporting privacy parameters for DP-SGD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Piloto, Sofia Liguori, Sephora Madjiheurem, Miha Zgubic, Sean Lovett, Hamish Tomlinson, Sophie Elster, Chris Apps, Sims Witherspoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal Power Flow (OPF) refers to a wide range of related optimization
problems with the goal of operating power systems efficiently and securely. In
the simplest setting, OPF determines how much power to generate in order to
minimize costs while meeting demand for power and satisfying physical and
operational constraints. In even the simplest case, power grid operators use
approximations of the AC-OPF problem because solving the exact problem is
prohibitively slow with state-of-the-art solvers. These approximations
sacrifice accuracy and operational feasibility in favor of speed. This
trade-off leads to costly "uplift payments" and increased carbon emissions,
especially for large power grids. In the present work, we train a deep learning
system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF
cost) without compromising speed (running in as little as 33--65 ms).
Importantly, CANOS scales to realistic grid sizes with promising empirical
results on grids containing as many as 10,000 buses. Finally, because CANOS is
a Graph Neural Network, it is robust to changes in topology. We show that CANOS
is accurate across N-1 topological perturbations of a base grid typically used
in security-constrained analysis. This paves the way for more efficient
optimization of more complex OPF problems which alter grid connectivity such as
unit commitment, topology optimization and security-constrained OPF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGHormer: An Energy-Saving Graph <span class="highlight-title">Transformer</span> Driven by Spikes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huizhe Zhang, Jintang Li, Liang Chen, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Transformers (GTs) with powerful representation learning ability make a
huge success in wide range of graph tasks. However, the costs behind
outstanding performances of GTs are higher energy consumption and computational
overhead. The complex structure and quadratic complexity during attention
calculation in vanilla transformer seriously hinder its scalability on the
large-scale graph data. Though existing methods have made strides in
simplifying combinations among blocks or attention-learning paradigm to improve
GTs' efficiency, a series of energy-saving solutions originated from
biologically plausible structures are rarely taken into consideration when
constructing GT framework. To this end, we propose a new spiking-based graph
transformer (SGHormer). It turns full-precision embeddings into sparse and
binarized spikes to reduce memory and computational costs. The spiking graph
self-attention and spiking rectify blocks in SGHormer explicitly capture global
structure information and recover the expressive power of spiking embeddings,
respectively. In experiments, SGHormer achieves comparable performances to
other full-precision GTs with extremely low computational energy consumption.
The results show that SGHomer makes a remarkable progress in the field of
low-energy GTs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-aware Distributional Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaocong Chen, Siyu Wang, Tong Yu, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) presents distinct challenges as it relies
solely on observational data. A central concern in this context is ensuring the
safety of the learned policy by quantifying uncertainties associated with
various actions and environmental stochasticity. Traditional approaches
primarily emphasize mitigating epistemic uncertainty by learning risk-averse
policies, often overlooking environmental stochasticity. In this study, we
propose an uncertainty-aware distributional offline RL method to simultaneously
address both epistemic uncertainty and environmental stochasticity. We propose
a model-free offline RL algorithm capable of learning risk-averse policies and
characterizing the entire distribution of discounted cumulative rewards, as
opposed to merely maximizing the expected value of accumulated discounted
returns. Our method is rigorously evaluated through comprehensive experiments
in both risk-sensitive and risk-neutral benchmarks, demonstrating its superior
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PeersimGym: An Environment for Solving the Task Offloading Problem with
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederico Metelo, Stevo Racković, Pedro Ákos, Cláudia Soares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task offloading, crucial for balancing computational loads across devices in
networks such as the Internet of Things, poses significant optimization
challenges, including minimizing latency and energy usage under strict
communication and storage constraints. While traditional optimization falls
short in scalability; and heuristic approaches lack in achieving optimal
outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the
learning of optimal offloading strategies through iterative interactions.
However, the efficacy of RL hinges on access to rich datasets and
custom-tailored, realistic training environments. To address this, we introduce
PeersimGym, an open-source, customizable simulation environment tailored for
developing and optimizing task offloading strategies within computational
networks. PeersimGym supports a wide range of network topologies and
computational constraints and integrates a \textit{PettingZoo}-based interface
for RL agent deployment in both solo and multi-agent setups. Furthermore, we
demonstrate the utility of the environment through experiments with Deep
Reinforcement Learning agents, showcasing the potential of RL-based approaches
to significantly enhance offloading strategies in distributed computing
settings. PeersimGym thus bridges the gap between theoretical RL models and
their practical applications, paving the way for advancements in efficient task
offloading methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retentive Decision <span class="highlight-title">Transformer</span> with Adaptive Masking for Reinforcement
  Learning based Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Wang, Xiaocong Chen, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise
across a spectrum of applications, from e-commerce platforms to streaming
services. Yet, they grapple with challenges, notably in crafting reward
functions and harnessing large pre-existing datasets within the RL framework.
Recent advancements in offline RLRS provide a solution for how to address these
two challenges. However, existing methods mainly rely on the transformer
architecture, which, as sequence lengths increase, can introduce challenges
associated with computational resources and training costs. Additionally, the
prevalent methods employ fixed-length input trajectories, restricting their
capacity to capture evolving user preferences. In this study, we introduce a
new offline RLRS method to deal with the above problems. We reinterpret the
RLRS challenge by modeling sequential decision-making as an inference task,
leveraging adaptive masking configurations. This adaptive approach selectively
masks input tokens, transforming the recommendation task into an inference
challenge based on varying token subsets, thereby enhancing the agent's ability
to infer across diverse trajectory lengths. Furthermore, we incorporate a
multi-scale segmented retention mechanism that facilitates efficient modeling
of long sequences, significantly enhancing computational efficiency. Our
experimental analysis, conducted on both online simulator and offline datasets,
clearly demonstrates the advantages of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven Energy Consumption Modelling for Electric Micromobility
  using an Open <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li, Mingming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating challenges of traffic congestion and environmental degradation
underscore the critical importance of embracing E-Mobility solutions in urban
spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,
play a pivotal role in this transition, offering sustainable alternatives for
urban commuters. However, the energy consumption patterns for these tools are a
critical aspect that impacts their effectiveness in real-world scenarios and is
essential for trip planning and boosting user confidence in using these. To
this effect, recent studies have utilised physical models customised for
specific mobility tools and conditions, but these models struggle with
generalization and effectiveness in real-world scenarios due to a notable
absence of open datasets for thorough model evaluation and verification. To
fill this gap, our work presents an open dataset, collected in Dublin, Ireland,
specifically designed for energy modelling research related to E-Scooters and
E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption
modelling based on the dataset using a set of representative machine learning
algorithms and compare their performance against the contemporary mathematical
models as a baseline. Our results demonstrate a notable advantage for
data-driven models in comparison to the corresponding mathematical models for
estimating energy consumption. Specifically, data-driven models outperform
physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for
E-Scooters based on an in-depth analysis of the dataset under certain
assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 4 tables. This manuscript has been accepted by
  the IEEE ITEC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake or JPEG? Revealing Common Biases in Generated Image Detection
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of generative image models has highlighted the urgent
need to detect artificial content, which is a crucial step in combating
widespread manipulation and misinformation. Consequently, numerous detectors
and associated datasets have emerged. However, many of these datasets
inadvertently introduce undesirable biases, thereby impacting the effectiveness
and evaluation of detectors. In this paper, we emphasize that many datasets for
AI-generated image detection contain biases related to JPEG compression and
image size. Using the GenImage dataset, we demonstrate that detectors indeed
learn from these undesired factors. Furthermore, we show that removing the
named biases substantially increases robustness to JPEG compression and
significantly alters the cross-generator performance of evaluated detectors.
Specifically, it leads to more than 11 percentage points increase in
cross-generator performance for ResNet50 and Swin-T detectors on the GenImage
dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous
website: https://www.unbiased-genimage.org
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASIL: Learner-Aware Supervised Imitation Learning For Long-term
  Microscopic Traffic Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Guo, Zhenwei Miao, Wei Jing, Weiwei Liu, Weizi Li, Dayang Hao, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic traffic simulation plays a crucial role in transportation
engineering by providing insights into individual vehicle behavior and overall
traffic flow. However, creating a realistic simulator that accurately
replicates human driving behaviors in various traffic conditions presents
significant challenges. Traditional simulators relying on heuristic models
often fail to deliver accurate simulations due to the complexity of real-world
traffic environments. Due to the covariate shift issue, existing imitation
learning-based simulators often fail to generate stable long-term simulations.
In this paper, we propose a novel approach called learner-aware supervised
imitation learning to address the covariate shift problem in multi-agent
imitation learning. By leveraging a variational autoencoder simultaneously
modeling the expert and learner state distribution, our approach augments
expert states such that the augmented state is aware of learner state
distribution. Our method, applied to urban traffic simulation, demonstrates
significant improvements over existing state-of-the-art baselines in both
short-term microscopic and long-term macroscopic realism when evaluated on the
real-world dataset pNEUMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by cvpr 2024. arXiv admin note: text overlap with
  arXiv:2306.06401</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Benefits of Over-parameterization for Out-of-Distribution
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Hao, Yong Lin, Difan Zou, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, machine learning models have achieved success based on the
independently and identically distributed assumption. However, this assumption
can be easily violated in real-world applications, leading to the
Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized
DNNs behave under non-trivial natural distributional shifts is essential, as
current theoretical understanding is insufficient. Existing theoretical works
often provide meaningless results for over-parameterized models in OOD
scenarios or even contradict empirical findings. To this end, we are
investigating the performance of the over-parameterized model in terms of OOD
generalization under the general benign overfitting conditions. Our analysis
focuses on a random feature model and examines non-trivial natural
distributional shifts, where the benign overfitting estimators demonstrate a
constant excess OOD loss, despite achieving zero excess in-distribution (ID)
loss. We demonstrate that in this scenario, further increasing the model's
parameterization can significantly reduce the OOD loss. Intuitively, the
variance term of ID loss remains low due to orthogonality of long-tail
features, meaning overfitting noise during training generally doesn't raise
testing loss. However, in OOD cases, distributional shift increases the
variance term. Thankfully, the inherent shift is unrelated to individual x,
maintaining the orthogonality of long-tail features. Expanding the hidden
dimension can additionally improve this orthogonality by mapping the features
into higher-dimensional spaces, thereby reducing the variance term. We further
show that model ensembles also improve OOD loss, akin to increasing model
capacity. These insights explain the empirical phenomenon of enhanced OOD
generalization through model ensembles, supported by consistent simulations
with theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haddouchi Maissae, Berrado Abdelaziz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random Forest (RF) is well-known as an efficient ensemble learning method in
terms of predictive performance. It is also considered a Black Box because of
its hundreds of deep decision trees. This lack of interpretability can be a
real drawback for acceptance of RF models in several real-world applications,
especially those affecting one's lives, such as in healthcare, security, and
law. In this work, we present Forest-ORE, a method that makes RF interpretable
via an optimized rule ensemble (ORE) for local and global interpretation.
Unlike other rule-based approaches aiming at interpreting the RF model, this
method simultaneously considers several parameters that influence the choice of
an interpretable rule ensemble. Existing methods often prioritize predictive
performance over interpretability coverage and do not provide information about
existing overlaps or interactions between rules. Forest-ORE uses a
mixed-integer optimization program to build an ORE that considers the trade-off
between predictive performance, interpretability coverage, and model size (size
of the rule ensemble, rule lengths, and rule overlaps). In addition to
providing an ORE competitive in predictive performance with RF, this method
enriches the ORE through other rules that afford complementary information. It
also enables monitoring of the rule selection process and delivers various
metrics that can be used to generate a graphical representation of the final
model. This framework is illustrated through an example, and its robustness is
assessed through 36 benchmark datasets. A comparative analysis of well-known
methods shows that Forest-ORE provides an excellent trade-off between
predictive performance, interpretability coverage, and model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Memory Networks: A Versatile Adaptation Approach for
  Vision-Language Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024; Codes are available at \url{https://github.com/YBZh/DMN}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Zero-Data, Controllable, Adaptive Dialog System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Väth, Lindsey Vanderlyn, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Tree Search (V\"ath et al., 2023) is a recent approach to
controllable dialog systems, where domain experts shape the behavior of a
Reinforcement Learning agent through a dialog tree. The agent learns to
efficiently navigate this tree, while adapting to information needs, e.g.,
domain familiarity, of different users. However, the need for additional
training data hinders deployment in new domains. To address this, we explore
approaches to generate this data directly from dialog trees. We improve the
original approach, and show that agents trained on synthetic data can achieve
comparable dialog success to models trained on human data, both when using a
commercial Large Language Model for generation, or when using a smaller
open-source model, running on a single GPU. We further demonstrate the
scalability of our approach by collecting and testing on two new datasets:
ONBOARD, a new domain helping foreign residents moving to a new city, and the
medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and
head symptoms. Finally, we perform human testing, where no statistically
significant differences were found in either objective or subjective measures
between models trained on human and generated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Privacy in Federated Learning through Local Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Bastianello, Changxin Liu, Karl H. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose the federated private local training algorithm
(Fed-PLT) for federated learning, to overcome the challenges of (i) expensive
communications and (ii) privacy preservation. We address (i) by allowing for
both partial participation and local training, which significantly reduce the
number of communication rounds between the central coordinator and computing
agents. The algorithm matches the state of the art in the sense that the use of
local training demonstrably does not impact accuracy. Additionally, agents have
the flexibility to choose from various local training solvers, such as
(stochastic) gradient descent and accelerated gradient descent. Further, we
investigate how employing local training can enhance privacy, addressing point
(ii). In particular, we derive differential privacy bounds and highlight their
dependence on the number of local training epochs. We assess the effectiveness
of the proposed algorithm by comparing it to alternative techniques,
considering both theoretical analysis and numerical results from a
classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Deep Learning and State-of-the-arts Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohd Halim Mohd Noor, Ayokunle Olalekan Ige
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning, a branch of artificial intelligence, is a computational model
that uses multiple layers of interconnected units (neurons) to learn intricate
patterns and representations directly from raw input data. Empowered by this
learning capability, it has become a powerful tool for solving complex problems
and is the core driver of many groundbreaking technologies and innovations.
Building a deep learning model is a challenging task due to the algorithm`s
complexity and the dynamic nature of real-world problems. Several studies have
reviewed deep learning concepts and applications. However, the studies mostly
focused on the types of deep learning models and convolutional neural network
architectures, offering limited coverage of the state-of-the-art of deep
learning models and their applications in solving complex problems across
different domains. Therefore, motivated by the limitations, this study aims to
comprehensively review the state-of-the-art deep learning models in computer
vision, natural language processing, time series analysis and pervasive
computing. We highlight the key features of the models and their effectiveness
in solving the problems within each domain. Furthermore, this study presents
the fundamentals of deep learning, various deep learning model types and
prominent convolutional neural network architectures. Finally, challenges and
future directions in deep learning research are discussed to offer a broader
perspective for future researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been achieved in sensing real large-scale
outdoor 3D environments, particularly by using modern acquisition equipment
such as LiDAR sensors. Unfortunately, they are fundamentally limited in their
ability to produce dense, complete 3D scenes. To address this issue, recent
learning-based methods integrate neural implicit representations and
optimizable feature grids to approximate surfaces of 3D scenes. However,
naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results
due to the nature of sparse, conflicting LiDAR measurements. Instead, in this
work we depart from fitting LiDAR data exactly, instead letting the network
optimize a non-metric monotonic implicit field defined in 3D space. To fit our
field, we design a learning system integrating a monotonicity loss that enables
optimizing neural monotonic fields and leverages recent progress in large-scale
3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as
captured by multiple quantitative and perceptual measures and visual results
obtained for Mai City, Newer College, and KITTI benchmarks. The code of our
approach will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VDSC: Enhancing Exploration Timing with Value Discrepancy and State
  Counts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Captari, Remo Sasso, Matthia Sabatelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable attention given to the questions of \textit{how
much} and \textit{how to} explore in deep reinforcement learning, the
investigation into \textit{when} to explore remains relatively less researched.
While more sophisticated exploration strategies can excel in specific, often
sparse reward environments, existing simpler approaches, such as
$\epsilon$-greedy, persist in outperforming them across a broader spectrum of
domains. The appeal of these simpler strategies lies in their ease of
implementation and generality across a wide range of domains. The downside is
that these methods are essentially a blind switching mechanism, which
completely disregards the agent's internal state. In this paper, we propose to
leverage the agent's internal state to decide \textit{when} to explore,
addressing the shortcomings of blind switching mechanisms. We present Value
Discrepancy and State Counts through homeostasis (VDSC), a novel approach for
efficient exploration timing. Experimental results on the Atari suite
demonstrate the superiority of our strategy over traditional methods such as
$\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like
Noisy Nets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range
  Air Combat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edvards Scukins, Markus Klein, Lars Kroon, Petter Ögren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating new air combat tactics and discovering novel maneuvers can require
numerous hours of expert pilots' time. Additionally, for each different combat
scenario, the same strategies may not work since small changes in equipment
performance may drastically change the air combat outcome. For this reason, we
created a reinforcement learning environment to help investigate potential air
combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR
Gym. This type of air combat is important since long-range missiles are often
the first weapon to be used in aerial combat. Some existing environments
provide high-fidelity simulations but are either not open source or are not
adapted to the BVR air combat domain. Other environments are open source but
use less accurate simulation models. Our work provides a high-fidelity
environment based on the open-source flight dynamics simulator JSBSim and is
adapted to the BVR air combat domain. This article describes the building
blocks of the environment and some use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Adversarial Training via Fisher-Rao Norm-based Regularization <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Yin, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is extensively utilized to improve the adversarial
robustness of deep neural networks. Yet, mitigating the degradation of standard
generalization performance in adversarial-trained models remains an open
problem. This paper attempts to resolve this issue through the lens of model
complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant
metric for model complexity, to establish the non-trivial bounds of the
Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer
Perceptron. Then we generalize a complexity-related variable, which is
sensitive to the changes in model width and the trade-off factors in
adversarial training. Moreover, intensive empirical evidence validates that
this variable highly correlates with the generalization gap of Cross-Entropy
loss between adversarial-trained and standard-trained models, especially during
the initial and final phases of the training process. Building upon this
observation, we propose a novel regularization framework, called Logit-Oriented
Adversarial Training (LOAT), which can mitigate the trade-off between
robustness and accuracy while imposing only a negligible increase in
computational overhead. Our extensive experiments demonstrate that the proposed
regularization strategy can boost the performance of the prevalent adversarial
training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,
across various network architectures. Our code will be available at
https://github.com/TrustAI/LOAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction-sharing During Training and Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two firms are engaged in a competitive prediction task. Each firm has two
sources of data -- labeled historical data and unlabeled inference-time data --
and uses the former to derive a prediction model, and the latter to make
predictions on new instances. We study data-sharing contracts between the
firms. The novelty of our study is to introduce and highlight the differences
between contracts that share prediction models only, contracts to share
inference-time predictions only, and contracts to share both. Our analysis
proceeds on three levels. First, we develop a general Bayesian framework that
facilitates our study. Second, we narrow our focus to two natural settings
within this framework: (i) a setting in which the accuracy of each firm's
prediction model is common knowledge, but the correlation between the
respective models is unknown; and (ii) a setting in which two hypotheses exist
regarding the optimal predictor, and one of the firms has a structural
advantage in deducing it. Within these two settings we study optimal contract
choice. More specifically, we find the individually rational and Pareto-optimal
contracts for some notable cases, and describe specific settings where each of
the different sharing contracts emerge as optimal. Finally, in the third level
of our analysis we demonstrate the applicability of our concepts in a synthetic
simulation using real loan data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangchen Yin, Yue Yin, Yuda W. Tang, Hai Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning force fields (MLFFs) have emerged as a promising approach to
bridge the accuracy of quantum mechanical methods and the efficiency of
classical force fields. However, the abundance of MLFF models and the challenge
of accurately predicting atomic forces pose significant obstacles in their
practical application. In this paper, we propose a novel ensemble learning
framework, EL-MLFFs, which leverages the stacking method to integrate
predictions from diverse MLFFs and enhance force prediction accuracy. By
constructing a graph representation of molecular structures and employing a
graph neural network (GNN) as the meta-model, EL-MLFFs effectively captures
atomic interactions and refines force predictions. We evaluate our approach on
two distinct datasets: methane molecules and methanol adsorbed on a Cu(100)
surface. The results demonstrate that EL-MLFFs significantly improves force
prediction accuracy compared to individual MLFFs, with the ensemble of all
eight models yielding the best performance. Moreover, our ablation study
highlights the crucial roles of the residual network and graph attention layers
in the model's architecture. The EL-MLFFs framework offers a promising solution
to the challenges of model selection and force prediction accuracy in MLFFs,
paving the way for more reliable and efficient molecular simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free
  Class-Incremental Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Zhiping Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) under an exemplar-free constraint has
presented a significant challenge. Existing methods adhering to this constraint
are prone to catastrophic forgetting, far more so than replay-based techniques
that retain access to past samples. In this paper, to solve the exemplar-free
CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The
DS-AL contains a main stream offering an analytical (i.e., closed-form) linear
solution, and a compensation stream improving the inherent under-fitting
limitation due to adopting linear mapping. The main stream redefines the CIL
problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an
equivalence between the CIL and its joint-learning counterpart. The
compensation stream is governed by a Dual-Activation Compensation (DAC) module.
This module re-activates the embedding with a different activation function
from the main stream one, and seeks fitting compensation by projecting the
embedding to the null space of the main stream's linear mapping. Empirical
results demonstrate that the DS-AL, despite being an exemplar-free technique,
delivers performance comparable with or better than that of replay-based
methods across various datasets, including CIFAR-100, ImageNet-100 and
ImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to
execute CIL in a phase-invariant manner. This is evidenced by a
never-before-seen 500-phase CIL ImageNet task, which performs on a level
identical to a 5-phase one. Our codes are available at
https://github.com/ZHUANGHP/Analytic-continual-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Graph Auto-Encoder Based Inductive Learning Method for
  Semi-Supervised Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxuan Yang, Zhaoxin Yu, Qingchao Kong, Wei Liu, Wenji Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning is a fundamental research issue in various
domains of applications, of which the inductive learning problem is
particularly challenging as it requires models to generalize to unseen graph
structures during inference. In recent years, graph neural networks (GNNs) have
emerged as powerful graph models for inductive learning tasks such as node
classification, whereas they typically heavily rely on the annotated nodes
under a fully supervised training setting. Compared with the GNN-based methods,
variational graph auto-encoders (VGAEs) are known to be more generalizable to
capture the internal structural information of graphs independent of node
labels and have achieved prominent performance on multiple unsupervised
learning tasks. However, so far there is still a lack of work focusing on
leveraging the VGAE framework for inductive learning, due to the difficulties
in training the model in a supervised manner and avoiding over-fitting the
proximity information of graphs. To solve these problems and improve the model
performance of VGAEs for inductive graph representation learning, in this work,
we propose the Self-Label Augmented VGAE model. To leverage the label
information for training, our model takes node labels as one-hot encoded inputs
and then performs label reconstruction in model training. To overcome the
scarcity problem of node labels for semi-supervised settings, we further
propose the Self-Label Augmentation Method (SLAM), which uses pseudo labels
generated by our model with a node-wise masking approach to enhance the label
information. Experiments on benchmark inductive learning graph datasets verify
that our proposed model archives promising results on node classification with
particular superiority under semi-supervised learning settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capacity Provisioning Motivated Online Non-Convex Optimization Problem
  with Memory and Switching Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Vaze, Jayakrishnan Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An online non-convex optimization problem is considered where the goal is to
minimize the flow time (total delay) of a set of jobs by modulating the number
of active servers, but with a switching cost associated with changing the
number of active servers over time. Each job can be processed by at most one
fixed speed server at any time. Compared to the usual online convex
optimization (OCO) problem with switching cost, the objective function
considered is non-convex and more importantly, at each time, it depends on all
past decisions and not just the present one. Both worst-case and stochastic
inputs are considered; for both cases, competitive algorithms are derived.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Requirements Testability Measurement Based on
  Requirement Smells 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morteza Zakeri-Nasrabadi, Saeed Parsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Requirements form the basis for defining software systems' obligations and
tasks. Testable requirements help prevent failures, reduce maintenance costs,
and make it easier to perform acceptance tests. However, despite the importance
of measuring and quantifying requirements testability, no automatic approach
for measuring requirements testability has been proposed based on the
requirements smells, which are at odds with the requirements testability. This
paper presents a mathematical model to evaluate and rank the natural language
requirements testability based on an extensive set of nine requirements smells,
detected automatically, and acceptance test efforts determined by requirement
length and its application domain. Most of the smells stem from uncountable
adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary
is required to detect such words. We offer a neural word-embedding technique to
generate such a dictionary automatically. Using the dictionary, we could
automatically detect Polysemy smell (domain-specific ambiguity) for the first
time in 10 application domains. Our empirical study on nearly 1000 software
requirements from six well-known industrial and academic projects demonstrates
that the proposed smell detection approach outperforms Smella, a
state-of-the-art tool, in detecting requirements smells. The precision and
recall of smell detection are improved with an average of 0.03 and 0.33,
respectively, compared to the state-of-the-art. The proposed requirement
testability model measures the testability of 985 requirements with a mean
absolute error of 0.12 and a mean squared error of 0.03, demonstrating the
model's potential for practical use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 16 figures, and 13 tables; submitted as a journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Kernel for Neural Network Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao-Qun Zhang, Zong-Yi Chen, Yong-Ming Tian, Xun Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past decades have witnessed a great interest in the distinction and
connection between neural network learning and kernel learning. Recent
advancements have made theoretical progress in connecting infinite-wide neural
networks and Gaussian processes. Two predominant approaches have emerged: the
Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The
former, rooted in Bayesian inference, represents a zero-order kernel, while the
latter, grounded in the tangent space of gradient descents, is a first-order
kernel. In this paper, we present the Unified Neural Kernel (UNK), which
characterizes the learning dynamics of neural networks with gradient descents
and parameter initialization. The proposed UNK kernel maintains the limiting
properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite
learning step and converging to NNGP as the learning step approaches infinity.
Besides, we also theoretically characterize the uniform tightness and learning
convergence of the UNK kernel, providing comprehensive insights into this
unified kernel. Experimental results underscore the effectiveness of our
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim, Hyoungshick Kim, Jin B. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper provides empirical comparisons between recent IDSs to provide an
objective comparison between them to help users choose the most appropriate
solution based on their requirements. Our results show that no one solution is
the best, but is dependent on external variables such as the types of attacks,
complexity, and network environment in the dataset. For example, BoT_IoT and
Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural
network performed the best when tested using the BoT_IoT dataset while HELAD
performed the best when tested using the Stratosphere IoT dataset. So although
we found that a deep neural network solution had the highest average F1 scores
on tested datasets, it is not always the best-performing one. We further
discuss difficulties in using IDS from literature and project repositories,
which complicated drawing definitive conclusions regarding IDS selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imitating Cost-Constrained Behaviors in Reinforcement Learning <span class="chip">ICAPS-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Pradeep Varakantham, Shih-Fen Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcement learning has focused on imitation
primarily in unconstrained settings (e.g., no limit on fuel consumed by the
vehicle). However, in many real-world domains, the behavior of an expert is
governed not only by reward (or preference) but also by constraints. For
instance, decisions on self-driving delivery vehicles are dependent not only on
the route preferences/rewards (depending on past demand data) but also on the
fuel in the vehicle and the time available. In such problems, imitation
learning is challenging as decisions are not only dictated by the reward model
but are also dependent on a cost-constrained model. In this paper, we provide
multiple methods that match expert distributions in the presence of trajectory
cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to
find a good trade-off between expected return and minimizing constraint
violation; and (c) Cost-violation-based alternating gradient. We empirically
show that leading imitation learning approaches imitate cost-constrained
behaviors poorly and our meta-gradient-based approach achieves the best
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 34th International Conference on Automated Planning
  and Scheduling (ICAPS-24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain of Compression: A Systematic Approach to Combinationally Compress
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Shen, Minqing Sun, Jie Zhao, An Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have achieved significant popularity,
but their computational and memory intensity poses challenges for
resource-constrained computing systems, particularly with the prerequisite of
real-time performance. To release this burden, model compression has become an
important research focus. Many approaches like quantization, pruning, early
exit, and knowledge distillation have demonstrated the effect of reducing
redundancy in neural networks. Upon closer examination, it becomes apparent
that each approach capitalizes on its unique features to compress the neural
network, and they can also exhibit complementary behavior when combined. To
explore the interactions and reap the benefits from the complementary features,
we propose the Chain of Compression, which works on the combinational sequence
to apply these common techniques to compress the neural network. Validated on
the image-based regression and classification networks across different data
sets, our proposed Chain of Compression can significantly compress the
computation cost by 100-1000 times with ignorable accuracy loss compared with
the baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Exponential Smoothing into MLP: A Simple but Effective
  Sequence Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiqun Chu, Zuoquan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling long-range dependencies in sequential data is a crucial step in
sequence learning. A recently developed model, the Structured State Space (S4),
demonstrated significant effectiveness in modeling long-range sequences.
However, It is unclear whether the success of S4 can be attributed to its
intricate parameterization and HiPPO initialization or simply due to State
Space Models (SSMs). To further investigate the potential of the deep SSMs, we
start with exponential smoothing (ETS), a simple SSM, and propose a stacked
architecture by directly incorporating it into an element-wise MLP. We augment
simple ETS with additional parameters and complex field to reduce the inductive
bias. Despite increasing less than 1\% of parameters of element-wise MLP, our
models achieve comparable results to S4 on the LRA benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Particle identification with machine learning from incomplete data in
  the ALICE experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maja Karwowska, Łukasz Graczykowski, Kamil Deja, Miłosz Kasak, Małgorzata Janik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ALICE experiment at the LHC measures properties of the strongly
interacting matter formed in ultrarelativistic heavy-ion collisions. Such
studies require accurate particle identification (PID). ALICE provides PID
information via several detectors for particles with momentum from about 100
MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular
cuts. Acmuch better performance can be achieved with machine learning (ML)
methods. Our solution uses multiple neural networks (NN) serving as binary
classifiers. Moreover, we extended our particle classifier with Feature Set
Embedding and attention in order to train on data with incomplete samples. We
also present the integration of the ML project with the ALICE analysis
software, and we discuss domain adaptation, the ML technique needed to transfer
the knowledge between simulated and real experimental data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of 3rd Artificial Intelligence for the Electron Ion
  Collider workshop -- AI4EIC2023, 28.11-1.12.2023. Prepared for submission to
  JINST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Scalable Model Editing for Large Language Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can make predictions using parametric
knowledge--knowledge encoded in the model weights--or contextual
knowledge--knowledge presented in the context. In many scenarios, a desirable
behavior is that LLMs give precedence to contextual knowledge when it conflicts
with the parametric knowledge, and fall back to using their parametric
knowledge when the context is irrelevant. This enables updating and correcting
the model's knowledge by in-context editing instead of retraining. Previous
works have shown that LLMs are inclined to ignore contextual knowledge and fail
to reliably fall back to parametric knowledge when presented with irrelevant
context. In this work, we discover that, with proper prompting methods,
instruction-finetuned LLMs can be highly controllable by contextual knowledge
and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit
models by REading Notes) to improve the scalability and robustness of LLM
editing. To better evaluate the robustness of model editors, we collect a new
dataset, that contains irrelevant questions that are more challenging than the
ones in existing datasets. Empirical results show that our method outperforms
current state-of-the-art methods by a large margin. Unlike existing techniques,
it can integrate knowledge from multiple edits, and correctly respond to
syntactically similar but semantically unrelated inputs (and vice versa). The
source code can be found at https://github.com/thunlp/EREN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 paper, 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion
  Rate Prediction with a Single Model <span class="chip">CIKM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ouyang, Xiuwu Zhang, Chaofeng Guo, Shukui Ren, Yupei Sui, Kun Zhang, Jinmei Luo, Yunfeng Chen, Dongbo Xu, Xiangzheng Liu, Yanlong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world advertising systems, conversions have different types in nature
and ads can be shown in different display scenarios, both of which highly
impact the actual conversion rate (CVR). This results in the multi-type and
multi-scenario CVR prediction problem. A desired model for this problem should
satisfy the following requirements: 1) Accuracy: the model should achieve
fine-grained accuracy with respect to any conversion type in any display
scenario. 2) Scalability: the model parameter size should be affordable. 3)
Convenience: the model should not require a large amount of effort in data
partitioning, subset processing and separate storage. Existing approaches
cannot simultaneously satisfy these requirements. For example, building a
separate model for each (conversion type, display scenario) pair is neither
scalable nor convenient. Building a unified model trained on all the data with
conversion type and display scenario included as two features is not accurate
enough. In this paper, we propose the Masked Multi-domain Network (MMN) to
solve this problem. To achieve the accuracy requirement, we model
domain-specific parameters and propose a dynamically weighted loss to account
for the loss scale imbalance issue within each mini-batch. To achieve the
scalability requirement, we propose a parameter sharing and composition
strategy to reduce model parameters from a product space to a sum space. To
achieve the convenience requirement, we propose an auto-masking strategy which
can take mixed data from all the domains as input. It avoids the overhead
caused by data partitioning, individual processing and separate storage. Both
offline and online experimental results validate the superiority of MMN for
multi-type and multi-scenario CVR prediction. MMN is now the serving model for
real-time CVR prediction in UC Toutiao.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2023 (larger figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On permutation-invariant neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanari Kimura, Ryotaro Shimizu, Yuki Hirakawa, Ryosuke Goto, Yuki Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional machine learning algorithms have traditionally been designed
under the assumption that input data follows a vector-based format, with an
emphasis on vector-centric paradigms. However, as the demand for tasks
involving set-based inputs has grown, there has been a paradigm shift in the
research community towards addressing these challenges. In recent years, the
emergence of neural network architectures such as Deep Sets and Transformers
has presented a significant advancement in the treatment of set-based data.
These architectures are specifically engineered to naturally accommodate sets
as input, enabling more effective representation and processing of set
structures. Consequently, there has been a surge of research endeavors
dedicated to exploring and harnessing the capabilities of these architectures
for various tasks involving the approximation of set functions. This
comprehensive survey aims to provide an overview of the diverse problem
settings and ongoing research efforts pertaining to neural networks that
approximate set functions. By delving into the intricacies of these approaches
and elucidating the associated challenges, the survey aims to equip readers
with a comprehensive understanding of the field. Through this comprehensive
perspective, we hope that researchers can gain valuable insights into the
potential applications, inherent limitations, and future directions of
set-based neural networks. Indeed, from this survey we gain two insights: i)
Deep Sets and its variants can be generalized by differences in the aggregation
function, and ii) the behavior of Deep Sets is sensitive to the choice of the
aggregation function. From these observations, we show that Deep Sets, one of
the well-known permutation-invariant neural networks, can be generalized in the
sense of a quasi-arithmetic mean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcribing Bengali Text with Regional Dialects to IPA using District
  Guided Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S M Jishanul Islam, Sadia Ahmmed, Sahid Hossain Mustakim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate transcription of Bengali text to the International Phonetic Alphabet
(IPA) is a challenging task due to the complex phonology of the language and
context-dependent sound changes. This challenge is even more for regional
Bengali dialects due to unavailability of standardized spelling conventions for
these dialects, presence of local and foreign words popular in those regions
and phonological diversity across different regions. This paper presents an
approach to this sequence-to-sequence problem by introducing the District
Guided Tokens (DGT) technique on a new dataset spanning six districts of
Bangladesh. The key idea is to provide the model with explicit information
about the regional dialect or "district" of the input text before generating
the IPA transcription. This is achieved by prepending a district token to the
input sequence, effectively guiding the model to understand the unique phonetic
patterns associated with each district. The DGT technique is applied to
fine-tune several transformer-based models, on this new dataset. Experimental
results demonstrate the effectiveness of DGT, with the ByT5 model achieving
superior performance over word-based models like mT5, BanglaT5, and umT5. This
is attributed to ByT5's ability to handle a high percentage of
out-of-vocabulary words in the test set. The proposed approach highlights the
importance of incorporating regional dialect information into ubiquitous
natural language processing systems for languages with diverse phonological
variations. The following work was a result of the "Bhashamul" challenge, which
is dedicated to solving the problem of Bengali text with regional dialects to
IPA transcription https://www.kaggle.com/competitions/regipa/. The training and
inference notebooks are available through the competition link.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work became the champion of the Bhashamul challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization Error Analysis for Sparse Mixture-of-Experts: A
  Preliminary Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Zhao, Peihao Wang, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates
predictions from several specialized sub-models (referred to as experts). This
fusion is accomplished through a router mechanism, dynamically assigning
weights to each expert's contribution based on the input data. Conventional MoE
mechanisms select all available experts, incurring substantial computational
costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages
only a limited number, or even just one expert, significantly reducing
computation overhead while empirically preserving, and sometimes even
enhancing, performance. Despite its wide-ranging applications and these
advantageous characteristics, MoE's theoretical underpinnings have remained
elusive. In this paper, we embark on an exploration of Sparse MoE's
generalization error concerning various critical factors. Specifically, we
investigate the impact of the number of data samples, the total number of
experts, the sparsity in expert selection, the complexity of the routing
mechanism, and the complexity of individual experts. Our analysis sheds light
on \textit{how \textbf{sparsity} contributes to the MoE's generalization},
offering insights from the perspective of classical learning theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application-Driven Innovation in Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Rolnick, Alan Aspuru-Guzik, Sara Beery, Bistra Dilkina, Priya L. Donti, Marzyeh Ghassemi, Hannah Kerner, Claire Monteleoni, Esther Rolf, Milind Tambe, Adam White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As applications of machine learning proliferate, innovative algorithms
inspired by specific real-world challenges have become increasingly important.
Such work offers the potential for significant impact not merely in domains of
application but also in machine learning itself. In this paper, we describe the
paradigm of application-driven research in machine learning, contrasting it
with the more standard paradigm of methods-driven research. We illustrate the
benefits of application-driven machine learning and how this approach can
productively synergize with methods-driven work. Despite these benefits, we
find that reviewing, hiring, and teaching practices in machine learning often
hold back application-driven innovation. We outline how these processes may be
improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated that diffusion models are capable of
generating high-quality samples, but their quality heavily depends on sampling
guidance techniques, such as classifier guidance (CG) and classifier-free
guidance (CFG). These techniques are often not applicable in unconditional
generation or in various downstream tasks such as image restoration. In this
paper, we propose a novel sampling guidance, called Perturbed-Attention
Guidance (PAG), which improves diffusion sample quality across both
unconditional and conditional settings, achieving this without requiring
additional training or the integration of external modules. PAG is designed to
progressively enhance the structure of samples throughout the denoising
process. It involves generating intermediate samples with degraded structure by
substituting selected self-attention maps in diffusion U-Net with an identity
matrix, by considering the self-attention mechanisms' ability to capture
structural information, and guiding the denoising process away from these
degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves
sample quality in conditional and even unconditional scenarios. Moreover, PAG
significantly improves the baseline performance in various downstream tasks
where existing guidances such as CG or CFG cannot be fully utilized, including
ControlNet with empty prompts and image restoration such as inpainting and
deblurring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at
  https://ku-cvlab.github.io/Perturbed-Attention-Guidance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIDE: An Automatic Data Engine for Object Detection in Autonomous
  Driving <span class="chip">CVPR-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingfu Liang, Jong-Chyi Su, Samuel Schulter, Sparsh Garg, Shiyu Zhao, Ying Wu, Manmohan Chandraker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicle (AV) systems rely on robust perception models as a
cornerstone of safety assurance. However, objects encountered on the road
exhibit a long-tailed distribution, with rare or unseen categories posing
challenges to a deployed perception model. This necessitates an expensive
process of continuously curating and annotating data with significant human
effort. We propose to leverage recent advances in vision-language and large
language models to design an Automatic Data Engine (AIDE) that automatically
identifies issues, efficiently curates data, improves the model through
auto-labeling, and verifies the model through generation of diverse scenarios.
This process operates iteratively, allowing for continuous self-improvement of
the model. We further establish a benchmark for open-world detection on AV
datasets to comprehensively evaluate various learning paradigms, demonstrating
our method's superior performance at a reduced cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Moreau Envelope Approach for LQR Meta-Policy Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Aravind, Mohammad Taha Toghani, César A. Uribe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of policy estimation for the Linear Quadratic Regulator
(LQR) in discrete-time linear time-invariant uncertain dynamical systems. We
propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of
realizations of the uncertain system, to define a meta-policy efficiently
adjustable to new realizations. Moreover, we design an algorithm to find an
approximate first-order stationary point of the meta-LQR cost function.
Numerical results show that the proposed approach outperforms naive averaging
of controllers on new realizations of the linear system. We also provide
empirical evidence that our method has better sample complexity than
Model-Agnostic Meta-Learning (MAML) approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Trajectory Planning with Dual-Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibei Zhang, Tian Xiang, Chentao Mao, Yuhua Zheng, Shuai Li, Haoyi Niu, Xiangming Xi, Wenyuan Bai, Feng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'
performance in dynamic tasks. Traditional methods rely on solving complex
nonlinear programming problems, bringing significant delays in generating
optimized trajectories. In this paper, we propose a two-stage approach to
accelerate time-jerk optimal trajectory planning. Firstly, we introduce a
dual-encoder based transformer model to establish a good preliminary
trajectory. This trajectory is subsequently refined through sequential
quadratic programming to improve its optimality and robustness. Our approach
outperforms the state-of-the-art by up to 79.72\% in reducing trajectory
planning time. Compared with existing methods, our method shrinks the
optimality gap with the objective function value decreasing by up to 29.9\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn from Heterophily: Heterophilous Information-enhanced Graph Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zheng, Jiahao Xu, Lihui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under circumstances of heterophily, where nodes with different labels tend to
be connected based on semantic meanings, Graph Neural Networks (GNNs) often
exhibit suboptimal performance. Current studies on graph heterophily mainly
focus on aggregation calibration or neighbor extension and address the
heterophily issue by utilizing node features or structural information to
improve GNN representations. In this paper, we propose and demonstrate that the
valuable semantic information inherent in heterophily can be utilized
effectively in graph learning by investigating the distribution of neighbors
for each individual node within the graph. The theoretical analysis is carried
out to demonstrate the efficacy of the idea in enhancing graph learning. Based
on this analysis, we propose HiGNN, an innovative approach that constructs an
additional new graph structure, that integrates heterophilous information by
leveraging node distribution to enhance connectivity between nodes that share
similar semantic characteristics. We conduct empirical assessments on node
classification tasks using both homophilous and heterophilous benchmark
datasets and compare HiGNN to popular GNN baselines and SoTA methods,
confirming the effectiveness in improving graph representations. In addition,
by incorporating heterophilous information, we demonstrate a notable
enhancement in existing GNN-based approaches, and the homophily degree across
real-world datasets, thus affirming the efficacy of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models are Free Boosters for Biomedical Imaging Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Anna Hovakimyan, Naira Hovakimyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we uncover the unexpected efficacy of residual-based large
language models (LLMs) as part of encoders for biomedical imaging tasks, a
domain traditionally devoid of language or textual data. The approach diverges
from established methodologies by utilizing a frozen transformer block,
extracted from pre-trained LLMs, as an innovative encoder layer for the direct
processing of visual tokens. This strategy represents a significant departure
from the standard multi-modal vision-language frameworks, which typically hinge
on language-driven prompts and inputs. We found that these LLMs could boost
performance across a spectrum of biomedical imaging applications, including
both 2D and 3D visual classification tasks, serving as plug-and-play boosters.
More interestingly, as a byproduct, we found that the proposed framework
achieved superior performance, setting new state-of-the-art results on
extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we
aim to open new avenues for employing LLMs in biomedical imaging and enriching
the understanding of their potential in this specialized domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Pursuit of Fairness in Artificial Intelligence Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahsin Alamgir Kheya, Mohamed Reda Bouadjenek, Sunil Aryal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) models are now being utilized in all facets of
our lives such as healthcare, education and employment. Since they are used in
numerous sensitive environments and make decisions that can be life altering,
potential biased outcomes are a pressing matter. Developers should ensure that
such models don't manifest any unexpected discriminatory practices like
partiality for certain genders, ethnicities or disabled people. With the
ubiquitous dissemination of AI systems, researchers and practitioners are
becoming more aware of unfair models and are bound to mitigate bias in them.
Significant research has been conducted in addressing such issues to ensure
models don't intentionally or unintentionally perpetuate bias. This survey
offers a synopsis of the different ways researchers have promoted fairness in
AI systems. We explore the different definitions of fairness existing in the
current literature. We create a comprehensive taxonomy by categorizing
different types of bias and investigate cases of biased AI in different
application domains. A thorough study is conducted of the approaches and
techniques employed by researchers to mitigate bias in AI models. Moreover, we
also delve into the impact of biased models on user experience and the ethical
considerations to contemplate when developing and deploying such models. We
hope this survey helps researchers and practitioners understand the intricate
details of fairness and bias in AI systems. By sharing this thorough survey, we
aim to promote additional discourse in the domain of equitable and responsible
AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Support Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhoo Lee, Hyunho Lee, Kyomin Hwang, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the success of deep learning is commonly attributed to its theoretical
equivalence with Support Vector Machines (SVM), the practical implications of
this relationship have not been thoroughly explored. This paper pioneers an
exploration in this domain, specifically focusing on the identification of Deep
Support Vectors (DSVs) within deep learning models. We introduce the concept of
DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT)
conditions tailored for deep learning. Through empirical investigations, we
illustrate that DSVs exhibit similarities to support vectors in SVM, offering a
tangible method to interpret the decision-making criteria of models.
Additionally, our findings demonstrate that models can be effectively
reconstructed using DSVs, resembling the process in SVM. The code will be
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An optimal control perspective on diffusion-based generative modeling <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Berner, Lorenz Richter, Karen Ullrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a connection between stochastic optimal control and generative
models based on stochastic differential equations (SDEs), such as recently
developed diffusion probabilistic models. In particular, we derive a
Hamilton-Jacobi-Bellman equation that governs the evolution of the
log-densities of the underlying SDE marginals. This perspective allows to
transfer methods from optimal control theory to generative modeling. First, we
show that the evidence lower bound is a direct consequence of the well-known
verification theorem from control theory. Further, we can formulate
diffusion-based generative modeling as a minimization of the Kullback-Leibler
divergence between suitable measures in path space. Finally, we develop a novel
diffusion-based method for sampling from unnormalized densities -- a problem
frequently occurring in statistics and computational sciences. We demonstrate
that our time-reversed diffusion sampler (DIS) can outperform other
diffusion-based sampling approaches on multiple numerical examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at NeurIPS 2022 Workshop on
  Score-Based Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Independent Communication in Multi-Agent Reinforcement Learning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Pina, Varuna De Silva, Corentin Artaud, Xiaolan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper appearing on AAMAS 2024 with the same
  title. 11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A randomized algorithm for nonconvex minimization with inexact
  evaluations and complexity guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyao Li, Stephen J. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider minimization of a smooth nonconvex function with inexact oracle
access to gradient and Hessian (without assuming access to the function value)
to achieve approximate second-order optimality. A novel feature of our method
is that if an approximate direction of negative curvature is chosen as the
step, we choose its sense to be positive or negative with equal probability. We
allow gradients to be inexact in a relative sense and relax the coupling
between inexactness thresholds for the first- and second-order optimality
conditions. Our convergence analysis includes both an expectation bound based
on martingale analysis and a high-probability bound based on concentration
inequalities. We apply our algorithm to empirical risk minimization problems
and obtain improved gradient sample complexity over existing works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Borrowing Treasures from Neighbors: In-Context Learning for Multimodal
  Learning with Missing Modalities and Data Scarcity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal machine learning with missing modalities is an increasingly
relevant challenge arising in various applications such as healthcare. This
paper extends the current research into missing modalities to the low-data
regime, i.e., a downstream task has both missing modalities and limited sample
size issues. This problem setting is particularly challenging and also
practical as it is often expensive to get full-modality data and sufficient
annotated training samples. We propose to use retrieval-augmented in-context
learning to address these two crucial issues by unleashing the potential of a
transformer's in-context learning ability. Diverging from existing methods,
which primarily belong to the parametric paradigm and often require sufficient
training samples, our work exploits the value of the available full-modality
data, offering a novel perspective on resolving the challenge. The proposed
data-dependent framework exhibits a higher degree of sample efficiency and is
empirically demonstrated to enhance the classification model's performance on
both full- and missing-modality data in the low-data regime across various
multimodal learning tasks. When only 1% of the training data are available, our
proposed method demonstrates an average improvement of 6.1% over a recent
strong baseline across various datasets and missing states. Notably, our method
also reduces the performance gap between full-modality and missing-modality
data compared with the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistically Rewired Message-Passing Neural Networks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chendi Qian, Andrei Manolache, Kareem Ahmed, Zhe Zeng, Guy Van den Broeck, Mathias Niepert, Christopher Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-passing graph neural networks (MPNNs) emerged as powerful tools for
processing graph-structured input. However, they operate on a fixed input graph
structure, ignoring potential noise and missing information. Furthermore, their
local aggregation mechanism can lead to problems such as over-squashing and
limited expressive power in capturing relevant graph structures. Existing
solutions to these challenges have primarily relied on heuristic methods, often
disregarding the underlying data distribution. Hence, devising principled
approaches for learning to infer graph structures relevant to the given
prediction task remains an open challenge. In this work, leveraging recent
progress in exact and differentiable $k$-subset sampling, we devise
probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges
while omitting less beneficial ones. For the first time, our theoretical
analysis explores how PR-MPNNs enhance expressive power, and we identify
precise conditions under which they outperform purely randomized approaches.
Empirically, we demonstrate that our approach effectively mitigates issues like
over-squashing and under-reaching. In addition, on established real-world
datasets, our method exhibits competitive or superior predictive performance
compared to traditional MPNN models and recent graph transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Data Splitting in Distributed Optimization for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Medyakov, Gleb Molodtsov, Aleksandr Beznosikov, Alexander Gasnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distributed optimization problem has become increasingly relevant
recently. It has a lot of advantages such as processing a large amount of data
in less time compared to non-distributed methods. However, most distributed
approaches suffer from a significant bottleneck - the cost of communications.
Therefore, a large amount of research has recently been directed at solving
this problem. One such approach uses local data similarity. In particular,
there exists an algorithm provably optimally exploiting the similarity
property. But this result, as well as results from other works solve the
communication bottleneck by focusing only on the fact that communication is
significantly more expensive than local computing and does not take into
account the various capacities of network devices and the different
relationship between communication time and local computing expenses. We
consider this setup and the objective of this study is to achieve an optimal
ratio of distributed data between the server and local machines for any costs
of communications and local computations. The running times of the network are
compared between uniform and optimal distributions. The superior theoretical
performance of our solutions is experimentally validated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growing concerns about safety and alignment of AI systems highlight the
importance of embedding moral capabilities in artificial agents. A promising
solution is the use of learning from experience, i.e., Reinforcement Learning.
In multi-agent (social) environments, complex population-level phenomena may
emerge from interactions between individual learning agents. Many of the
existing studies rely on simulated social dilemma environments to study the
interactions of independent learning agents. However, they tend to ignore the
moral heterogeneity that is likely to be present in societies of agents in
practice. For example, at different points in time a single learning agent may
face opponents who are consequentialist (i.e., caring about maximizing some
outcome over time) or norm-based (i.e., focusing on conforming to a specific
norm here and now). The extent to which agents' co-development may be impacted
by such moral heterogeneity in populations is not well understood. In this
paper, we present a study of the learning dynamics of morally heterogeneous
populations interacting in a social dilemma setting. Using a Prisoner's Dilemma
environment with a partner selection mechanism, we investigate the extent to
which the prevalence of diverse moral agents in populations affects individual
agents' learning behaviors and emergent population-level outcomes. We observe
several types of non-trivial interactions between pro-social and anti-social
agents, and find that certain classes of moral agents are able to steer selfish
agents towards more cooperative behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Room Transfer Function Reconstruction Using Complex-valued Neural
  Networks and Irregularly Distributed Microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several impor- tant real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of measurements at scattered points in the room. In this paper, we employ
complex-valued neural networks to estimate room transfer functions in the
frequency range of the first room resonances, using a few irregularly
distributed microphones. To the best of our knowledge, this is the first time
that complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex- valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
kernel-based signal processing approach for sound field reconstruction, showing
that the proposed technique exhibits relevant advantages in terms of phase
accuracy and overall quality of the reconstructed sound field. For informative
purposes, we also compare the model with a similarly-structured data-driven
approach that, however, applies a real-valued neural network to reconstruct
only the magnitude of the sound field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activations and Gradients Compression for Model-Parallel Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Rudakov, Aleksandr Beznosikov, Yaroslav Kholodov, Alexander Gasnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large neural networks require enormous computational clusters of machines.
Model-parallel training, when the model architecture is partitioned
sequentially between workers, is a popular approach for training modern models.
Information compression can be applied to decrease workers communication time,
as it is often a bottleneck in such systems. This work explores how
simultaneous compression of activations and gradients in model-parallel
distributed training setup affects convergence. We analyze compression methods
such as quantization and TopK compression, and also experiment with error
compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error
feedback approach. We conduct experiments on image classification and language
model fine-tuning tasks. Our findings demonstrate that gradients require milder
compression rates than activations. We observe that $K=10\%$ is the lowest TopK
compression level, which does not harm model convergence severely. Experiments
also show that models trained with TopK perform well only when compression is
also applied during inference. We find that error feedback techniques do not
improve model-parallel training compared to plain compression, but allow model
inference without compression with almost no quality drop. Finally, when
applied with the AQ-SGD approach, TopK stronger than with $ K=30\%$ worsens
model performance significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially private multivariate medians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly Ramsay, Aukosh Jagannath, Shoja'eddin Chenouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical tools which satisfy rigorous privacy guarantees are necessary for
modern data analysis. It is well-known that robustness against contamination is
linked to differential privacy. Despite this fact, using multivariate medians
for differentially private and robust multivariate location estimation has not
been systematically studied. We develop novel finite-sample performance
guarantees for differentially private multivariate depth-based medians, which
are essentially sharp. Our results cover commonly used depth functions, such as
the halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.
We show that under Cauchy marginals, the cost of heavy-tailed location
estimation outweighs the cost of privacy. We demonstrate our results
numerically using a Gaussian contamination model in dimensions up to d = 100,
and compare them to a state-of-the-art private mean estimation algorithm. As a
by-product of our investigation, we prove concentration inequalities for the
output of the exponential mechanism about the maximizer of the population
objective function. This bound applies to objective functions that satisfy a
mild regularity condition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINN surrogate of Li-ion battery models for parameter inference. Part
  II: Regularization and application of the pseudo-2D model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian parameter inference is useful to improve Li-ion battery diagnostics
and can help formulate battery aging models. However, it is computationally
intensive and cannot be easily repeated for multiple cycles, multiple operating
conditions, or multiple replicate cells. To reduce the computational cost of
Bayesian calibration, numerical solvers for physics-based models can be
replaced with faster surrogates. A physics-informed neural network (PINN) is
developed as a surrogate for the pseudo-2D (P2D) battery model calibration. For
the P2D surrogate, additional training regularization was needed as compared to
the PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and
P2D surrogate models are exercised for parameter inference and compared to data
obtained from a direct numerical solution of the governing equations. A
parameter inference study highlights the ability to use these PINNs to
calibrate scaling parameters for the cathode Li diffusion and the anode
exchange current density. By realizing computational speed-ups of 2250x for the
P2D model, as compared to using standard integrating methods, the PINN
surrogates enable rapid state-of-health diagnostics. In the low-data
availability scenario, the testing error was estimated to 2mV for the SPM
surrogate and 10mV for the P2D surrogate which could be mitigated with
additional data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for AI policy act, if designed by the governments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINN surrogate of Li-ion battery models for parameter inference. Part I:
  Implementation and multi-fidelity hierarchies for the single-particle model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To plan and optimize energy storage demands that account for Li-ion battery
aging dynamics, techniques need to be developed to diagnose battery internal
states accurately and rapidly. This study seeks to reduce the computational
resources needed to determine a battery's internal states by replacing
physics-based Li-ion battery models -- such as the single-particle model (SPM)
and the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN)
surrogate. The surrogate model makes high-throughput techniques, such as
Bayesian calibration, tractable to determine battery internal parameters from
voltage responses. This manuscript is the first of a two-part series that
introduces PINN surrogates of Li-ion battery models for parameter inference
(i.e., state-of-health diagnostics). In this first part, a method is presented
for constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical
training, where several neural nets are trained with multiple physics-loss
fidelities is shown to significantly improve the surrogate accuracy when only
training on the governing equation residuals. The implementation is made
available in a companion repository (https://github.com/NREL/pinnstripes). The
techniques used to develop a PINN surrogate of the SPM are extended in Part II
for the PINN surrogate for the P2D battery model, and explore the Bayesian
calibration capabilities of both surrogates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling the Spectral Properties of the Hodge Laplacian: Not All
  Small Eigenvalues Are Equal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent P. Grande, Michael T. Schaub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rich spectral information of the graph Laplacian has been instrumental in
graph theory, machine learning, and graph signal processing for applications
such as graph classification, clustering, or eigenmode analysis. Recently, the
Hodge Laplacian has come into focus as a generalisation of the ordinary
Laplacian for higher-order graph models such as simplicial and cellular
complexes. Akin to the traditional analysis of graph Laplacians, many authors
analyse the smallest eigenvalues of the Hodge Laplacian, which are connected to
important topological properties such as homology. However, small eigenvalues
of the Hodge Laplacian can carry different information depending on whether
they are related to curl or gradient eigenmodes, and thus may not be
comparable. We therefore introduce the notion of persistent eigenvector
similarity and provide a method to track individual harmonic, curl, and
gradient eigenvectors/-values through the so-called persistence filtration,
leveraging the full information contained in the Hodge-Laplacian spectrum
across all possible scales of a point cloud. Finally, we use our insights (a)
to introduce a novel form of Hodge spectral clustering and (b) to classify
edges and higher-order simplices based on their relationship to the smallest
harmonic, curl, and gradient eigenvectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Crowd Counting from Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Duan, Fan Wan, Rui Sun, Zeyu Wang, Varun Ojha, Yu Guan, Hubert P. H. Shum, Bingzhang Hu, Yang Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Crowd behavior analysis can be applied to effectively help the
daily transportation statistics and planning, which helps the smart city
construction. As one of the most important keys, crowd counting has drawn
increasing attention. Recent works achieved promising performance but relied on
the supervised paradigm with expensive crowd annotations. To alleviate the
annotation cost in real-world transportation scenarios, in this work we
proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can
leverage both unlabeled/labeled data for robust crowd counting. In the
unsupervised pathway, two \textit{self-supervised losses} were proposed to
simulate the crowd variations such as scale, illumination, based on which
supervised information pseudo labels were generated and gradually refined. We
also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit
(GCRU)}, which can preserve discriminant crowd information by extracting
second-order statistics, yielding pseudo labels with improved quality. A joint
loss including both unsupervised/supervised information was proposed, and a
dynamic weighting strategy was employed to balance the importance of the
unsupervised loss and supervised loss at different training stages. We
conducted extensive experiments on four popular crowd counting datasets in
semi-supervised settings. Experimental results supported the effectiveness of
each proposed component in our $S^{4}$Crowd framework. Our method achieved
competitive performance in semi-supervised learning approaches on these crowd
counting datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Theory of Causation for Interpreting Neural Code Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03788v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03788v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly
progressing from research prototypes to commercial developer tools. As such,
understanding the capabilities and limitations of such models is becoming
critical. However, the abilities of these models are typically measured using
automated metrics that often only reveal a portion of their real-world
performance. While, in general, the performance of NCMs appears promising,
currently much is unknown about how such models arrive at decisions. To this
end, this paper introduces $do_{code}$, a post hoc interpretability method
specific to NCMs that is capable of explaining model predictions. $do_{code}$
is based upon causal inference to enable programming language-oriented
explanations. While the theoretical underpinnings of $do_{code}$ are extensible
to exploring different model properties, we provide a concrete instantiation
that aims to mitigate the impact of spurious correlations by grounding
explanations of model behavior in properties of programming languages. To
demonstrate the practical benefit of $do_{code}$, we illustrate the insights
that our framework can provide by performing a case study on two popular deep
learning architectures and ten NCMs. The results of this case study illustrate
that our studied NCMs are sensitive to changes in code syntax. All our NCMs,
except for the BERT-like model, statistically learn to predict tokens related
to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding
bias as compared to other programming language constructs. These insights
demonstrate the potential of $do_{code}$ as a useful method to detect and
facilitate the elimination of confounding bias in NCMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in IEEE Transactions on Software Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15909v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15909v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Bhatia, Samer B. Nashed, Shlomo Zilberstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as
promising approaches for learning data-efficient RL algorithms tailored to a
given task distribution. However, they show poor asymptotic performance and
struggle with out-of-distribution tasks because they rely on sequence models,
such as recurrent neural networks or transformers, to process experiences
rather than summarize them using general-purpose RL components such as value
functions. In contrast, traditional RL algorithms are data-inefficient as they
do not use domain knowledge, but they do converge to an optimal policy in the
limit. We propose RL$^3$, a principled hybrid approach that incorporates
action-values, learned per task through traditional RL, in the inputs to
meta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,
compared to RL$^2$, while maintaining data-efficiency in the short term, and
generalizes better to out-of-distribution tasks. Experiments are conducted on
both custom and benchmark discrete domains from the meta-RL literature that
exhibit a range of short-term, long-term, and complex dependencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Optimization for Sparse Deep Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. S. Hotegni, M. Berkemeier, S. Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different conflicting optimization criteria arise naturally in various Deep
Learning scenarios. These can address different main tasks (i.e., in the
setting of Multi-Task Learning), but also main and secondary tasks such as loss
minimization versus sparsity. The usual approach is a simple weighting of the
criteria, which formally only works in the convex setting. In this paper, we
present a Multi-Objective Optimization algorithm using a modified Weighted
Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect
to several tasks. By employing this scalarization technique, the algorithm can
identify all optimal solutions of the original problem while reducing its
complexity to a sequence of single-objective problems. The simplified problems
are then solved using an Augmented Lagrangian method, enabling the use of
popular optimization techniques such as Adam and Stochastic Gradient Descent,
while efficaciously handling constraints. Our work aims to address the
(economical and also ecological) sustainability issue of DNN models, with a
particular focus on Deep Multi-Task models, which are typically designed with a
very large number of weights to perform equally well on multiple tasks. Through
experiments conducted on two Machine Learning datasets, we demonstrate the
possibility of adaptively sparsifying the model during training without
significantly impacting its performance, if we are willing to apply
task-specific adaptations to the network weights. Code is available at
https://github.com/salomonhotegni/MDMTN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial Neural Nets and the Representation of Human Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Freiesleben
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What do artificial neural networks (ANNs) learn? The machine learning (ML)
community shares the narrative that ANNs must develop abstract human concepts
to perform complex tasks. Some go even further and believe that these concepts
are stored in individual units of the network. Based on current research, I
systematically investigate the assumptions underlying this narrative. I
conclude that ANNs are indeed capable of performing complex prediction tasks,
and that they may learn human and non-human concepts to do so. However,
evidence indicates that ANNs do not represent these concepts in individual
units.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For: Philosophy of Science for Machine Learning: Core Issues and New
  Perspectives, edited by Juan Duran and Giorgia Pozzi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Conic Proxies for AC Optimal Power Flow <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guancheng Qiu, Mathieu Tanneau, Pascal Van Hentenryck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been significant interest in the development of
machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF).
Although significant progress has been achieved in predicting high-quality
primal solutions, no existing learning-based approach can provide valid dual
bounds for AC-OPF. This paper addresses this gap by training optimization
proxies for a convex relaxation of AC-OPF. Namely, the paper considers a
second-order cone (SOC) relaxation of AC-OPF, and proposes \revision{a novel
architecture} that embeds a fast, differentiable (dual) feasibility recovery,
thus providing valid dual bounds. The paper combines this new architecture with
a self-supervised learning scheme, which alleviates the need for costly
training data generation. Extensive numerical experiments on medium- and
large-scale power grids demonstrate the efficiency and scalability of the
proposed methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to PSCC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonic Control Lyapunov Barrier Functions for Constrained Optimal
  Control with Reach-Avoid Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Mukherjee, Ruikun Zhou, Haocheng Chang, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces harmonic control Lyapunov barrier functions (harmonic
CLBF) that aid in constrained control problems such as reach-avoid problems.
Harmonic CLBFs exploit the maximum principle that harmonic functions satisfy to
encode the properties of control Lyapunov barrier functions (CLBFs). As a
result, they can be initiated at the start of an experiment rather than trained
based on sample trajectories. The control inputs are selected to maximize the
inner product of the system dynamics with the steepest descent direction of the
harmonic CLBF. Numerical results are presented with four different systems
under different reach-avoid environments. Harmonic CLBFs show a significantly
low risk of entering unsafe regions and a high probability of entering the goal
region.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Feature and Model Importance in Android Malware Detection:
  An Implemented <span class="highlight-title">Survey</span> and Experimental Comparison of ML-Based Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Muzaffar, Hani Ragab Hassen, Hind Zantout, Michael A Lones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of Android means it is a common target for malware. Over the
years, various studies have found that machine learning models can effectively
discriminate malware from benign applications. However, as the operating system
evolves, so does malware, bringing into question the findings of these previous
studies, many of which report very high accuracies using small, outdated, and
often imbalanced datasets. In this paper, we reimplement 18 representative past
works and reevaluate them using a balanced, relevant, and up-to-date dataset
comprising 124,000 applications. We also carry out new experiments designed to
fill holes in existing knowledge, and use our findings to identify the most
effective features and models to use for Android malware detection within a
contemporary environment. We show that high detection accuracies (up to 96.8%)
can be achieved using features extracted through static analysis alone,
yielding a modest benefit (1%) from using far more expensive dynamic analysis.
API calls and opcodes are the most productive static and TCP network traffic
provide the most predictive dynamic features. Random forests are generally the
most effective model, outperforming more complex deep learning approaches.
Whilst directly combining static and dynamic features is generally ineffective,
ensembling models separately leads to performances comparable to the best
models but using less brittle features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Search of a Data Transformation That Accelerates Neural Field
  Training <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Seo, Sangyoon Lee, Kwang In Kim, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural field is an emerging paradigm in data representation that trains a
neural network to approximate the given signal. A key obstacle that prevents
its widespread adoption is the encoding speed-generating neural fields requires
an overfitting of a neural network, which can take a significant number of SGD
steps to reach the desired fidelity level. In this paper, we delve into the
impacts of data transformations on the speed of neural field training,
specifically focusing on how permuting pixel locations affect the convergence
speed of SGD. Counterintuitively, we find that randomly permuting the pixel
locations can considerably accelerate the training. To explain this phenomenon,
we examine the neural field training through the lens of PSNR curves, loss
landscapes, and error patterns. Our analyses suggest that the random pixel
permutations remove the easy-to-fit patterns, which facilitate easy
optimization in the early stage but hinder capturing fine details of the
signal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Randomization via Entropy Maximization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Tiboni, Pascal Klink, Jan Peters, Tatiana Tommasi, Carlo D'Eramo, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Varying dynamics parameters in simulation is a popular Domain Randomization
(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).
Nevertheless, DR heavily hinges on the choice of the sampling distribution of
the dynamics parameters, since high variability is crucial to regularize the
agent's behavior but notoriously leads to overly conservative policies when
randomizing excessively. In this paper, we propose a novel approach to address
sim-to-real transfer, which automatically shapes dynamics distributions during
training in simulation without requiring real-world data. We introduce DOmain
RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization
problem that directly maximizes the entropy of the training distribution while
retaining generalization capabilities. In achieving this, DORAEMON gradually
increases the diversity of sampled dynamics parameters as long as the
probability of success of the current policy is sufficiently high. We
empirically validate the consistent benefits of DORAEMON in obtaining highly
adaptive and generalizable policies, i.e. solving the task at hand across the
widest range of dynamics parameters, as opposed to representative baselines
from the DR literature. Notably, we also demonstrate the Sim2Real applicability
of DORAEMON through its successful zero-shot transfer in a robotic manipulation
setup under unknown real-world parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024. Project website at
  https://gabrieletiboni.github.io/doraemon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discretized Distributed Optimization over Dynamic Digraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Doostmohammadian, Wei Jiang, Muwahida Liaquat, Alireza Aghasi, Houman Zarrabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a discrete-time model of continuous-time distributed optimization
over dynamic directed-graphs (digraphs) with applications to distributed
learning. Our optimization algorithm works over general strongly connected
dynamic networks under switching topologies, e.g., in mobile multi-agent
systems and volatile networks due to link failures. Compared to many existing
lines of work, there is no need for bi-stochastic weight designs on the links.
The existing literature mostly needs the link weights to be stochastic using
specific weight-design algorithms needed both at the initialization and at all
times when the topology of the network changes. This paper eliminates the need
for such algorithms and paves the way for distributed optimization over
time-varying digraphs. We derive the bound on the gradient-tracking step-size
and discrete time-step for convergence and prove dynamic stability using
arguments from consensus algorithms, matrix perturbation theory, and Lyapunov
theory. This work, particularly, is an improvement over existing
stochastic-weight undirected networks in case of link removal or packet drops.
This is because the existing literature may need to rerun time-consuming and
computationally complex algorithms for stochastic design, while the proposed
strategy works as long as the underlying network is weight-symmetric and
balanced. The proposed optimization framework finds applications to distributed
classification and learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPR: Continual Learning Human Preference through Optimal Policy
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15694v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15694v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Regularization (COPR), in which we compute the distribution of optimal
policy bypassing the partition function and then regularize the current policy
based on the historically optimal distribution to mitigate Catastrophic
Forgetting (CF). COPR involves a single learning phase and doesn't necessitate
complex reinforcement learning. Importantly, it shares the capability with RLHF
to learn from unlabeled data by maintaining a scoring module, similar to reward
model, making it flexible for continually learning without human feedback. Our
experimental results show that COPR outperforms strong Continuous Learning (CL)
baselines when it comes to consistently aligning with human preferences on
incremental tasks and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SD4Match: Learning to <span class="highlight-title">Prompt</span> Stable Diffusion Model for Semantic
  Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of matching semantically similar
keypoints across image pairs. Existing research indicates that the intermediate
output of the UNet within the Stable Diffusion (SD) can serve as robust image
feature maps for such a matching task. We demonstrate that by employing a basic
prompt tuning technique, the inherent potential of Stable Diffusion can be
harnessed, resulting in a significant enhancement in accuracy over previous
approaches. We further introduce a novel conditional prompting module that
conditions the prompt on the local details of the input image pairs, leading to
a further improvement in performance. We designate our approach as SD4Match,
short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of
SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets
new benchmarks in accuracy across all these datasets. Particularly, SD4Match
outperforms the previous state-of-the-art by a margin of 12 percentage points
on the challenging SPair-71k dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project website:
  https://sd4match.active.vision/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Linear Subspace Identification: A Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03197v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03197v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loris Di Natale, Muhammad Zakwan, Bratislav Svetozarevic, Philipp Heer, Giancarlo Ferrari-Trecate, Colin N. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) and linear System Identification (SI) have been
historically developed independently. In this paper, we leverage
well-established ML tools - especially the automatic differentiation framework
- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space
SI methods using backpropagation. SIMBa relies on a novel
Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure
the stability of the identified model.
  We show how SIMBa generally outperforms traditional linear state-space SI
methods, and sometimes significantly, although at the price of a higher
computational burden. This performance gap is particularly remarkable compared
to other SI methods with stability guarantees, where the gain is frequently
above 25% in our investigations, hinting at SIMBa's ability to simultaneously
achieve state-of-the-art fitting performance and enforce stability.
Interestingly, these observations hold for a wide variety of input-output
systems and on both simulated and real-world data, showcasing the flexibility
of the proposed approach. We postulate that this new SI paradigm presents a
great extension potential to identify structured nonlinear models from data,
and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RetroBridge: Modeling Retrosynthesis with Markov Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilia Igashov, Arne Schneuing, Marwin Segler, Michael Bronstein, Bruno Correia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrosynthesis planning is a fundamental challenge in chemistry which aims at
designing reaction pathways from commercially available starting materials to a
target molecule. Each step in multi-step retrosynthesis planning requires
accurate prediction of possible precursor molecules given the target molecule
and confidence estimates to guide heuristic search algorithms. We model
single-step retrosynthesis planning as a distribution learning problem in a
discrete state space. First, we introduce the Markov Bridge Model, a generative
framework aimed to approximate the dependency between two intractable discrete
distributions accessible via a finite sample of coupled data points. Our
framework is based on the concept of a Markov bridge, a Markov process pinned
at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does
not need a tractable noise distribution as a sampling proxy and directly
operates on the input product molecules as samples from the intractable prior
distribution. We then address the retrosynthesis planning problem with our
novel framework and introduce RetroBridge, a template-free retrosynthesis
modeling approach that achieves state-of-the-art results on standard evaluation
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChIRAAG: Chat<span class="highlight-title">GPT</span> Informed Rapid and Automated Assertion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhabesh Mali, Karthik Maddala, Sweeya Reddy, Vatsal Gupta, Chandan Karfa, Ramesh Karri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System Verilog Assertion (SVA) formulation- a critical yet complex task is a
prerequisite in the Formal Property Verification (FPV) process. Traditionally,
SVA formulation involves expert-driven interpretation of specifications, which
is timeconsuming and prone to human error. However, LLM-informed automatic
assertion generation is gaining interest. We designeda novel framework called
ChIRAAG, based on OpenAI GPT4, to generate SVA assertions from natural language
specifications. ChIRAAG constitutes the systematic breakdown of design
specifications into a standardized format, further generating assertions from
formatted specifications using LLM. Furthermore, we developed testbenches to
verify/validate the LLM-generated assertions. Automatic feedback of log files
from the simulation tool to the LLM ensures that the framework can generate
correc SVAs automatically. Only 33% of LLM-generated raw assertions had errors.
Our results on OpenTitan designs shows that LLMs can streamline and assist
engineers in the assertion generation process, reshaping verification
workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures and 2 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The personalization of machine learning (ML) models to address data drift is
a significant challenge in the context of Internet of Things (IoT)
applications. Presently, most approaches focus on fine-tuning either the full
base model or its last few layers to adapt to new data, while often neglecting
energy costs. However, various types of data drift exist, and fine-tuning the
full base model or the last few layers may not result in optimal performance in
certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy
adaptive personalization framework designed for resource-constrained devices.
We categorize data drift and personalization into three types: input-level,
feature-level, and output-level. For each type, we fine-tune different blocks
of the model to achieve optimal performance with reduced energy costs.
Specifically, input-, feature-, and output-level correspond to fine-tuning the
front, middle, and rear blocks of the model. We evaluate TBFT on a ResNet
model, three datasets, three different training sizes, and a Raspberry Pi.
Compared with the $Block Avg$, where each block is fine-tuned individually and
their performance improvements are averaged, TBFT exhibits an improvement in
model accuracy by an average of 15.30% whilst saving 41.57% energy consumption
on average compared with full fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedCSD: A Federated Learning Based Approach for Code-Smell Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadi Alawadi, Khalid Alkharabsheh, Fahed Alkhabbas, Victor Kebande, Feras M. Awaysheh, Fabio Palomba, Mohammed Awad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a Federated Learning Code Smell Detection (FedCSD)
approach that allows organizations to collaboratively train federated ML models
while preserving their data privacy. These assertions have been supported by
three experiments that have significantly leveraged three manually validated
datasets aimed at detecting and examining different code smell scenarios. In
experiment 1, which was concerned with a centralized training experiment,
dataset two achieved the lowest accuracy (92.30%) with fewer smells, while
datasets one and three achieved the highest accuracy with a slight difference
(98.90% and 99.5%, respectively). This was followed by experiment 2, which was
concerned with cross-evaluation, where each ML model was trained using one
dataset, which was then evaluated over the other two datasets. Results from
this experiment show a significant drop in the model's accuracy (lowest
accuracy: 63.80\%) where fewer smells exist in the training dataset, which has
a noticeable reflection (technical debt) on the model's performance. Finally,
the last and third experiments evaluate our approach by splitting the dataset
into 10 companies. The ML model was trained on the company's site, then all
model-updated weights were transferred to the server. Ultimately, an accuracy
of 98.34% was achieved by the global model that has been trained using 10
companies for 100 training rounds. The results reveal a slight difference in
the global model's accuracy compared to the highest accuracy of the centralized
model, which can be ignored in favour of the global model's comprehensive
knowledge, lower training cost, preservation of data privacy, and avoidance of
the technical debt problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, Journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedCau: A Proactive Stop Policy for Communication and Computation
  Efficient Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afsaneh Mahmoudi, Hossein S. Ghadikolaei, José Mairton Barros Da Silva Júnior, Carlo Fischione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates efficient distributed training of a Federated
Learning~(FL) model over a wireless network of wireless devices. The
communication iterations of the distributed training algorithm may be
substantially deteriorated or even blocked by the effects of the devices'
background traffic, packet losses, congestion, or latency. We abstract the
communication-computation impacts as an `iteration cost' and propose a
cost-aware causal FL algorithm~(FedCau) to tackle this problem. We propose an
iteration-termination method that trade-offs the training performance and
networking costs. We apply our approach when clients use the slotted-ALOHA, the
carrier-sense multiple access with collision avoidance~(CSMA/CA), and the
orthogonal frequency-division multiple access~(OFDMA) protocols. We show that,
given a total cost budget, the training performance degrades as either the
background communication traffic or the dimension of the training problem
increases. Our results demonstrate the importance of proactively designing
optimal cost-efficient stopping criteria to avoid unnecessary
communication-computation costs to achieve only a marginal FL training
improvement. We validate our method by training and testing FL over the MNIST
dataset. Finally, we apply our approach to existing communication efficient FL
methods from the literature, achieving further efficiency. We conclude that
cost-efficient stopping criteria are essential for the success of practical FL
over wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamComposer: Controllable 3D Object Generation via Multi-View
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yhyang-myron.github.io/DreamComposer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transport meets Variational Inference: Controlled Monte Carlo Diffusions <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01050v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01050v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Vargas, Shreyas Padhy, Denis Blessing, Nikolas Nüsken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connecting optimal transport and variational inference, we present a
principled and systematic framework for sampling and generative modelling
centred around divergences on path space. Our work culminates in the
development of the \emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for
Bayesian computation, a score-based annealing technique that crucially adapts
both forward and backward dynamics in a diffusion model. On the way, we clarify
the relationship between the EM-algorithm and iterative proportional fitting
(IPF) for Schr{\"o}dinger bridges, deriving as well a regularised objective
that bypasses the iterative bottleneck of standard IPF-updates. Finally, we
show that CMCD has a strong foundation in the Jarzinsky and Crooks identities
from statistical physics, and that it convincingly outperforms competing
approaches across a wide array of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on New Frontiers in Learning, Control, and Dynamical Systems
  at the International Conference on Machine Learning (ICML), Honolulu, Hawaii,
  USA, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P2ANet: A <span class="highlight-title">Dataset</span> and Benchmark for Dense Action Detection from Table
  Tennis Match Broadcasting Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Bian, Xuhong Li, Tao Wang, Qingzhong Wang, Jun Huang, Chen Liu, Jun Zhao, Feixiang Lu, Dejing Dou, Haoyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has been widely used for video analytics, such as video
classification and action detection, dense action detection with fast-moving
subjects from sports videos is still challenging. In this work, we release yet
another sports video benchmark \TheName{} for \emph{\underline{P}}ing
\emph{\underline{P}}ong-\emph{\underline{A}}ction detection, which consists of
2,721 video clips collected from the broadcasting videos of professional table
tennis matches in World Table Tennis Championships and Olympiads. We work with
a crew of table tennis professionals and referees on a specially designed
annotation toolbox to obtain fine-grained action labels (in 14 classes) for
every ping-pong action that appeared in the dataset, and formulate two sets of
action detection problems -- \emph{action localization} and \emph{action
recognition}. We evaluate a number of commonly-seen action recognition (e.g.,
TSM, TSN, Video SwinTransformer, and Slowfast) and action localization models
(e.g., BSN, BSN++, BMN, TCANet), using \TheName{} for both problems, under
various settings. These models can only achieve 48\% area under the AR-AN curve
for localization and 82\% top-one accuracy for recognition since the ping-pong
actions are dense with fast-moving subjects but broadcasting videos are with
only 25 FPS. The results confirm that \TheName{} is still a challenging task
and can be used as a special benchmark for dense action detection from videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Implicit GNN Solver for Poisson-like problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10891v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10891v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Nastorg, Michele Alessandro Bucci, Thibault Faney, Jean-Marc Gratien, Guillaume Charpiat, Marc Schoenauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents $\Psi$-GNN, a novel Graph Neural Network (GNN) approach
for solving the ubiquitous Poisson PDE problems with mixed boundary conditions.
By leveraging the Implicit Layer Theory, $\Psi$-GNN models an "infinitely" deep
network, thus avoiding the empirical tuning of the number of required Message
Passing layers to attain the solution. Its original architecture explicitly
takes into account the boundary conditions, a critical prerequisite for
physical applications, and is able to adapt to any initially provided solution.
$\Psi$-GNN is trained using a "physics-informed" loss, and the training process
is stable by design, and insensitive to its initialization. Furthermore, the
consistency of the approach is theoretically proven, and its flexibility and
generalization efficiency are experimentally demonstrated: the same learned
model can accurately handle unstructured meshes of various sizes, as well as
different boundary conditions. To the best of our knowledge, $\Psi$-GNN is the
first physics-informed GNN-based method that can handle various unstructured
domains, boundary conditions and initial solutions while also providing
convergence guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble learning for Physics Informed Neural Networks: a Gradient
  Boosting approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Fang, Sifan Wang, Paris Perdikaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the popularity of physics-informed neural networks (PINNs) is steadily
rising, to this date, PINNs have not been successful in simulating multi-scale
and singular perturbation problems. In this work, we present a new training
paradigm referred to as "gradient boosting" (GB), which significantly enhances
the performance of physics informed neural networks (PINNs). Rather than
learning the solution of a given PDE using a single neural network directly,
our algorithm employs a sequence of neural networks to achieve a superior
outcome. This approach allows us to solve problems presenting great challenges
for traditional PINNs. Our numerical experiments demonstrate the effectiveness
of our algorithm through various benchmarks, including comparisons with finite
element methods and PINNs. Furthermore, this work also unlocks the door to
employing ensemble learning techniques in PINNs, providing opportunities for
further improvement in solving PDEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian data-driven discovery of partial differential equations with
  variable coefficients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.01432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.01432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoxue Chen, Yifan Du, Liyao Mars Gao, Guang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of Partial Differential Equations (PDEs) is an essential task
for applied science and engineering. However, data-driven discovery of PDEs is
generally challenging, primarily stemming from the sensitivity of the
discovered equation to noise and the complexities of model selection. In this
work, we propose an advanced Bayesian sparse learning algorithm for PDE
discovery with variable coefficients, predominantly when the coefficients are
spatially or temporally dependent. Specifically, we apply threshold Bayesian
group Lasso regression with a spike-and-slab prior (tBGL-SS) and leverage a
Gibbs sampler for Bayesian posterior estimation of PDE coefficients. This
approach not only enhances the robustness of point estimation with valid
uncertainty quantification but also relaxes the computational burden from
Bayesian inference through the integration of coefficient thresholds as an
approximate MCMC method. Moreover, from the quantified uncertainties, we
propose a Bayesian total error bar criteria for model selection, which
outperforms classic metrics including the root mean square and the Akaike
information criterion. The capability of this method is illustrated by the
discovery of several classical benchmark PDEs with spatially or temporally
varying coefficients from solution data obtained from the reference
simulations. In the experiments, we show that the tBGL-SS method is more robust
than the baseline methods under noisy environments and provides better model
selection criteria along the regularization path.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Signal Diffusion Model for Collaborative Filtering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqin Zhu, Chao Wang, Qi Zhang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is a critical technique in recommender systems. Among
various methods, an increasingly popular paradigm is to reconstruct user-item
interactions based on the historical observations. This can be viewed as a
conditional generative task, where recently developed diffusion model
demonstrates great potential. However, existing studies on diffusion models
lack effective solutions for modeling implicit feedback data. Particularly, the
isotropic nature of the standard diffusion process fails to account for the
heterogeneous dependencies among items, leading to a misalignment with the
graphical structure of the interaction space. Meanwhile, random noise
destroying personalized information in interaction vectors, causing difficulty
in reverse reconstruction. In this paper, we make novel adaptions of diffusion
model and propose Graph Signal Diffusion Model for Collaborative Filtering
(named GiffCF). To better represent the high-dimensional and sparse
distribution of implicit feedback, we define a generalized form of denoising
diffusion using heat equation on the item-item similarity graph. Our forward
process smooths interaction signals with an advanced family of graph filters.
Hence, instead of losing information, it involves item-item similarities as
beneficial prior knowledge for recommendation. To reconstruct high-quality
interactions, our reverse process iteratively refines and sharpens preference
signals in a deterministic manner, where the update direction is conditioned on
the user history and computed from a carefully designed two-stage denoiser.
Finally, through extensive experiments, we show that GiffCF effectively
leverages the advantages of both diffusion model and graph signal processing,
and achieves state-of-the-art performance on three benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Riemannian Laplace Approximation with the Fisher Metric <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laplace's method approximates a target density with a Gaussian distribution
at its mode. It is computationally efficient and asymptotically exact for
Bayesian inference due to the Bernstein-von Mises theorem, but for complex
targets and finite-data posteriors it is often too crude an approximation. A
recent generalization of the Laplace Approximation transforms the Gaussian
approximation according to a chosen Riemannian geometry providing a richer
approximation family, while still retaining computational efficiency. However,
as shown here, its properties depend heavily on the chosen metric, indeed the
metric adopted in previous work results in approximations that are overly
narrow as well as being biased even at the limit of infinite data. We correct
this shortcoming by developing the approximation family further, deriving two
alternative variants that are exact at the limit of infinite data, extending
the theoretical analysis of the method, and demonstrating practical
improvements in a range of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2024, with additional fixes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISL: Fueling Research with A Large <span class="highlight-title">Dataset</span> of Solidity Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Morello, Mojtaba Eshghie, Sofia Bobadilla, Martin Monperrus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The DISL dataset features a collection of $514,506$ unique Solidity files
that have been deployed to Ethereum mainnet. It caters to the need for a large
and diverse dataset of real-world smart contracts. DISL serves as a resource
for developing machine learning systems and for benchmarking software
engineering tools designed for smart contracts. By aggregating every verified
smart contract from Etherscan up to January 15, 2024, DISL surpasses existing
datasets in size and recency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Autoencoders Are Robust Neural Architecture Search Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Hu, Xiangxiang Chu, Bo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) currently relies heavily on labeled data,
which is both expensive and time-consuming to acquire. In this paper, we
propose a novel NAS framework based on Masked Autoencoders (MAE) that
eliminates the need for labeled data during the search process. By replacing
the supervised learning objective with an image reconstruction task, our
approach enables the robust discovery of network architectures without
compromising performance and generalization ability. Additionally, we address
the problem of performance collapse encountered in the widely-used
Differentiable Architecture Search (DARTS) method in the unsupervised paradigm
by introducing a multi-scale decoder. Through extensive experiments conducted
on various search spaces and datasets, we demonstrate the effectiveness and
robustness of the proposed method, providing empirical evidence of its
superiority over baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07728v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07728v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Planning Diffusion: Learning and Planning of Robot Motions with
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Carvalho, An T. Le, Mark Baierl, Dorothea Koert, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning priors on trajectory distributions can help accelerate robot motion
planning optimization. Given previously successful plans, learning trajectory
generative models as priors for a new planning problem is highly desirable.
Prior works propose several ways on utilizing this prior to bootstrapping the
motion planning problem. Either sampling the prior for initializations or using
the prior distribution in a maximum-a-posterior formulation for trajectory
optimization. In this work, we propose learning diffusion models as priors. We
then can sample directly from the posterior trajectory distribution conditioned
on task goals, by leveraging the inverse denoising process of diffusion models.
Furthermore, diffusion has been recently shown to effectively encode data
multimodality in high-dimensional settings, which is particularly well-suited
for large trajectory dataset. To demonstrate our method efficacy, we compare
our proposed method - Motion Planning Diffusion - against several baselines in
simulated planar robot and 7-dof robot arm manipulator environments. To assess
the generalization capabilities of our method, we test it in environments with
previously unseen obstacles. Our experiments show that diffusion models are
strong priors to encode high-dimensional trajectory distributions of robot
motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Lightweight and Gradient-Stable Neural Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.04088v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.04088v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyao Yu, Yin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enhance resource efficiency and model deployability of neural networks, we
propose a neural-layer architecture based on Householder weighting and
absolute-value activating, called Householder-absolute neural layer or simply
Han-layer. Compared to a fully connected layer with $d$-neurons and $d$
outputs, a Han-layer reduces the number of parameters and the corresponding
computational complexity from $O(d^2)$ to $O(d)$. {The Han-layer structure
guarantees that the Jacobian of the layer function is always orthogonal, thus
ensuring gradient stability (i.e., free of gradient vanishing or exploding
issues) for any Han-layer sub-networks.} Extensive numerical experiments show
that one can strategically use Han-layers to replace fully connected (FC)
layers, reducing the number of model parameters while maintaining or even
improving the generalization performance. We will also showcase the
capabilities of the Han-layer architecture on a few small stylized models, and
discuss its current limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain Networks and Intelligence: A Graph Neural Network Based Approach
  to Resting State fMRI Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishal Thapaliya, Esra Akbas, Jiayu Chen, Raam Sapkota, Bhaskar Ray, Pranav Suresh, Vince Calhoun, Jingyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful
tool for investigating the relationship between brain function and cognitive
processes as it allows for the functional organization of the brain to be
captured without relying on a specific task or stimuli. In this paper, we
present a novel modeling architecture called BrainRGIN for predicting
intelligence (fluid, crystallized, and total intelligence) using graph neural
networks on rsfMRI derived static functional network connectivity matrices.
Extending from the existing graph convolution networks, our approach
incorporates a clustering-based embedding and graph isomorphism network in the
graph convolutional layer to reflect the nature of the brain sub-network
organization and efficient network expression, in combination with TopK pooling
and attention-based readout functions. We evaluated our proposed architecture
on a large dataset, specifically the Adolescent Brain Cognitive Development
Dataset, and demonstrated its effectiveness in predicting individual
differences in intelligence. Our model achieved lower mean squared errors and
higher correlation scores than existing relevant graph architectures and other
traditional machine learning models for all of the intelligence prediction
tasks. The middle frontal gyrus exhibited a significant contribution to both
fluid and crystallized intelligence, suggesting their pivotal role in these
cognitive processes. Total composite scores identified a diverse set of brain
regions to be relevant which underscores the complex nature of total
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All Rivers Run to the Sea: Private Learning with Asymmetric Flows <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Niu, Ramy E. Ali, Saurav Prakash, Salman Avestimehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data privacy is of great concern in cloud machine-learning service platforms,
when sensitive data are exposed to service providers. While private computing
environments (e.g., secure enclaves), and cryptographic approaches (e.g.,
homomorphic encryption) provide strong privacy protection, their computing
performance still falls short compared to cloud GPUs. To achieve privacy
protection with high computing performance, we propose Delta, a new private
training and inference framework, with comparable model performance as
non-private centralized training. Delta features two asymmetric data flows: the
main information-sensitive flow and the residual flow. The main part flows into
a small model while the residuals are offloaded to a large model. Specifically,
Delta embeds the information-sensitive representations into a low-dimensional
space while pushing the information-insensitive part into high-dimension
residuals. To ensure privacy protection, the low-dimensional
information-sensitive part is secured and fed to a small model in a private
environment. On the other hand, the residual part is sent to fast cloud GPUs,
and processed by a large model. To further enhance privacy and reduce the
communication cost, Delta applies a random binary quantization technique along
with a DP-based technique to the residuals before sharing them with the public
platform. We theoretically show that Delta guarantees differential privacy in
the public environment and greatly reduces the complexity in the private
environment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet
datasets and ResNet-18 and ResNet-34, showing that Delta achieves strong
privacy protection, fast training, and inference without significantly
compromising the model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready for CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein
  Classification in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Shan Lan, Pin-Yu Chen, Tsung-Yi Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein classification tasks are essential in drug discovery. Real-world
protein structures are dynamic, which will determine the properties of
proteins. However, the existing machine learning methods, like ProNet (Wang et
al., 2022a), only access limited conformational characteristics and protein
side-chain features, leading to impractical protein structure and inaccuracy of
protein classes in their predictions. In this paper, we propose novel semantic
data augmentation methods, Novel Augmentation of New Node Attributes (NaNa),
and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate
backbone chemical and side-chain biophysical information into protein
classification tasks and a co-embedding residual learning framework.
Specifically, we leverage molecular biophysical, secondary structure, chemical
bonds, and ionic features of proteins to facilitate protein classification
tasks. Furthermore, our semantic augmentation methods and the co-embedding
residual learning framework can improve the performance of GIN (Xu et al.,
2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41%
and 11.33% respectively. Our code is available at
https://github.com/r08b46009/Code_for_MIGU_NANA/tree/main.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Generation with $K^2$-trees <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19125v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19125v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhui Jang, Dongwoo Kim, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating graphs from a target distribution is a significant challenge
across many domains, including drug discovery and social network analysis. In
this work, we introduce a novel graph generation method leveraging $K^2$-tree
representation, originally designed for lossless graph compression. The
$K^2$-tree representation {encompasses inherent hierarchy while enabling
compact graph generation}. In addition, we make contributions by (1) presenting
a sequential $K^2$-treerepresentation that incorporates pruning, flattening,
and tokenization processes and (2) introducing a Transformer-based architecture
designed to generate the sequence by incorporating a specialized tree
positional encoding scheme. Finally, we extensively evaluate our algorithm on
four general and two molecular graph datasets to confirm its superiority for
graph generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple and Scalable Representation for Graph Generation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhui Jang, Seul Lee, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a surge of interest in employing neural networks for
graph generation, a fundamental statistical learning problem with critical
applications like molecule design and community analysis. However, most
approaches encounter significant limitations when generating large-scale
graphs. This is due to their requirement to output the full adjacency matrices
whose size grows quadratically with the number of nodes. In response to this
challenge, we introduce a new, simple, and scalable graph representation named
gap encoded edge list (GEEL) that has a small representation size that aligns
with the number of edges. In addition, GEEL significantly reduces the
vocabulary size by incorporating the gap encoding and bandwidth restriction
schemes. GEEL can be autoregressively generated with the incorporation of node
positional encoding, and we further extend GEEL to deal with attributed graphs
by designing a new grammar. Our findings reveal that the adoption of this
compact representation not only enhances scalability but also bolsters
performance by simplifying the graph generation process. We conduct a
comprehensive evaluation across ten non-attributed and two molecular graph
generation tasks, demonstrating the effectiveness of GEEL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification of Craving Maps among Marijuana Users via the Analysis of
  Functional Brain Networks with High-Order Attention Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00033v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00033v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-En Ding, Shihao Yang, Anna Zilverstand, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The excessive consumption of marijuana can induce substantial psychological
and social consequences. In this investigation, we propose an elucidative
framework termed high-order graph attention neural networks (HOGANN) for the
classification of Marijuana addiction, coupled with an analysis of localized
brain network communities exhibiting abnormal activities among chronic
marijuana users. HOGANN integrates dynamic intrinsic functional brain networks,
estimated from resting-state functional magnetic resonance imaging (rs-fMRI),
using long short-term memory (LSTM) to capture temporal network dynamics. We
employ a high-order attention module for information fusion and message passing
among neighboring nodes, enhancing the network community analysis. Our model is
validated across two distinct data cohorts, yielding substantially higher
classification accuracy than benchmark algorithms. Furthermore, we discern the
most pertinent subnetworks and cognitive regions affected by persistent
marijuana consumption, indicating adverse effects on functional brain networks,
particularly within the dorsal attention and frontoparietal networks.
Intriguingly, our model demonstrates superior performance in cohorts exhibiting
prolonged dependence, implying that prolonged marijuana usage induces more
pronounced alterations in brain networks. The model proficiently identifies
craving brain maps, thereby delineating critical brain regions for analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siteng Huang, Biao Gong, Yutong Feng, Min Zhang, Yiliang Lv, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained
vision-language models (VLMs) by constructing trainable prompts only for
composed state-object pairs. Relying on learning the joint representation of
seen compositions, these methods ignore the explicit modeling of the state and
object, thus limiting the exploitation of pre-trained knowledge and
generalization to unseen compositions. With a particular focus on the
universality of the solution, in this work, we propose a novel paradigm for
CZSL models that establishes three identification branches (i.e., Multi-Path)
to jointly model the state, object, and composition. The presented Troika is
our implementation that aligns the branch-specific prompt representations with
decomposed visual features. To calibrate the bias between semantically similar
multi-modal representations, we further devise a Cross-Modal Traction module
into Troika that shifts the prompt representation towards the current visual
content. We conduct extensive experiments on three popular benchmarks, where
our method significantly outperforms existing methods in both closed-world and
open-world settings. The code will be available at
https://github.com/bighuang624/Troika.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction Error Estimation in Random Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Krupkin, Johanna Hardin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, error estimates of classification Random Forests are
quantitatively assessed. Based on the initial theoretical framework built by
Bates et al. (2023), the true error rate and expected error rate are
theoretically and empirically investigated in the context of a variety of error
estimation methods common to Random Forests. We show that in the classification
case, Random Forests' estimates of prediction error is closer on average to the
true error rate instead of the average prediction error. This is opposite the
findings of Bates et al. (2023) which are given for logistic regression. We
further show that our result holds across different error estimation strategies
such as cross-validation, bagging, and data splitting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2104.00673 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated promising capabilities as
automatic evaluators in assessing the quality of generated natural language.
However, LLMs still exhibit biases in evaluation and often struggle to generate
coherent evaluations that align with human assessments. In this work, we first
conduct a systematic study of the misalignment between LLM evaluators and human
judgement, revealing that existing calibration methods aimed at mitigating
biases are insufficient for effectively aligning LLM evaluators. Inspired by
the use of preference data in RLHF, we formulate the evaluation as a ranking
problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided
search method that employs LLMs to conduct pairwise comparisons and efficiently
ranks candidate texts. PairS achieves state-of-the-art performance on
representative evaluation tasks and demonstrates significant improvements over
direct scoring. Furthermore, we provide insights into the role of pairwise
preference in quantifying the transitivity of LLMs and demonstrate how PairS
benefits from calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models
  through Logic <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13339v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13339v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models have showcased their remarkable
generalizability across various domains. However, their reasoning abilities
still have significant room for improvement, especially when confronted with
scenarios requiring multi-step reasoning. Although large language models
possess extensive knowledge, their reasoning often fails to effectively utilize
this knowledge to establish a coherent thinking paradigm. These models
sometimes show hallucinations as their reasoning procedures are unconstrained
by logical principles. Aiming at improving the zero-shot chain-of-thought
reasoning ability of large language models, we propose LoT (Logical Thoughts),
a self-improvement prompting framework that leverages principles rooted in
symbolic logic, particularly Reductio ad Absurdum, to systematically verify and
rectify the reasoning processes step by step. Experimental evaluations
conducted on language tasks in diverse domains, including arithmetic,
commonsense, symbolic, causal inference, and social problems, demonstrate the
efficacy of enhanced reasoning by logic. The implementation code for LoT can be
accessed at: https://github.com/xf-zhao/LoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh
  <span class="highlight-title">Transformer</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12467v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12467v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youn-Yeol Yu, Jeongwhan Choi, Woojin Cho, Kookjin Lee, Nayong Kim, Kiseok Chang, Chang-Seung Woo, Ilho Kim, Seok-Woo Lee, Joon-Young Yang, Sooyoung Yoon, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many mesh-based graph neural network (GNN) models have been
proposed for modeling complex high-dimensional physical systems. Remarkable
achievements have been made in significantly reducing the solving time compared
to traditional numerical solvers. These methods are typically designed to i)
reduce the computational cost in solving physical dynamics and/or ii) propose
techniques to enhance the solution accuracy in fluid and rigid body dynamics.
However, it remains under-explored whether they are effective in addressing the
challenges of flexible body dynamics, where instantaneous collisions occur
within a very short timeframe. In this paper, we present Hierarchical Contact
Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn
long-range dependencies (occurred by collisions) among spatially distant
positions of a body -- two close positions in a higher-level mesh correspond to
two distant positions in a lower-level mesh. HCMT enables long-range
interactions, and the hierarchical mesh structure quickly propagates collision
effects to faraway positions. To this end, it consists of a contact mesh
Transformer and a hierarchical mesh Transformer (CMT and HMT, respectively).
Lastly, we propose a flexible body dynamics dataset, consisting of trajectories
that reflect experimental settings frequently used in the display industry for
product designs. We also compare the performance of several baselines using
well-known benchmark datasets. Our results show that HCMT provides significant
performance improvements over existing methods. Our code is available at
https://github.com/yuyudeep/hcmt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Diffusion Models with Moving Average Sampling in Frequency
  Domain <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently brought a powerful revolution in image
generation. Despite showing impressive generative capabilities, most of these
models rely on the current sample to denoise the next one, possibly resulting
in denoising instability. In this paper, we reinterpret the iterative denoising
process as model optimization and leverage a moving average mechanism to
ensemble all the prior samples. Instead of simply applying moving average to
the denoised samples at different timesteps, we first map the denoised samples
to data space and then perform moving average to avoid distribution shift
across timesteps. In view that diffusion models evolve the recovery from
low-frequency components to high-frequency details, we further decompose the
samples into different frequency components and execute moving average
separately on each component. We name the complete approach "Moving Average
Sampling in Frequency domain (MASF)". MASF could be seamlessly integrated into
mainstream pre-trained diffusion models and sampling schedules. Extensive
experiments on both unconditional and conditional diffusion models demonstrate
that our MASF leads to superior performances compared to the baselines, with
almost negligible additional complexity cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of datasets, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed dataset, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed dataset and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The dataset, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastPerson: Enhancing Video Learning through Effective Video
  Summarization that Preserves Linguistic and Visual Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Kawamura, Jun Rekimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly understanding lengthy lecture videos is essential for learners with
limited time and interest in various topics to improve their learning
efficiency. To this end, video summarization has been actively researched to
enable users to view only important scenes from a video. However, these studies
focus on either the visual or audio information of a video and extract
important segments in the video. Therefore, there is a risk of missing
important information when both the teacher's speech and visual information on
the blackboard or slides are important, such as in a lecture video. To tackle
this issue, we propose FastPerson, a video summarization approach that
considers both the visual and auditory information in lecture videos.
FastPerson creates summary videos by utilizing audio transcriptions along with
on-screen images and text, minimizing the risk of overlooking crucial
information for learners. Further, it provides a feature that allows learners
to switch between the summary and original videos for each chapter of the
video, enabling them to adjust the pace of learning based on their interests
and level of understanding. We conducted an evaluation with 40 participants to
assess the effectiveness of our method and confirmed that it reduced viewing
time by 53\% at the same level of comprehension as that when using traditional
video playback methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Panonut360: A Head and Eye Tracking <span class="highlight-title">Dataset</span> for Panoramic Video <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xu, Junhao Du, Jiahe Wang, Yuwei Ning, Sihan Zhou Yang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development and widespread application of VR/AR technology,
maximizing the quality of immersive panoramic video services that match users'
personal preferences and habits has become a long-standing challenge.
Understanding the saliency region where users focus, based on data collected
with HMDs, can promote multimedia encoding, transmission, and quality
assessment. At the same time, large-scale datasets are essential for
researchers and developers to explore short/long-term user behavior patterns
and train AI models related to panoramic videos. However, existing panoramic
video datasets often include low-frequency user head or eye movement data
through short-term videos only, lacking sufficient data for analyzing users'
Field of View (FoV) and generating video saliency regions.
  Driven by these practical factors, in this paper, we present a head and eye
tracking dataset involving 50 users (25 males and 25 females) watching 15
panoramic videos. The dataset provides details on the viewport and gaze
attention locations of users. Besides, we present some statistics samples
extracted from the dataset. For example, the deviation between head and eye
movements challenges the widely held assumption that gaze attention decreases
from the center of the FoV following a Gaussian distribution. Our analysis
reveals a consistent downward offset in gaze fixations relative to the FoV in
experimental settings involving multiple users and videos. That's why we name
the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also
provide a script that generates saliency distributions based on given head or
eye coordinates and pre-generated saliency distribution map sets of each video
from the collected eye tracking data.
  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,ACM MMSys'24 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Memory Networks: A Versatile Adaptation Approach for
  Vision-Language Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024; Codes are available at \url{https://github.com/YBZh/DMN}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Visually Localize Sound Sources from Mixtures without Prior
  Source Knowledge <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjin Kim, Sung Jin Um, Sangmin Lee, Jung Uk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of the multi-sound source localization task is to localize sound
sources from the mixture individually. While recent multi-sound source
localization methods have shown improved performance, they face challenges due
to their reliance on prior information about the number of objects to be
separated. In this paper, to overcome this limitation, we present a novel
multi-sound source localization method that can perform localization without
prior knowledge of the number of sound sources. To achieve this goal, we
propose an iterative object identification (IOI) module, which can recognize
sound-making objects in an iterative manner. After finding the regions of
sound-making objects, we devise object similarity-aware clustering (OSC) loss
to guide the IOI module to effectively combine regions of the same object but
also distinguish between different objects and backgrounds. It enables our
method to perform accurate localization of sound-making objects without any
prior knowledge. Extensive experimental results on the MUSIC and VGGSound
benchmarks show the significant performance improvements of the proposed method
over the existing methods for both single and multi-source. Our code is
available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Convolutional <span class="highlight-title">Transformer</span>: Harmonizing Real vs. Complex
  Multi-View Spectral Operators for Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badri N. Patro, Vinay P. Namboodiri, Vijay S. Agneeswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers used in vision have been investigated through diverse
architectures - ViT, PVT, and Swin. These have worked to improve the attention
mechanism and make it more efficient. Differently, the need for including local
information was felt, leading to incorporating convolutions in transformers
such as CPVT and CvT. Global information is captured using a complex Fourier
basis to achieve global token mixing through various methods, such as AFNO,
GFNet, and Spectformer. We advocate combining three diverse views of data -
local, global, and long-range dependence. We also investigate the simplest
global representation using only the real domain spectral representation -
obtained through the Hartley transform. We use a convolutional operator in the
initial layers to capture local information. Through these two contributions,
we are able to optimize and obtain a spectral convolution transformer (SCT)
that provides improved performance over the state-of-the-art methods while
reducing the number of parameters. Through extensive experiments, we show that
SCT-C-small gives state-of-the-art performance on the ImageNet dataset and
reaches 84.5\% top-1 accuracy, while SCT-C-Large reaches 85.9\% and SCT-C-Huge
reaches 86.4\%. We evaluate SCT on transfer learning on datasets such as
CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on
downstream tasks i.e. instance segmentation on the MSCOCO dataset. The project
page is available on this webpage.\url{https://github.com/badripatro/sct}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time-consuming. Therefore, the challenging task of reconstructing
visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is
gaining attention in the vision research community. A major challenge in this
research problem is the lack of datasets, which capture diverse scene
conditions (e.g., lighting, shadows, weather, locations, landscapes, objects,
humans, buildings) and various image features (e.g., color, contrast,
saturation, hue, luminance, brightness, radiance). To address this gap, in this
paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic
HDR images sampled from the GTA-V video game. We perform thorough evaluation of
the proposed dataset, which demonstrates significant qualitative and
quantitative improvements of the state-of-the-art HDR image reconstruction
methods. Furthermore, we demonstrate the effectiveness of the proposed dataset
and its impact on additional computer vision tasks including 3D human pose
estimation, human body part segmentation, and holistic scene segmentation. The
dataset, data collection pipeline, and evaluation code are available at:
https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation
  with Unified Audio-Visual Speech Representation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongsoo Choi, Se Jin Park, Minsu Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech
Translation (AV2AV) framework, where the input and output of the system are
multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key
advantages can be brought: 1) We can perform real-like conversations with
individuals worldwide in a virtual meeting by utilizing our own primary
languages. In contrast to Speech-to-Speech Translation (A2A), which solely
translates between audio modalities, the proposed AV2AV directly translates
between audio-visual speech. This capability enhances the dialogue experience
by presenting synchronized lip movements along with the translated speech. 2)
We can improve the robustness of the spoken language translation system. By
employing the complementary information of audio-visual speech, the system can
effectively translate spoken language even in the presence of acoustic noise,
showcasing robust performance. To mitigate the problem of the absence of a
parallel AV2AV translation dataset, we propose to train our spoken language
translation system with the audio-only dataset of A2A. This is done by learning
unified audio-visual speech representations through self-supervised learning in
advance to train the translation system. Moreover, we propose an AV-Renderer
that can generate raw audio and video in parallel. It is designed with
zero-shot speaker modeling, thus the speaker in source audio-visual speech can
be maintained at the target translated audio-visual speech. The effectiveness
of AV2AV is evaluated with extensive experiments in a many-to-many language
translation setting. Demo page is available on
https://choijeongsoo.github.io/av2av.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Approach to Industrial Defect Generation through Blended Latent
  Diffusion Model with Online Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxi Li, Zhengxun Zhang, Hao Chen, Lin Wu, Bo Li, Deyin Liu, Mingwen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively addressing the challenge of industrial Anomaly Detection (AD)
necessitates an ample supply of defective samples, a constraint often hindered
by their scarcity in industrial contexts. This paper introduces a novel
algorithm designed to augment defective samples, thereby enhancing AD
performance. The proposed method tailors the blended latent diffusion model for
defect sample generation, employing a diffusion model to generate defective
samples in the latent space. A feature editing process, controlled by a
``trimap" mask and text prompts, refines the generated samples. The image
generation inference process is structured into three stages: a free diffusion
stage, an editing diffusion stage, and an online decoder adaptation stage. This
sophisticated inference strategy yields high-quality synthetic defective
samples with diverse pattern variations, leading to significantly improved AD
accuracies based on the augmented training set. Specifically, on the widely
recognized MVTec AD dataset, the proposed method elevates the state-of-the-art
(SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD
metrics AP, IAP, and IAP90, respectively. The implementation code of this work
can be found at the GitHub repository
https://github.com/GrandpaXun242/AdaBLDM.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProCQA: A Large-scale Community-based Programming Question Answering
  <span class="highlight-title">Dataset</span> for Code Search <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-based code question answering seeks to match user queries in
natural language to relevant code snippets. Previous approaches typically rely
on pretraining models using crafted bi-modal and uni-modal datasets to align
text and code representations. In this paper, we introduce ProCQA, a
large-scale programming question answering dataset extracted from the
StackOverflow community, offering naturally structured mixed-modal QA pairs. To
validate its effectiveness, we propose a modality-agnostic contrastive
pre-training approach to improve the alignment of text and code representations
of current code language models. Compared to previous models that primarily
employ bimodal and unimodal pairs extracted from CodeSearchNet for
pre-training, our model exhibits significant performance improvements across a
wide range of code retrieval benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToXCL: A Unified Framework for Toxic Speech Detection and Explanation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nhat M. Hoang, Xuan Long Do, Duc Anh Do, Duc Anh Vu, Luu Anh Tuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of online toxic speech is a pertinent problem posing
threats to demographic groups. While explicit toxic speech contains offensive
lexical signals, implicit one consists of coded or indirect language.
Therefore, it is crucial for models not only to detect implicit toxic speech
but also to explain its toxicity. This draws a unique need for unified
frameworks that can effectively detect and explain implicit toxic speech. Prior
works mainly formulated the task of toxic speech detection and explanation as a
text generation problem. Nonetheless, models trained using this strategy can be
prone to suffer from the consequent error propagation problem. Moreover, our
experiments reveal that the detection results of such models are much lower
than those that focus only on the detection task. To bridge these gaps, we
introduce ToXCL, a unified framework for the detection and explanation of
implicit toxic speech. Our model consists of three modules: a (i) Target Group
Generator to generate the targeted demographic group(s) of a given post; an
(ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit
toxic speech and is boosted by a (iii) Teacher Classifier via knowledge
distillation, and the decoder generates the necessary explanation. ToXCL
achieves new state-of-the-art effectiveness, and outperforms baselines
significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Who is bragging more online? A large scale analysis of bragging in
  social media <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mali Jin, Daniel Preoţiuc-Pietro, A. Seza Doğruöz, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bragging is the act of uttering statements that are likely to be positively
viewed by others and it is extensively employed in human communication with the
aim to build a positive self-image of oneself. Social media is a natural
platform for users to employ bragging in order to gain admiration, respect,
attention and followers from their audiences. Yet, little is known about the
scale of bragging online and its characteristics. This paper employs
computational sociolinguistics methods to conduct the first large scale study
of bragging behavior on Twitter (U.S.) by focusing on its overall prevalence,
temporal dynamics and impact of demographic factors. Our study shows that the
prevalence of bragging decreases over time within the same population of users.
In addition, younger, more educated and popular users in the U.S. are more
likely to brag. Finally, we conduct an extensive linguistics analysis to unveil
specific bragging themes associated with different user traits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking
  on Russia-Ukraine Conflict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking is the task of verifying the factuality of a given claim by
examining the available evidence. High-quality evidence plays a vital role in
enhancing fact-checking systems and facilitating the generation of explanations
that are understandable to humans. However, the provision of both sufficient
and relevant evidence for explainable fact-checking systems poses a challenge.
To tackle this challenge, we propose a method based on a Large Language Model
to automatically retrieve and summarize evidence from the Web. Furthermore, we
construct RU22Fact, a novel multilingual explainable fact-checking dataset on
the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world
claims, optimized evidence, and referenced explanation. To establish a baseline
for our dataset, we also develop an end-to-end explainable fact-checking system
to verify claims and generate explanations. Experimental results demonstrate
the prospect of optimized evidence in increasing fact-checking performance and
also indicate the possibility of further progress in the end-to-end claim
verification and explanation generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, accepted by lrec-coling2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grammatical vs Spelling Error Correction: An Investigation into the
  Responsiveness of <span class="highlight-title">Transformer</span>-based Language Models using BART and MarianMT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Raju, Peeta Basa Pati, SA Gandheesh, Gayatri Sanjana Sannala, Suriya KS
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text continues to remain a relevant form of representation for information.
Text documents are created either in digital native platforms or through the
conversion of other media files such as images and speech. While the digital
native text is invariably obtained through physical or virtual keyboards,
technologies such as OCR and speech recognition are utilized to transform the
images and speech signals into text content. All these variety of mechanisms of
text generation also introduce errors into the captured text.
  This project aims at analyzing different kinds of error that occurs in text
documents. The work employs two of the advanced deep neural network-based
language models, namely, BART and MarianMT, to rectify the anomalies present in
the text. Transfer learning of these models with available dataset is performed
to finetune their capacity for error correction. A comparative study is
conducted to investigate the effectiveness of these models in handling each of
the defined error categories. It is observed that while both models can bring
down the erroneous sentences by 20+%, BART can handle spelling errors far
better (24.6%) than grammatical errors (8.8%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative analysis of embedding models for patent similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grazia Sveva Ascione, Valerio Sterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantically Enriched Cross-Lingual Sentence Embeddings for
  Crisis-related Social Media Texts <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tasks such as semantic search and clustering on crisis-related social media
texts enhance our comprehension of crisis discourse, aiding decision-making and
targeted interventions. Pre-trained language models have advanced performance
in crisis informatics, but their contextual embeddings lack semantic
meaningfulness. Although the CrisisTransformers family includes a sentence
encoder to address the semanticity issue, it remains monolingual, processing
only English texts. Furthermore, employing separate models for different
languages leads to embeddings in distinct vector spaces, introducing challenges
when comparing semantic similarities between multi-lingual texts. Therefore, we
propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed
crisis-related social media texts for over 50 languages, such that texts with
similar meanings are in close proximity within the same vector space,
irrespective of language diversity. Results in sentence encoding and sentence
matching tasks are promising, suggesting these models could serve as robust
baselines when embedding multi-lingual crisis-related social media texts. The
models are publicly available at: https://huggingface.co/crisistransformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISCRAM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conversational Grounding: Annotation and Analysis of Grounding Acts and
  Grounding Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biswesh Mohapatra, Seemab Hassan, Laurent Romary, Justine Cassell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successful conversations often rest on common understanding, where all
parties are on the same page about the information being shared. This process,
known as conversational grounding, is crucial for building trustworthy dialog
systems that can accurately keep track of and recall the shared information.
The proficiencies of an agent in grounding the conveyed information
significantly contribute to building a reliable dialog system. Despite recent
advancements in dialog systems, there exists a noticeable deficit in their
grounding capabilities. Traum provided a framework for conversational grounding
introducing Grounding Acts and Grounding Units, but substantial progress,
especially in the realm of Large Language Models, remains lacking. To bridge
this gap, we present the annotation of two dialog corpora employing Grounding
Acts, Grounding Units, and a measure of their degree of grounding. We discuss
our key findings during the annotation and also provide a baseline model to
test the performance of current Language Models in categorizing the grounding
acts of the dialogs. Our work aims to provide a useful resource for further
research in making conversations with machines better understood and more
reliable in natural day-to-day collaborative dialogs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain
  Machine Generated Text Detection Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashok Urlana, Aditya Saibewar, Bala Mallikarjunarao Garlapati, Charaka Vinayak Kumar, Ajeet Kumar Singh, Srinivasa Rao Chalamala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Large Language Models (LLMs) exhibit remarkable ability to generate
fluent content across a wide spectrum of user queries. However, this capability
has raised concerns regarding misinformation and personal information leakage.
In this paper, we present our methods for the SemEval2024 Task8, aiming to
detect machine-generated text across various domains in both mono-lingual and
multi-lingual contexts. Our study comprehensively analyzes various methods to
detect machine-generated text, including statistical, neural, and pre-trained
model approaches. We also detail our experimental setup and perform a in-depth
error analysis to evaluate the effectiveness of these methods. Our methods
obtain an accuracy of 86.9\% on the test set of subtask-A mono and 83.7\% for
subtask-B. Furthermore, we also highlight the challenges and essential factors
for consideration in future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 Figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models (or Humans) Distill Text? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak, Moa Johansson, Richard Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the potential of large language models (LLMs) to distill text:
to remove the textual traces of an undesired forbidden variable. We employ a
range of LLMs with varying architectures and training approaches to distill
text by identifying and removing information about the target variable while
preserving other relevant signals. Our findings shed light on the strengths and
limitations of LLMs in addressing the distillation and provide insights into
the strategies for leveraging these models in computational social science
investigations involving text data. In particular, we show that in the strong
test of removing sentiment, the statistical association between the processed
text and sentiment is still clearly detectable to machine learning classifiers
post-LLM-distillation. Furthermore, we find that human annotators also struggle
to distill sentiment while preserving other semantic content. This suggests
there may be limited separability between concept variables in some text
contexts, highlighting limitations of methods relying on text-level
transformations and also raising questions about the robustness of distillation
methods that achieve statistical independence in representation space if this
is difficult for human coders operating on raw text to attain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NSINA: A News Corpus for Sinhala <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu Ranasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of large language models (LLMs) has advanced natural
language processing (NLP), but their effectiveness is largely dependent on
pre-training resources. This is especially evident in low-resource languages,
such as Sinhala, which face two primary challenges: the lack of substantial
training data and limited benchmarking datasets. In response, this study
introduces NSINA, a comprehensive news corpus of over 500,000 articles from
popular Sinhala news websites, along with three NLP tasks: news media
identification, news category prediction, and news headline generation. The
release of NSINA aims to provide a solution to challenges in adapting LLMs to
Sinhala, offering valuable resources and benchmarks for improving NLP in the
Sinhala language. NSINA is the largest news corpus for Sinhala, available up to
date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PE: A Poincare Explanation Method for Fast Text Hierarchy Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Chen, Xiaofeng He, Hongzhao Li, Hongyu Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The black-box nature of deep learning models in NLP hinders their widespread
application. The research focus has shifted to Hierarchical Attribution (HA)
for its ability to model feature interactions. Recent works model
non-contiguous combinations with a time-costly greedy search in Eculidean
spaces, neglecting underlying linguistic information in feature
representations. In this work, we introduce a novel method, namely Poincar\'e
Explanation (PE), for modeling feature interactions using hyperbolic spaces in
an $O(n^2logn)$ time complexity. Inspired by Poincar\'e model, we propose a
framework to project the embeddings into hyperbolic spaces, which exhibit
better inductive biases for syntax and semantic hierarchical structures.
Eventually, we prove that the hierarchical clustering process in the projected
space could be viewed as building a minimum spanning tree and propose a time
efficient algorithm. Experimental results demonstrate the effectiveness of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pointer-Generator Networks for Low-Resource Machine Translation: Don't
  Copy That! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niyati Bafna, Philipp Koehn, David Yarowsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Transformer-based neural machine translation (NMT) is very effective in
high-resource settings, many languages lack the necessary large parallel
corpora to benefit from it. In the context of low-resource (LR) MT between two
closely-related languages, a natural intuition is to seek benefits from
structural "shortcuts", such as copying subwords from the source to the target,
given that such language pairs often share a considerable number of identical
words, cognates, and borrowings. We test Pointer-Generator Networks for this
purpose for six language pairs over a variety of resource ranges, and find weak
improvements for most settings. However, analysis shows that the model does not
show greater improvements for closely-related vs. more distant language pairs,
or for lower resource ranges, and that the models do not exhibit the expected
usage of the mechanism for shared subwords. Our discussion of the reasons for
this behaviour highlights several general challenges for LR NMT, such as modern
tokenization strategies, noisy real-world conditions, and linguistic
complexities. We call for better scrutiny of linguistically motivated
improvements to NMT given the blackbox nature of Transformer models, as well as
for a focus on the above problems in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Second Look on BASS -- Boosting Abstractive Summarization with Unified
  Semantic Graphs -- A Replication Study <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osman Alperen Koraş, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a detailed replication study of the BASS framework, an abstractive
summarization system based on the notion of Unified Semantic Graphs. Our
investigation includes challenges in replicating key components and an ablation
study to systematically isolate error sources rooted in replicating novel
components. Our findings reveal discrepancies in performance compared to the
original work. We highlight the significance of paying careful attention even
to reasonably omitted details for replicating advanced frameworks like BASS,
and emphasize key practices for writing replicable papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Advances in Information Retrieval, 46th European Conference on
  Information Retrieval, ECIR 2024. 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongHeads: Multi-Head Attention is Secretly a Long Context Processor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved impressive performance in numerous
domains but often struggle to process lengthy inputs effectively and
efficiently due to limited length generalization and attention's quadratic
computational demands. Many sought to mitigate this by restricting the
attention window within the pre-trained length. However, these methods
introduce new issues such as ignoring the middle context and requiring
additional training. To address these problems, we propose LongHeads, a
training-free framework that enhances LLM's long context ability by unlocking
multi-head attention's untapped potential. Instead of allowing each head to
attend to the full sentence, which struggles with generalizing to longer
sequences due to out-of-distribution (OOD) issues, we allow each head to
process in-distribution length by selecting and attending to important context
chunks. To this end, we propose a chunk selection strategy that relies on the
inherent correlation between the query and the key representations, efficiently
distributing context chunks to different heads. In this way, each head ensures
it can effectively process attended tokens within the trained length, while
different heads in different layers can collectively process longer contexts.
LongHeads works efficiently in linear time, fits seamlessly with many LLMs that
use relative positional encoding. LongHeads achieves 100% accuracy at the 128k
length on passkey retrieval task, verifying LongHeads's efficacy in extending
the usable context window for existing models. We release our code at
https://github.com/LuLuLuyi/LongHeads .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Relationship between Skill Neurons and Robustness in <span class="highlight-title">Prompt</span>
  Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Ackermann, Xenia Ohmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt Tuning is a popular parameter-efficient finetuning method for
pre-trained large language models (PLMs). Based on experiments with RoBERTa, it
has been suggested that Prompt Tuning activates specific neurons in the
transformer's feed-forward networks, that are highly predictive and selective
for the given task. In this paper, we study the robustness of Prompt Tuning in
relation to these "skill neurons", using RoBERTa and T5. We show that prompts
tuned for a specific task are transferable to tasks of the same type but are
not very robust to adversarial data. While prompts tuned for RoBERTa yield
below-chance performance on adversarial data, prompts tuned for T5 are slightly
more robust and retain above-chance performance in two out of three cases. At
the same time, we replicate the finding that skill neurons exist in RoBERTa and
further show that skill neurons also exist in T5. Interestingly, the skill
neurons of T5 determined on non-adversarial data are also among the most
predictive neurons on the adversarial data, which is not the case for RoBERTa.
We conclude that higher adversarial robustness may be related to a model's
ability to consistently activate the relevant skill neurons on adversarial
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chitchat as Interference: Adding User Backstories to Task-Oriented
  Dialogues <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Stricker, Patrick Paroubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During task-oriented dialogues (TODs), human users naturally introduce
chitchat that is beyond the immediate scope of the task, interfering with the
flow of the conversation. To address this issue without the need for expensive
manual data creation, we use few-shot prompting with Llama-2-70B to enhance the
MultiWOZ dataset with user backstories, a typical example of chitchat
interference in TODs. We assess the impact of this addition by testing two
models: one trained solely on TODs and another trained on TODs with a
preliminary chitchat interaction. Our analysis demonstrates that our enhanced
dataset poses a challenge for these systems. Moreover, we demonstrate that our
dataset can be effectively used for training purposes, enabling a system to
consistently acknowledge the user's backstory while also successfully moving
the task forward in the same turn, as confirmed by human evaluation. These
findings highlight the benefits of generating novel chitchat-TOD scenarios to
test TOD systems more thoroughly and improve their resilience to natural user
interferences
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @ LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties
  in Generative Language Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19531v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19531v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Xue Bai, Zijia Lin, Hui Chen, Guiguang Ding, Wei Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative language models are usually pretrained on large text corpus via
predicting the next token (i.e., sub-word/word/phrase) given the previous ones.
Recent works have demonstrated the impressive performance of large generative
language models on downstream tasks. However, existing generative language
models generally neglect an inherent challenge in text corpus during training,
i.e., the imbalance between frequent tokens and infrequent ones. It can lead a
language model to be dominated by common and easy-to-learn tokens, thereby
overlooking the infrequent and difficult-to-learn ones. To alleviate that, we
propose a MiLe Loss function for mitigating the bias of learning difficulties
with tokens. During training, it can dynamically assess the learning difficulty
of a to-be-learned token, according to the information entropy of the
corresponding predicted probability distribution over the vocabulary. Then it
scales the training loss adaptively, trying to lead the model to focus more on
the difficult-to-learn tokens. On the Pile dataset, we train generative
language models at different scales of 468M, 1.2B, and 6.7B parameters.
Experiments reveal that models incorporating the proposed MiLe Loss can gain
consistent performance improvement on downstream benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align-to-Distill: Trainable Attention Alignment for Knowledge
  Distillation in Neural Machine Translation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01479v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01479v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh, Yeonsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of scalable deep models and large datasets has improved the
performance of Neural Machine Translation. Knowledge Distillation (KD) enhances
efficiency by transferring knowledge from a teacher model to a more compact
student model. However, KD approaches to Transformer architecture often rely on
heuristics, particularly when deciding which teacher layers to distill from. In
this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to
address the feature mapping problem by adaptively aligning student attention
heads with their teacher counterparts during training. The Attention Alignment
Module in A2D performs a dense head-by-head comparison between student and
teacher attention heads across layers, turning the combinatorial mapping
heuristics into a learning problem. Our experiments show the efficacy of A2D,
demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb
and WMT-2014 En->De, respectively, compared to Transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To share or not to share: What risks would laypeople accept to give
  sensitive data to differentially-private NLP systems? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Weiss, Frauke Kreuter, Ivan Habernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the NLP community has adopted central differential privacy as a
go-to framework for privacy-preserving model training or data sharing, the
choice and interpretation of the key parameter, privacy budget $\varepsilon$
that governs the strength of privacy protection, remains largely arbitrary. We
argue that determining the $\varepsilon$ value should not be solely in the
hands of researchers or system developers, but must also take into account the
actual people who share their potentially sensitive data. In other words: Would
you share your instant messages for $\varepsilon$ of 10? We address this
research gap by designing, implementing, and conducting a behavioral experiment
(311 lay participants) to study the behavior of people in uncertain
decision-making situations with respect to privacy-threatening situations.
Framing the risk perception in terms of two realistic NLP scenarios and using a
vignette behavioral study help us determine what $\varepsilon$ thresholds would
lead lay people to be willing to share sensitive textual data - to our
knowledge, the first study of its kind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024; final camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DPStyler: Dynamic <span class="highlight-title">Prompt</span>Styler for Source-Free Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Tang, Yuxuan Wan, Lei Qi, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-Free Domain Generalization (SFDG) aims to develop a model that works
for unseen target domains without relying on any source domain. Recent work,
PromptStyler, employs text prompts to simulate different distribution shifts in
the joint vision-language space, allowing the model to generalize effectively
to unseen domains without using any images. However, 1) PromptStyler's style
generation strategy has limitations, as all style patterns are fixed after the
first training phase. This leads to the training set in the second training
phase being restricted to a limited set of styles. Additionally, 2) the frozen
text encoder in PromptStyler result in the encoder's output varying with the
style of the input text prompts, making it difficult for the model to learn
domain-invariant features. In this paper, we introduce Dynamic PromptStyler
(DPStyler), comprising Style Generation and Style Removal modules to address
these issues. The Style Generation module refreshes all styles at every
training epoch, while the Style Removal module eliminates variations in the
encoder's output features caused by input styles. Moreover, since the Style
Generation module, responsible for generating style word vectors using random
sampling or style mixing, makes the model sensitive to input text prompts, we
introduce a model ensemble method to mitigate this sensitivity. Extensive
experiments demonstrate that our framework outperforms state-of-the-art methods
on benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Performance of Deep Learning for Automated Gleason Grading
  in Prostate Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Daniel Hieber, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Frank Kramer, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer is a dominant health concern calling for advanced diagnostic
tools. Utilizing digital pathology and artificial intelligence, this study
explores the potential of 11 deep neural network architectures for automated
Gleason grading in prostate carcinoma focusing on comparing traditional and
recent architectures. A standardized image classification pipeline, based on
the AUCMEDI framework, facilitated robust evaluation using an in-house dataset
consisting of 34,264 annotated tissue tiles. The results indicated varying
sensitivity across architectures, with ConvNeXt demonstrating the strongest
performance. Notably, newer architectures achieved superior performance, even
though with challenges in differentiating closely related Gleason grades. The
ConvNeXt model was capable of learning a balance between complexity and
generalizability. Overall, this study lays the groundwork for enhanced Gleason
grading systems, potentially improving diagnostic efficiency for prostate
cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synapse: Learning Preferential Concepts from Visual Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of preference learning, which aims to learn
user-specific preferences (e.g., "good parking spot", "convenient drop-off
location") from visual input. Despite its similarity to learning factual
concepts (e.g., "red cube"), preference learning is a fundamentally harder
problem due to its subjective nature and the paucity of person-specific
training data. We address this problem using a new framework called Synapse,
which is a neuro-symbolic approach designed to efficiently learn preferential
concepts from limited demonstrations. Synapse represents preferences as
neuro-symbolic programs in a domain-specific language (DSL) that operates over
images, and leverages a novel combination of visual parsing, large language
models, and program synthesis to learn programs representing individual
preferences. We evaluate Synapse through extensive experimentation including a
user case study focusing on mobility-related concepts in mobile robotics and
autonomous driving. Our evaluation demonstrates that Synapse significantly
outperforms existing baselines as well as its own ablations. The code and other
details can be found on the project website https://amrl.cs.utexas.edu/synapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures; Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepGleason: a System for Automated Gleason Grading of Prostate Cancer
  using Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in digital pathology and artificial intelligence (AI) offer
promising opportunities for clinical decision support and enhancing diagnostic
workflows. Previous studies already demonstrated AI's potential for automated
Gleason grading, but lack state-of-the-art methodology and model reusability.
To address this issue, we propose DeepGleason: an open-source deep neural
network based image classification system for automated Gleason grading using
whole-slide histopathology images from prostate tissue sections. Implemented
with the standardized AUCMEDI framework, our tool employs a tile-wise
classification approach utilizing fine-tuned image preprocessing techniques in
combination with a ConvNeXt architecture which was compared to various
state-of-the-art architectures. The neural network model was trained and
validated on an in-house dataset of 34,264 annotated tiles from 369 prostate
carcinoma slides. We demonstrated that DeepGleason is capable of highly
accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,
AUC of 0.991, and Accuracy of 0.974. The internal architecture comparison
revealed that the ConvNeXt model was superior performance-wise on our dataset
to established and other modern architectures like transformers. Furthermore,
we were able to outperform the current state-of-the-art in tile-wise
fine-classification with a sensitivity and specificity of 0.94 and 0.98 for
benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs
Gleason 4 & 5 classification, respectively. Our tool contributes to the wider
adoption of AI-based Gleason grading within the research community and paves
the way for broader clinical application of deep learning models in digital
pathology. DeepGleason is open-source and publicly available for research
application in the following Git repository:
https://github.com/frankkramer-lab/DeepGleason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOOL: Addressing the Downlink Bottleneck in Satellite Computing with
  Neural Feature Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanosatellite constellations equipped with sensors capturing large geographic
regions provide unprecedented opportunities for Earth observation. As
constellation sizes increase, network contention poses a downlink bottleneck.
Orbital Edge Computing (OEC) leverages limited onboard compute resources to
reduce transfer costs by processing the raw captures at the source. However,
current solutions have limited practicability due to reliance on crude
filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression
method that preserves prediction performance. FOOL partitions high-resolution
satellite imagery to maximize throughput. Further, it embeds context and
leverages inter-tile dependencies to lower transfer costs with negligible
overhead. While FOOL is a feature compressor, it can recover images with
competitive scores on perceptual quality measures at lower bitrates. We
extensively evaluate transfer cost reduction by including the peculiarity of
intermittently available network connections in low earth orbit. Lastly, we
test the feasibility of our system for standardized nanosatellite form factors.
We demonstrate that FOOL permits downlinking over 100x the data volume without
relying on prior information on the downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, double column, 19 figures, 7 tables, Initial Submission to
  IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Zhang, Jinhong Deng, Peidong Liu, Wen Li, Shiyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing
attention in recent years due to its important application in various tasks.
The existing methods for MAV detection assume that the training set and testing
set have the same distribution. As a result, when deployed in new domains, the
detectors would have a significant performance degradation due to domain
discrepancy. In this paper, we study the problem of cross-domain MAV detection.
The contributions of this paper are threefold. 1) We propose a
Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and
realistic images. Compared to other existing datasets, the proposed one is more
comprehensive in the sense that it covers rich scenes, diverse MAV types, and
various viewing angles. A new benchmark for cross-domain MAV detection is
proposed based on the proposed dataset. 2) We propose a Noise Suppression
Network (NSN) based on the framework of pseudo-labeling and a large-to-small
training procedure. To reduce the challenging pseudo-label noises, two novel
modules are designed in this network. The first is a prior-based curriculum
learning module for allocating adaptive thresholds for pseudo labels with
different difficulties. The second is a masked copy-paste augmentation module
for pasting truly-labeled MAVs on unlabeled target images and thus decreasing
pseudo-label noises. 3) Extensive experimental results verify the superior
performance of the proposed method compared to the state-of-the-art ones. In
particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on
the tasks of simulation-to-real adaptation, cross-scene adaptation, and
cross-camera adaptation, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures. Accepted by IEEE Transactions on Automation
  Science and Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Propagation for Universal Medical Image Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Ding, Liulei Li, Wenguan Wang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prominent solutions for medical image segmentation are typically tailored for
automatic or interactive setups, posing challenges in facilitating progress
achieved in one task to another.$_{\!}$ This$_{\!}$ also$_{\!}$
necessitates$_{\!}$ separate$_{\!}$ models for each task, duplicating both
training time and parameters.$_{\!}$ To$_{\!}$ address$_{\!}$ above$_{\!}$
issues,$_{\!}$ we$_{\!}$ introduce$_{\!}$ S2VNet,$_{\!}$ a$_{\!}$
universal$_{\!}$ framework$_{\!}$ that$_{\!}$ leverages$_{\!}$
Slice-to-Volume$_{\!}$ propagation$_{\!}$ to$_{\!}$ unify automatic/interactive
segmentation within a single model and one training session. Inspired by
clustering-based segmentation techniques, S2VNet makes full use of the
slice-wise structure of volumetric data by initializing cluster centers from
the cluster$_{\!}$ results$_{\!}$ of$_{\!}$ previous$_{\!}$ slice.$_{\!}$ This
enables knowledge acquired from prior slices to assist in the segmentation of
the current slice, further efficiently bridging the communication between
remote slices using mere 2D networks. Moreover, such a framework readily
accommodates interactive segmentation with no architectural change, simply by
initializing centroids from user inputs. S2VNet distinguishes itself by swift
inference speeds and reduced memory consumption compared to prevailing 3D
solutions. It can also handle multi-class interactions with each of them
serving to initialize different centroids. Experiments on three benchmarks
demonstrate S2VNet surpasses task-specified solutions on both
automatic/interactive setups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Adaptive Reality-Guided Diffusion for Artifact-Free
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingping Zheng, Ling Zheng, Yuanfan Guo, Ying Li, Songcen Xu, Jiankang Deng, Hang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artifact-free super-resolution (SR) aims to translate low-resolution images
into their high-resolution counterparts with a strict integrity of the original
content, eliminating any distortions or synthetic details. While traditional
diffusion-based SR techniques have demonstrated remarkable abilities to enhance
image detail, they are prone to artifact introduction during iterative
procedures. Such artifacts, ranging from trivial noise to unauthentic textures,
deviate from the true structure of the source image, thus challenging the
integrity of the super-resolution process. In this work, we propose
Self-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that
delves into the latent space to effectively identify and mitigate the
propagation of artifacts. Our SARGD begins by using an artifact detector to
identify implausible pixels, creating a binary mask that highlights artifacts.
Following this, the Reality Guidance Refinement (RGR) process refines artifacts
by integrating this mask with realistic latent representations, improving
alignment with the original image. Nonetheless, initial realistic-latent
representations from lower-quality images result in over-smoothing in the final
output. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.
It dynamically computes a reality score, enhancing the sharpness of the
realistic latent. These alternating mechanisms collectively achieve
artifact-free super-resolution. Extensive experiments demonstrate the
superiority of our method, delivering detailed artifact-free high-resolution
images while reducing sampling steps by 2X. We release our code at
https://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Texture Loss for CT denoising with GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Feola, Lorenzo Tronchin, Valerio Guarrasi, Paolo Soda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have proved as a powerful framework
for denoising applications in medical imaging. However, GAN-based denoising
algorithms still suffer from limitations in capturing complex relationships
within the images. In this regard, the loss function plays a crucial role in
guiding the image generation process, encompassing how much a synthetic image
differs from a real image. To grasp highly complex and non-linear textural
relationships in the training process, this work presents a loss function that
leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence
Matrix (GLCM). Although the recent advances in deep learning have demonstrated
superior performance in classification and detection tasks, we hypothesize that
its information content can be valuable when integrated into GANs' training. To
this end, we propose a differentiable implementation of the GLCM suited for
gradient-based optimization. Our approach also introduces a self-attention
layer that dynamically aggregates the multi-scale texture information extracted
from the images. We validate our approach by carrying out extensive experiments
in the context of low-dose CT denoising, a challenging application that aims to
enhance the quality of noisy CT scans. We utilize three publicly available
datasets, including one simulated and two real datasets. The results are
promising as compared to other well-established loss functions, being also
consistent across three different GAN architectures. The code is available at:
https://github.com/FrancescoDiFeola/DenoTextureLoss
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Generated Video Detection via Spatio-Temporal Anomaly Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfa Bai, Man Lin, Gang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of generation models has led to the emergence of highly
realistic artificial intelligence (AI)-generated videos. Malicious users can
easily create non-existent videos to spread false information. This letter
proposes an effective AI-generated video detection (AIGVDet) scheme by
capturing the forensic traces with a two-branch spatio-temporal convolutional
neural network (CNN). Specifically, two ResNet sub-detectors are learned
separately for identifying the anomalies in spatical and optical flow domains,
respectively. Results of such sub-detectors are fused to further enhance the
discrimination ability. A large-scale generated video dataset (GVD) is
constructed as a benchmark for model training and evaluation. Extensive
experimental results verify the high generalization and robustness of our
AIGVDet scheme. Code and dataset will be available at
https://github.com/multimediaFor/AIGVDet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si Liu, Zihan Ding, Jiahui Fu, Hongyu Li, Siheng Chen, Shifeng Zhang, Xu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of the collaborative vehicle-to-everything perception task is
to enhance the individual vehicle's perception capability through message
communication among neighboring traffic agents. Previous methods focus on
achieving optimal performance within bandwidth limitations and typically adopt
BEV maps as the basic collaborative message units. However, we demonstrate that
collaboration with dense representations is plagued by object feature
destruction during message packing, inefficient message aggregation for
long-range collaboration, and implicit structure representation communication.
To tackle these issues, we introduce a brand new message unit, namely point
cluster, designed to represent the scene sparsely with a combination of
low-level structure information and high-level semantic information. The point
cluster inherently preserves object information while packing messages, with
weak relevance to the collaboration range, and supports explicit structure
modeling. Building upon this representation, we propose a novel framework
V2X-PC for collaborative perception. This framework includes a Point Cluster
Packing (PCP) module to keep object feature and manage bandwidth through the
manipulation of cluster point numbers. As for effective message aggregation, we
propose a Point Cluster Aggregation (PCA) module to match and merge point
clusters associated with the same object. To further handle time latency and
pose errors encountered in real-world scenarios, we propose parameter-free
solutions that can adapt to different noisy levels without finetuning.
Experiments on two widely recognized collaborative perception benchmarks
showcase the superior performance of our method compared to the previous
state-of-the-art approaches relying on BEV maps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuda Song, Zehao Sun, Xuanwu Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models have positioned them at the forefront
of image generation. Despite their superior performance, diffusion models are
not without drawbacks; they are characterized by complex architectures and
substantial computational demands, resulting in significant latency due to
their iterative sampling process. To mitigate these limitations, we introduce a
dual approach involving model miniaturization and a reduction in sampling
steps, aimed at significantly decreasing model latency. Our methodology
leverages knowledge distillation to streamline the U-Net and image decoder
architectures, and introduces an innovative one-step DM training technique that
utilizes feature matching and score distillation. We present two models,
SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS
(30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU,
respectively. Moreover, our training approach offers promising applications in
image-conditioned control, facilitating efficient image-to-image translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Busra Asan, Abdullah Akgul, Alper Unal, Melih Kandemir, Gozde Unal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seasonal forecasting is a crucial task when it comes to detecting the extreme
heat and colds that occur due to climate change. Confidence in the predictions
should be reliable since a small increase in the temperatures in a year has a
big impact on the world. Calibration of the neural networks provides a way to
ensure our confidence in the predictions. However, calibrating regression
models is an under-researched topic, especially in forecasters. We calibrate a
UNet++ based architecture, which was shown to outperform physics-based models
in temperature anomalies. We show that with a slight trade-off between
prediction error and calibration error, it is possible to get more reliable and
sharper forecasts. We believe that calibration should be an important part of
safety-critical machine learning applications such as weather forecasters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a workshop paper at "ICLR 2024 Tackling Climate Change
  with Machine Learning"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction
  and Defect-Focus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Li, Ruijie Ma, Xiang Qian, Xiaohao Wang, Xinghui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenge of data scarcity in industrial domains, transfer
learning emerges as a pivotal paradigm. This work introduces Style Filter, a
tailored methodology for industrial contexts. By selectively filtering source
domain data before knowledge transfer, Style Filter reduces the quantity of
data while maintaining or even enhancing the performance of transfer learning
strategy. Offering label-free operation, minimal reliance on prior knowledge,
independence from specific models, and re-utilization, Style Filter is
evaluated on authentic industrial datasets, highlighting its effectiveness when
employed before conventional transfer strategies in the deep learning domain.
The results underscore the effectiveness of Style Filter in real-world
industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures,4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for
  Aerial Semantic Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aysim Toker, Marvin Eisenberger, Daniel Cremers, Laura Leal-Taixé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, semantic segmentation has become a pivotal tool in
processing and interpreting satellite imagery. Yet, a prevalent limitation of
supervised learning techniques remains the need for extensive manual
annotations by experts. In this work, we explore the potential of generative
image diffusion to address the scarcity of annotated data in earth observation
tasks. The main idea is to learn the joint data manifold of images and labels,
leveraging recent advancements in denoising diffusion probabilistic models. To
the best of our knowledge, we are the first to generate both images and
corresponding masks for satellite segmentation. We find that the obtained pairs
not only display high quality in fine-scale features but also ensure a wide
sampling diversity. Both aspects are crucial for earth observation data, where
semantic classes can vary severely in scale and occurrence frequency. We employ
the novel data instances for downstream segmentation, as a form of data
augmentation. In our experiments, we provide comparisons to prior works based
on discriminative diffusion models or GANs. We demonstrate that integrating
generated samples yields significant quantitative improvements for satellite
semantic segmentation -- both compared to baselines and when training only on
the original data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kudaibergen Abutalip, Numan Saeed, Ikboljon Sobirov, Vincent Andrearczyk, Adrien Depeursinge, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying deep learning (DL) models in medical applications relies on
predictive performance and other critical factors, such as conveying
trustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide
potential solutions for evaluating prediction reliability and improving the
model confidence calibration. Despite increasing interest in UE, challenges
persist, such as the need for explicit methods to capture aleatoric uncertainty
and align uncertainty estimates with real-life disagreements among domain
experts. This paper proposes an Expert Disagreement-Guided Uncertainty
Estimation (EDUE) for medical image segmentation. By leveraging variability in
ground-truth annotations from multiple raters, we guide the model during
training and incorporate random sampling-based strategies to enhance
calibration confidence. Our method achieves 55% and 23% improvement in
correlation on average with expert disagreements at the image and pixel levels,
respectively, better calibration, and competitive segmentation performance
compared to the state-of-the-art deep ensembles, requiring only a single
forward pass.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In the Search for Optimal Multi-view Learning Models for Crop
  Classification with Global Remote Sensing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mena, Diego Arenas, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop classification is of critical importance due to its role in studying
crop pattern changes, resource management, and carbon sequestration. When
employing data-driven techniques for its prediction, utilizing various temporal
data sources is necessary. Deep learning models have proven to be effective for
this task by mapping time series data to high-level representation for
prediction. However, they face substantial challenges when dealing with
multiple input patterns. The literature offers limited guidance for Multi-View
Learning (MVL) scenarios, as it has primarily focused on exploring fusion
strategies with specific encoders and validating them in local regions. In
contrast, we investigate the impact of simultaneous selection of the fusion
strategy and the encoder architecture evaluated on a global-scale cropland and
crop-type classifications. We use a range of five fusion strategies (Input,
Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures
(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The
validation is on the CropHarvest dataset that provides optical, radar, and
weather time series, and topographic information as input data. We found that
in scenarios with a limited number of labeled samples, a unique configuration
is insufficient for all the cases. Instead, a specialized combination,
including encoder and fusion strategy, should be meticulously sought. To
streamline this search process, we suggest initially identifying the optimal
encoder architecture tailored for a particular fusion strategy, and then
determining the most suitable fusion strategy for the classification task. We
provide a technical framework for researchers exploring crop classification or
related tasks through a MVL approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegICL: A Universal In-context Learning Framework for Enhanced
  Segmentation in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shining Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation models adapting to new tasks in a training-free
manner through in-context learning is an exciting advancement. Universal
segmentation models aim to generalize across the diverse modality of medical
images, yet their effectiveness often diminishes when applied to
out-of-distribution (OOD) data modalities and tasks, requiring intricate
fine-tuning of model for optimal performance. For addressing this challenge, we
introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for
image segmentation. Unlike existing methods, SegICL has the capability to
employ text-guided segmentation and conduct in-context learning with a small
set of image-mask pairs, eliminating the need for training the model from
scratch or fine-tuning for OOD tasks (including OOD modality and dataset).
Extensive experimental validation of SegICL demonstrates a positive correlation
between the number of prompt samples and segmentation performance on OOD
modalities and tasks. This indicates that SegICL effectively address new
segmentation tasks based on contextual information. Additionally, SegICL also
exhibits comparable segmentation performance to mainstream models on OOD and
in-distribution tasks. Our code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Vulnerabilities of Neural Networks in Parameter Learning and
  Defense Against Explanation-Aware Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) strategies play a crucial part in
increasing the understanding and trustworthiness of neural networks.
Nonetheless, these techniques could potentially generate misleading
explanations. Blinding attacks can drastically alter a machine learning
algorithm's prediction and explanation, providing misleading information by
adding visually unnoticeable artifacts into the input, while maintaining the
model's accuracy. It poses a serious challenge in ensuring the reliability of
XAI methods. To ensure the reliability of XAI methods poses a real challenge,
we leverage statistical analysis to highlight the changes in CNN weights within
a CNN following blinding attacks. We introduce a method specifically designed
to limit the effectiveness of such attacks during the evaluation phase,
avoiding the need for extra training. The method we suggest defences against
most modern explanation-aware adversarial attacks, achieving an approximate
decrease of ~99\% in the Attack Success Rate (ASR) and a ~91\% reduction in the
Mean Square Error (MSE) between the original explanation and the defended
(post-attack) explanation across three unique types of attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elysium: Exploring Object-level Perception in Videos via MLLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Yanjie Wang, Yongjie Ye, Yuxiang Nie, Can Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have demonstrated their ability to
perceive objects in still images, but their application in video-related tasks,
such as object tracking, remains understudied. This lack of exploration is
primarily due to two key challenges. Firstly, extensive pretraining on
large-scale video datasets is required to equip MLLMs with the capability to
perceive objects across multiple frames and understand inter-frame
relationships. Secondly, processing a large number of frames within the context
window of Large Language Models (LLMs) can impose a significant computational
burden. To address the first challenge, we introduce ElysiumTrack-1M, a
large-scale video dataset paired with novel tasks: Referring Single Object
Tracking (RSOT) and Video Referring Expression Generation (Video-REG).
ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding
object boxes and descriptions. Leveraging this dataset, we conduct training of
MLLMs and propose a token-compression model T-Selector to tackle the second
challenge. Our proposed approach, Elysium: Exploring Object-level Perception in
Videos via MLLM, is an end-to-end trainable MLLM that makes the first attempt
to conduct object-level tasks in videos without requiring any additional
plug-in or expert models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QKFormer: Hierarchical Spiking <span class="highlight-title">Transformer</span> using Q-K Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlin Zhou, Han Zhang, Zhaokun Zhou, Liutao Yu, Liwei Huang, Xiaopeng Fan, Li Yuan, Zhengyu Ma, Huihui Zhou, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with
Transformer architectures, have attracted significant attention due to their
potential for energy efficiency and high performance. However, existing models
in this domain still suffer from suboptimal performance. We introduce several
innovations to improve the performance: i) We propose a novel spike-form Q-K
attention mechanism, tailored for SNNs, which efficiently models the importance
of token or channel dimensions through binary vectors with linear complexity.
ii) We incorporate the hierarchical structure, which significantly benefits the
performance of both the brain and artificial neural networks, into spiking
transformers to obtain multi-scale spiking representation. iii) We design a
versatile and powerful patch embedding module with a deformed shortcut
specifically for spiking transformers. Together, we develop QKFormer, a
hierarchical spiking transformer based on Q-K attention with direct training.
QKFormer shows significantly superior performance over existing
state-of-the-art SNN models on various mainstream datasets. Notably, with
comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a
groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially
outperforming Spikformer by 10.84%. To our best knowledge, this is the first
time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The
code and models are publicly available at
https://github.com/zhouchenlin2096/QKFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, code: https://github.com/zhouchenlin2096/QKFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOrA: 3D Visual Grounding with Order-Aware Referring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tung-Yu Wu, Sheng-Yu Huang, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding aims to identify the target object within a 3D point
cloud scene referred to by a natural language description. While previous works
attempt to exploit the verbo-visual relation with proposed cross-modal
transformers, unstructured natural utterances and scattered objects might lead
to undesirable performances. In this paper, we introduce DOrA, a novel 3D
visual grounding framework with Order-Aware referring. DOrA is designed to
leverage Large Language Models (LLMs) to parse language description, suggesting
a referential order of anchor objects. Such ordered anchor objects allow DOrA
to update visual features and locate the target object during the grounding
process. Experimental results on the NR3D and ScanRefer datasets demonstrate
our superiority in both low-resource and full-data scenarios. In particular,
DOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% grounding
accuracy under 1% data and 10% data settings, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate
  Spatiotemporal Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, Junwei Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining CNNs or ViTs, with RNNs for spatiotemporal forecasting, has yielded
unparalleled results in predicting temporal and spatial dynamics. However,
modeling extensive global information remains a formidable challenge; CNNs are
limited by their narrow receptive fields, and ViTs struggle with the intensive
computational demands of their attention mechanisms. The emergence of recent
Mamba-based architectures has been met with enthusiasm for their exceptional
long-sequence modeling capabilities, surpassing established vision models in
efficiency and accuracy, which motivates us to develop an innovative
architecture tailored for spatiotemporal forecasting. In this paper, we propose
the VMRNN cell, a new recurrent unit that integrates the strengths of Vision
Mamba blocks with LSTM. We construct a network centered on VMRNN cells to
tackle spatiotemporal prediction tasks effectively. Our extensive evaluations
show that our proposed approach secures competitive results on a variety of
tasks while maintaining a smaller model size. Our code is available at
https://github.com/yyyujintang/VMRNN-PyTorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:2308.09891 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Hu, Shaochong Jia, Mohammad Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been widely used for conditional data cross-modal
generation tasks such as text-to-image and text-to-video. However,
state-of-the-art models still fail to align the generated visual concepts with
high-level semantics in a language such as object count, spatial relationship,
etc. We approach this problem from a multimodal data fusion perspective and
investigate how different fusion strategies can affect vision-language
alignment. We discover that compared to the widely used early fusion of
conditioning text in a pretrained image feature space, a specially designed
intermediate fusion can: (i) boost text-to-image alignment with improved
generation quality and (ii) improve training and inference efficiency by
reducing low-rank text-to-image attention calculations. We perform experiments
using a text-to-image generation task on the MS-COCO dataset. We compare our
intermediate fusion mechanism with the classic early fusion mechanism on two
common conditioning methods on a U-shaped ViT backbone. Our intermediate fusion
model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and
50% increased training speed compared to a strong U-ViT baseline with an early
fusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Set Recognition in the Age of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimity Miller, Niko Sünderhauf, Alex Kenna, Keita Mason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Are vision-language models (VLMs) open-set models because they are trained on
internet-scale datasets? We answer this question with a clear no - VLMs
introduce closed-set assumptions via their finite query set, making them
vulnerable to open-set conditions. We systematically evaluate VLMs for open-set
recognition and find they frequently misclassify objects not contained in their
query set, leading to alarmingly low precision when tuned for high recall and
vice versa. We show that naively increasing the size of the query set to
contain more and more classes does not mitigate this problem, but instead
causes diminishing task performance and open-set performance. We establish a
revised definition of the open-set problem for the age of VLMs, define a new
benchmark and evaluation protocol to facilitate standardised evaluation and
research in this important area, and evaluate promising baseline approaches
based on predictive uncertainty and dedicated negative embeddings on a range of
VLM classifiers and object detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModeTv2: GPU-accelerated Motion Decomposition <span class="highlight-title">Transformer</span> for Pairwise
  Optimization in Medical Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiqiao Wang, Zhuoyuan Wang, Dong Ni, Yi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable image registration plays a crucial role in medical imaging, aiding
in disease diagnosis and image-guided interventions. Traditional iterative
methods are slow, while deep learning (DL) accelerates solutions but faces
usability and precision challenges. This study introduces a pyramid network
with the enhanced motion decomposition Transformer (ModeTv2) operator,
showcasing superior pairwise optimization (PO) akin to traditional methods. We
re-implement ModeT operator with CUDA extensions to enhance its computational
efficiency. We further propose RegHead module which refines deformation fields,
improves the realism of deformation and reduces parameters. By adopting the PO,
the proposed network balances accuracy, efficiency, and generalizability.
Extensive experiments on two public brain MRI datasets and one abdominal CT
dataset demonstrate the network's suitability for PO, providing a DL model with
enhanced usability and interpretability. The code is publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal
  Representation Learning for AD classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangqian Yang, Kangrui Du, Zhihan Yang, Ye Du, Yongping Zheng, Shujun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) is an incurable neurodegenerative condition leading
to cognitive and functional deterioration. Given the lack of a cure, prompt and
precise AD diagnosis is vital, a complex process dependent on multiple factors
and multi-modal data. While successful efforts have been made to integrate
multi-modal representation learning into medical datasets, scant attention has
been given to 3D medical images. In this paper, we propose Contrastive Masked
Vim Autoencoder (CMViM), the first efficient representation learning method
tailored for 3D multi-modal data. Our proposed framework is built on a masked
Vim autoencoder to learn a unified multi-modal representation and
long-dependencies contained in 3D medical images. We also introduce an
intra-modal contrastive learning module to enhance the capability of the
multi-modal Vim encoder for modeling the discriminative features in the same
modality, and an inter-modal contrastive learning module to alleviate
misaligned representation among modalities. Our framework consists of two main
steps: 1) incorporate the Vision Mamba (Vim) into the mask autoencoder to
reconstruct 3D masked multi-modal data efficiently. 2) align the multi-modal
representations with contrastive learning mechanisms from both intra-modal and
inter-modal aspects. Our framework is pre-trained and validated ADNI2 dataset
and validated on the downstream task for AD classification. The proposed CMViM
yields 2.7\% AUC performance improvement compared with other state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually Guided Generative Text-Layout <span class="highlight-title">Pre-train</span>ing for Document
  Intelligence <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let Real Images be as a Judger, Spotting Fake Images Synthesized with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyou Liang, Run Wang, Weifeng Liu, Yuyang Zhang, Wenyuan Yang, Lina Wang, Xingkai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last few years, generative models have shown their powerful
capabilities in synthesizing realistic images in both quality and diversity
(i.e., facial images, and natural subjects). Unfortunately, the artifact
patterns in fake images synthesized by different generative models are
inconsistent, leading to the failure of previous research that relied on
spotting subtle differences between real and fake. In our preliminary
experiments, we find that the artifacts in fake images always change with the
development of the generative model, while natural images exhibit stable
statistical properties. In this paper, we employ natural traces shared only by
real images as an additional predictive target in the detector. Specifically,
the natural traces are learned from the wild real images and we introduce
extended supervised contrastive learning to bring them closer to real images
and further away from fake ones. This motivates the detector to make decisions
based on the proximity of images to the natural traces. To conduct a
comprehensive experiment, we built a high-quality and diverse dataset that
includes generative models comprising 6 GAN and 6 diffusion models, to evaluate
the effectiveness in generalizing unknown forgery techniques and robustness in
surviving different transformations. Experimental results show that our
proposed method gives 96.1% mAP significantly outperforms the baselines.
Extensive experiments conducted on the widely recognized platform Midjourney
reveal that our proposed method achieves an accuracy exceeding 78.4%,
underscoring its practicality for real-world application deployment. The source
code and partial self-built dataset are available in supplementary material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable process of talking-head-based avatar-creating
solutions, directly generating anchor-style videos with full-body motions
remains challenging. In this study, we propose Make-Your-Anchor, a novel system
necessitating only a one-minute video clip of an individual for training,
subsequently enabling the automatic generation of anchor-style videos with
precise torso and hand movements. Specifically, we finetune a proposed
structure-guided diffusion model on input video to render 3D mesh conditions
into human appearances. We adopt a two-stage training strategy for the
diffusion model, effectively binding movements with specific appearances. To
produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise
diffusion model to a 3D style without additional training cost, and a simple
yet effective batch-overlapped temporal denoising module is proposed to bypass
the constraints on video length during inference. Finally, a novel
identity-specific face enhancement module is introduced to improve the visual
quality of facial regions in the output videos. Comparative experiments
demonstrate the effectiveness and superiority of the system in terms of visual
quality, temporal coherence, and identity preservation, outperforming SOTA
diffusion/non-diffusion methods. Project page:
\url{https://github.com/ICTMCG/Make-Your-Anchor}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Image Registration and Its Application in Retinal Images: A
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Nie, Xiaoqing Zhang, Yan Hu, Mingdao Gong, Jiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image registration is vital for disease diagnosis and treatment with
its ability to merge diverse information of images, which may be captured under
different times, angles, or modalities. Although several surveys have reviewed
the development of medical image registration, these surveys have not
systematically summarized methodologies of existing medical image registration
methods. To this end, we provide a comprehensive review of these methods from
traditional and deep learning-based directions, aiming to help audiences
understand the development of medical image registration quickly. In
particular, we review recent advances in retinal image registration at the end
of each section, which has not attracted much attention. Additionally, we also
discuss the current challenges of retinal image registration and provide
insights and prospects for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Learning for Medical Image Data with Anatomy-Oriented
  Imaging Planes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Zhang, Dong Wei, Mengmeng Zhua, Shi Gu, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has emerged as a powerful tool for pretraining deep
networks on unlabeled data, prior to transfer learning of target tasks with
limited annotation. The relevance between the pretraining pretext and target
tasks is crucial to the success of transfer learning. Various pretext tasks
have been proposed to utilize properties of medical image data (e.g., three
dimensionality), which are more relevant to medical image analysis than generic
ones for natural images. However, previous work rarely paid attention to data
with anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance
imaging views. As these imaging planes are defined according to the anatomy of
the imaged organ, pretext tasks effectively exploiting this information can
pretrain the networks to gain knowledge on the organ of interest. In this work,
we propose two complementary pretext tasks for this group of medical image data
based on the spatial relationship of the imaging planes. The first is to learn
the relative orientation between the imaging planes and implemented as
regressing their intersecting lines. The second exploits parallel imaging
planes to regress their relative slice locations within a stack. Both pretext
tasks are conceptually straightforward and easy to implement, and can be
combined in multitask learning for better representation learning. Thorough
experiments on two anatomical structures (heart and knee) and representative
target tasks (semantic segmentation and classification) demonstrate that the
proposed pretext tasks are effective in pretraining deep networks for
remarkably boosted performance on the target tasks, and superior to other
recent approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathoTune: Adapting Visual Foundation Model to Pathological Specialists <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Lu, Fang Yan, Xiaofan Zhang, Yue Gao, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As natural image understanding moves towards the pretrain-finetune era,
research in pathology imaging is concurrently evolving. Despite the predominant
focus on pretraining pathological foundation models, how to adapt foundation
models to downstream tasks is little explored. For downstream adaptation, we
propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the
Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework
designed to efficiently adapt pathological or even visual foundation models to
pathology-specific tasks via multi-modal prompt tuning. The proposed framework
leverages Task-specific Visual Prompts and Task-specific Textual Prompts to
identify task-relevant features, along with Instance-specific Visual Prompts
for encoding single pathological image features. Results across multiple
datasets at both patch-level and WSI-level demonstrate its superior performance
over single-modality prompt tuning approaches. Significantly, PathoTune
facilitates the direct adaptation of natural visual foundation models to
pathological tasks, drastically outperforming pathological foundation models
with simple linear probing. The code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid
  Convolution and <span class="highlight-title">Transformer</span> Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xu, Junjie Luo, Qi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CT-Bound, a fast boundary estimation method for noisy images using
a hybrid Convolution and Transformer neural network. The proposed architecture
decomposes boundary estimation into two tasks: local detection and global
regularization of image boundaries. It first estimates a parametric
representation of boundary structures only using the input image within a small
receptive field and then refines the boundary structure in the parameter domain
without accessing the input image. Because of this, a part of the network can
be easily trained using naive, synthetic images and still generalized to real
images, and the entire architecture is computationally efficient as the
boundary refinement is non-iterative and not in the image domain. Compared with
the previous highest accuracy methods, our experiment shows that CT-Bound is
100 times faster, producing comparably accurate, high-quality boundary and
color maps. We also demonstrate that CT-Bound can produce boundary and color
maps on real captured images without extra fine-tuning and real-time boundary
map and color map videos at ten frames per second.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaojie Ji, Yufeng Li, Yiyi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work tackles the challenging task of achieving real-time novel view
synthesis on various scenes, including highly reflective objects and unbounded
outdoor scenes. Existing real-time rendering methods, especially those based on
meshes, often have subpar performance in modeling surfaces with rich
view-dependent appearances. Our key idea lies in leveraging meshes for
rendering acceleration while incorporating a novel approach to parameterize
view-dependent information. We decompose the color into diffuse and specular,
and model the specular color in the reflected direction based on a neural
environment map. Our experiments demonstrate that our method achieves
comparable reconstruction quality for highly reflective surfaces compared to
state-of-the-art offline methods, while also efficiently enabling real-time
rendering on edge devices such as smartphones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page:https://xdimlab.github.io/REFRAME/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Camera-aware Label Refinement for Unsupervised Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengna Li, Kangyi Wu, Wenli Huang, Sanping Zhou, Jinjun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised person re-identification aims to retrieve images of a specified
person without identity labels. Many recent unsupervised Re-ID approaches adopt
clustering-based methods to measure cross-camera feature similarity to roughly
divide images into clusters. They ignore the feature distribution discrepancy
induced by camera domain gap, resulting in the unavoidable performance
degradation. Camera information is usually available, and the feature
distribution in the single camera usually focuses more on the appearance of the
individual and has less intra-identity variance. Inspired by the observation,
we introduce a \textbf{C}amera-\textbf{A}ware \textbf{L}abel
\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by
clustering intra-camera similarity. Specifically, we employ intra-camera
training to obtain reliable local pseudo labels within each camera, and then
refine global labels generated by inter-camera clustering and train the
discriminative model using more reliable global pseudo labels in a self-paced
manner. Meanwhile, we develop a camera-alignment module to align feature
distributions under different cameras, which could help deal with the camera
variance further. Extensive experiments validate the superiority of our
proposed method over state-of-the-art approaches. The code is accessible at
https://github.com/leeBooMla/CALR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ If CLIP Could Talk: Understanding Vision-Language Model Representations
  Through Their Preferred Concept Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works often assume that Vision-Language Model (VLM) representations
are based on visual attributes like shape. However, it is unclear to what
extent VLMs prioritize this information to represent concepts. We propose
Extract and Explore (EX2), a novel approach to characterize important textual
features for VLMs. EX2 uses reinforcement learning to align a large language
model with VLM preferences and generates descriptions that incorporate the
important features for the VLM. Then, we inspect the descriptions to identify
the features that contribute to VLM representations. We find that spurious
descriptions have a major role in VLM representations despite providing no
helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,
among informative descriptions, VLMs rely significantly on non-visual
attributes like habitat to represent visual concepts. Also, our analysis
reveals that different VLMs prioritize different attributes in their
representations. Overall, we show that VLMs do not simply match images to scene
descriptions and that non-visual or even spurious descriptions significantly
influence their representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/BatsResearch/ex2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Lin, Zhe Liu, Zhongyu Xia, Xinhao Wang, Yongtao Wang, Shengxiang Qi, Yang Dong, Nan Dong, Le Zhang, Ce Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional object detection is one of the key tasks in autonomous
driving. To reduce costs in practice, low-cost multi-view cameras for 3D object
detection are proposed to replace the expansive LiDAR sensors. However, relying
solely on cameras is difficult to achieve highly accurate and robust 3D object
detection. An effective solution to this issue is combining multi-view cameras
with the economical millimeter-wave radar sensor to achieve more reliable
multi-modal 3D object detection. In this paper, we introduce RCBEVDet, a
radar-camera fusion 3D object detection method in the bird's eye view (BEV).
Specifically, we first design RadarBEVNet for radar BEV feature extraction.
RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section
(RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based
encoder and a transformer-based encoder are proposed to extract radar features,
with an injection and extraction module to facilitate communication between the
two encoders. The RCS-aware BEV encoder takes RCS as the object size prior to
scattering the point feature in BEV. Besides, we present the Cross-Attention
Multi-layer Fusion module to automatically align the multi-modal BEV feature
from radar and camera with the deformable attention mechanism, and then fuse
the feature with channel and spatial fusion layers. Experimental results show
that RCBEVDet achieves new state-of-the-art radar-camera fusion results on
nuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore,
RCBEVDet achieves better 3D detection results than all real-time camera-only
and radar-camera 3D object detectors with a faster inference speed at 21~28
FPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Producing and Leveraging Online Map Uncertainty in Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, Boris Ivanovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) maps have played an integral role in the development of
modern autonomous vehicle (AV) stacks, albeit with high associated labeling and
maintenance costs. As a result, many recent works have proposed methods for
estimating HD maps online from sensor data, enabling AVs to operate outside of
previously-mapped regions. However, current online map estimation approaches
are developed in isolation of their downstream tasks, complicating their
integration in AV stacks. In particular, they do not produce uncertainty or
confidence estimates. In this work, we extend multiple state-of-the-art online
map estimation methods to additionally estimate uncertainty and show how this
enables more tightly integrating online mapping with trajectory forecasting. In
doing so, we find that incorporating uncertainty yields up to 50% faster
training convergence and up to 15% better prediction performance on the
real-world nuScenes driving dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures, 6 tables. CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Neuron Segmentation for Voltage Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yosuke Bando, Ramdas Pillai, Atsushi Kajita, Farhan Abdul Hakeem, Yves Quemener, Hua-an Tseng, Kiryl D. Piatkevich, Changyang Linghu, Xue Han, Edward S. Boyden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In voltage imaging, where the membrane potentials of individual neurons are
recorded at from hundreds to thousand frames per second using fluorescence
microscopy, data processing presents a challenge. Even a fraction of a minute
of recording with a limited image size yields gigabytes of video data
consisting of tens of thousands of frames, which can be time-consuming to
process. Moreover, millisecond-level short exposures lead to noisy video
frames, obscuring neuron footprints especially in deep-brain samples where
noisy signals are buried in background fluorescence. To address this challenge,
we propose a fast neuron segmentation method able to detect multiple,
potentially overlapping, spiking neurons from noisy video frames, and implement
a data processing pipeline incorporating the proposed segmentation method along
with GPU-accelerated motion correction. By testing on existing datasets as well
as on new datasets we introduce, we show that our pipeline extracts neuron
footprints that agree well with human annotation even from cluttered datasets,
and demonstrate real-time processing of voltage imaging data on a single
desktop computer for the first time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOCTR: Disentangled Object-Centric <span class="highlight-title">Transformer</span> for Point Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Yu, Hao Wang, Weiming Li, Qiang Wang, Soonyong Cho, Younghun Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point scene understanding is a challenging task to process real-world scene
point cloud, which aims at segmenting each object, estimating its pose, and
reconstructing its mesh simultaneously. Recent state-of-the-art method first
segments each object and then processes them independently with multiple stages
for the different sub-tasks. This leads to a complex pipeline to optimize and
makes it hard to leverage the relationship constraints between multiple
objects. In this work, we propose a novel Disentangled Object-Centric
TRansformer (DOCTR) that explores object-centric representation to facilitate
learning with multiple objects for the multiple sub-tasks in a unified manner.
Each object is represented as a query, and a Transformer decoder is adapted to
iteratively optimize all the queries involving their relationship. In
particular, we introduce a semantic-geometry disentangled query (SGDQ) design
that enables the query features to attend separately to semantic information
and geometric information relevant to the corresponding sub-tasks. A hybrid
bipartite matching module is employed to well use the supervisions from all the
sub-tasks during training. Qualitative and quantitative experimental results
demonstrate that our method achieves state-of-the-art performance on the
challenging ScanNet dataset. Code is available at
https://github.com/SAITPublic/DOCTR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarks and Challenges in Pose Estimation for Egocentric Hand
  Interactions with Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We interact with the world with our hands and see it through our own
(egocentric) perspective. A holistic 3D understanding of such interactions from
egocentric views is important for tasks in robotics, AR/VR, action recognition
and motion generation. Accurately reconstructing such interactions in 3D is
challenging due to heavy occlusion, viewpoint bias, camera distortion, and
motion blur from the head movement. To this end, we designed the HANDS23
challenge based on the AssemblyHands and ARCTIC datasets with carefully
designed training and testing splits. Based on the results of the top submitted
methods and more recent baselines on the leaderboards, we perform a thorough
analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates
the effectiveness of addressing distortion specific to egocentric cameras,
adopting high-capacity transformers to learn complex hand-object interactions,
and fusing predictions from different views. Our study further reveals
challenging scenarios intractable with state-of-the-art methods, such as fast
hand motion, object reconstruction from narrow egocentric views, and close
contact between two hands and objects. Our efforts will enrich the community's
knowledge foundation and facilitate future hand studies on egocentric
hand-object interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in
  Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul B. Nair, Michael Milford, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are increasingly popular in robotics due to their beneficial
features, such as low latency, energy efficiency, and high dynamic range.
Nevertheless, their downstream task performance is greatly influenced by the
optimization of bias parameters. These parameters, for instance, regulate the
necessary change in light intensity to trigger an event, which in turn depends
on factors such as the environment lighting and camera motion. This paper
introduces feedback control algorithms that automatically tune the bias
parameters through two interacting methods: 1) An immediate, on-the-fly fast
adaptation of the refractory period, which sets the minimum interval between
consecutive events, and 2) if the event rate exceeds the specified bounds even
after changing the refractory period repeatedly, the controller adapts the
pixel bandwidth and event thresholds, which stabilizes after a short period of
noise events across all pixels (slow adaptation). Our evaluation focuses on the
visual place recognition task, where incoming query images are compared to a
given reference database. We conducted comprehensive evaluations of our
algorithms' adaptive feedback control in real-time. To do so, we collected the
QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366
repeated traversals of a Scout Mini robot navigating through a 100 meter long
indoor lab setting (totaling over 35km distance traveled) in varying brightness
conditions with ground truth location information. Our proposed feedback
controllers result in superior performance when compared to the standard bias
settings and prior feedback control methods. Our findings also detail the
impact of bias adjustments on task performance and feature ablation studies on
the fast and slow adaptation mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refining Text-to-Image Generation: Towards Accurate Training-Free
  Glyph-Enhanced Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanyam Lakhanpal, Shivang Chopra, Vinija Jain, Aman Chadha, Man Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past few years, Text-to-Image (T2I) generation approaches based on
diffusion models have gained significant attention. However, vanilla diffusion
models often suffer from spelling inaccuracies in the text displayed within the
generated images. The capability to generate visual text is crucial, offering
both academic interest and a wide range of practical applications. To produce
accurate visual text images, state-of-the-art techniques adopt a
glyph-controlled image generation approach, consisting of a text layout
generator followed by an image generator that is conditioned on the generated
text layout. Nevertheless, our study reveals that these models still face three
primary challenges, prompting us to develop a testbed to facilitate future
research. We introduce a benchmark, LenCom-Eval, specifically designed for
testing models' capability in generating images with Lengthy and Complex visual
text. Subsequently, we introduce a training-free framework to enhance the
two-stage generation approaches. We examine the effectiveness of our approach
on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable
improvements across a range of evaluation metrics, including CLIPScore, OCR
precision, recall, F1 score, accuracy, and edit distance scores. For instance,
our proposed framework improves the backbone model, TextDiffuser, by more than
23\% and 13.5\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval,
respectively. Our work makes a unique contribution to the field by focusing on
generating images with long and rare text sequences, a niche previously
unexplored by existing literature
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Template-assisted Point Cloud Shape Correspondence Network <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Deng, Jiahao Lu, Tianzhu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised point cloud shape correspondence aims to establish point-wise
correspondences between source and target point clouds. Existing methods obtain
correspondences directly by computing point-wise feature similarity between
point clouds. However, non-rigid objects possess strong deformability and
unusual shapes, making it a longstanding challenge to directly establish
correspondences between point clouds with unconventional shapes. To address
this challenge, we propose an unsupervised Template-Assisted point cloud shape
correspondence Network, termed TANet, including a template generation module
and a template assistance module. The proposed TANet enjoys several merits.
Firstly, the template generation module establishes a set of learnable
templates with explicit structures. Secondly, we introduce a template
assistance module that extensively leverages the generated templates to
establish more accurate shape correspondences from multiple perspectives.
Extensive experiments on four human and animal datasets demonstrate that TANet
achieves favorable performance against state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spike-NeRF: Neural Radiance Field Based On Spike Camera <span class="chip">ICME2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Guo, Yuanxi Bai, Liwen Hu, Mianzhi Liu, Ziyi Guo, Lei Ma, Tiejun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a neuromorphic sensor with high temporal resolution, spike cameras offer
notable advantages over traditional cameras in high-speed vision applications
such as high-speed optical estimation, depth estimation, and object tracking.
Inspired by the success of the spike camera, we proposed Spike-NeRF, the first
Neural Radiance Field derived from spike data, to achieve 3D reconstruction and
novel viewpoint synthesis of high-speed scenes. Instead of the multi-view
images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike
streams captured by a moving spike camera in a very short time. To reconstruct
a correct and stable 3D scene from high-frequency but unstable spike data, we
devised spike masks along with a distinctive loss function. We evaluate our
method qualitatively and numerically on several challenging synthetic scenes
generated by blender with the spike camera simulator. Our results demonstrate
that Spike-NeRF produces more visually appealing results than the existing
methods and the baseline we proposed in high-speed scenes. Our code and data
will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ICME2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Long Video Generation: Challenges, Methods, and Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation is a rapidly advancing research area, garnering significant
attention due to its broad range of applications. One critical aspect of this
field is the generation of long-duration videos, which presents unique
challenges and opportunities. This paper presents the first survey of recent
advancements in long video generation and summarises them into two key
paradigms: divide and conquer temporal autoregressive.
  We delve into the common models employed in each paradigm, including aspects
of network design and conditioning techniques. Furthermore, we offer a
comprehensive overview and classification of the datasets and evaluation
metrics which are crucial for advancing long video generation research.
Concluding with a summary of existing studies, we also discuss the emerging
challenges and future directions in this dynamic field. We hope that this
survey will serve as an essential reference for researchers and practitioners
in the realm of long video generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble Adversarial Defense via Integration of Multiple Dispersed Low
  Curvature Models <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaikang Zhao, Xi Chen, Wei Huang, Liuxin Ding, Xianglong Kong, Fan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of an ensemble of deep learning models has been extensively
explored to enhance defense against adversarial attacks. The diversity among
sub-models increases the attack cost required to deceive the majority of the
ensemble, thereby improving the adversarial robustness. While existing
approaches mainly center on increasing diversity in feature representations or
dispersion of first-order gradients with respect to input, the limited
correlation between these diversity metrics and adversarial robustness
constrains the performance of ensemble adversarial defense. In this work, we
aim to enhance ensemble diversity by reducing attack transferability. We
identify second-order gradients, which depict the loss curvature, as a key
factor in adversarial robustness. Computing the Hessian matrix involved in
second-order gradients is computationally expensive. To address this, we
approximate the Hessian-vector product using differential approximation. Given
that low curvature provides better robustness, our ensemble model was designed
to consider the influence of curvature among different sub-models. We introduce
a novel regularizer to train multiple more-diverse low-curvature network
models. Extensive experiments across various datasets demonstrate that our
ensemble model exhibits superior robustness against a range of attacks,
underscoring the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The 2024 International Joint Conference on Neural
  Networks (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Schieber, Shiyu Li, Niklas Corell, Philipp Beckerle, Julian Kreimeier, Daniel Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical and industrial domains, providing guidance for assembly processes
is critical to ensure efficiency and safety. Errors in assembly can lead to
significant consequences such as extended surgery times, and prolonged
manufacturing or maintenance times in industry. Assembly scenarios can benefit
from in-situ AR visualization to provide guidance, reduce assembly times and
minimize errors. To enable in-situ visualization 6D pose estimation can be
leveraged. Existing 6D pose estimation techniques primarily focus on individual
objects and static captures. However, assembly scenarios have various dynamics
including occlusion during assembly and dynamics in the assembly objects
appearance. Existing work, combining object detection/6D pose estimation and
assembly state detection focuses either on pure deep learning-based approaches,
or limit the assembly state detection to building blocks. To address the
challenges of 6D pose estimation in combination with assembly state detection,
our approach ASDF builds upon the strengths of YOLOv8, a real-time capable
object detection framework. We extend this framework, refine the object pose
and fuse pose knowledge with network-detected pose information. Utilizing our
late fusion in our Pose2State module results in refined 6D pose estimation and
assembly state detection. By combining both pose and state information, our
Pose2State module predicts the final assembly state with precision. Our
evaluation on our ASDF dataset shows that our Pose2State module leads to an
improved assembly state detection and that the improvement of the assembly
state further leads to a more robust 6D pose estimation. Moreover, on the GBOT
dataset, we outperform the pure deep learning-based network, and even
outperform the hybrid and pure tracking-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-attention Associate Prediction Network for Visual Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinglong Sun, Haijiang Sun, Shan Jiang, Jiacheng Wang, Xilai Wei, Zhonghe Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification-regression prediction networks have realized impressive
success in several modern deep trackers. However, there is an inherent
difference between classification and regression tasks, so they have diverse
even opposite demands for feature matching. Existed models always ignore the
key issue and only employ a unified matching block in two task branches,
decaying the decision quality. Besides, these models also struggle with
decision misalignment situation. In this paper, we propose a multi-attention
associate prediction network (MAPNet) to tackle the above problems. Concretely,
two novel matchers, i.e., category-aware matcher and spatial-aware matcher, are
first designed for feature comparison by integrating self, cross, channel or
spatial attentions organically. They are capable of fully capturing the
category-related semantics for classification and the local spatial contexts
for regression, respectively. Then, we present a dual alignment module to
enhance the correspondences between two branches, which is useful to find the
optimal tracking solution. Finally, we describe a Siamese tracker built upon
the proposed prediction network, which achieves the leading performance on five
tracking benchmarks, consisting of LaSOT, TrackingNet, GOT-10k, TNL2k and
UAV123, and surpasses other state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and
  Interactive Image Fusion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunpeng Yi, Han Xu, Hao Zhang, Linfeng Tang, Jiayi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image fusion aims to combine information from different source images to
create a comprehensively representative image. Existing fusion methods are
typically helpless in dealing with degradations in low-quality source images
and non-interactive to multiple subjective and objective needs. To solve them,
we introduce a novel approach that leverages semantic text guidance image
fusion model for degradation-aware and interactive image fusion task, termed as
Text-IF. It innovatively extends the classical image fusion to the text guided
image fusion along with the ability to harmoniously address the degradation and
interaction issues during fusion. Through the text semantic encoder and
semantic interaction fusion decoder, Text-IF is accessible to the all-in-one
infrared and visible image degradation-aware processing and the interactive
flexible fusion outcomes. In this way, Text-IF achieves not only multi-modal
image fusion, but also multi-modal information fusion. Extensive experiments
prove that our proposed text guided image fusion strategy has obvious
advantages over SOTA methods in the image fusion performance and degradation
treatment. The code is available at https://github.com/XunpengYi/Text-IF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dia-LLaMA: Towards Large Language Model-driven CT Report Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Chen, Luyang Luo, Yequan Bie, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical report generation has achieved remarkable advancements yet has still
been faced with several challenges. First, the inherent imbalance in the
distribution of normal and abnormal cases may lead models to exhibit a biased
focus on normal samples, resulting in unreliable diagnoses. Second, the
frequent occurrence of common template sentences in the reports may overwhelm
the critical abnormal information. Moreover, existing works focus on 2D chest
X-rays, leaving CT report generation underexplored due to the high-dimensional
nature of CT images and the limited availability of CT-report pairs. Recently,
LLM has shown a great ability to generate reliable answers with appropriate
prompts, which shed light on addressing the aforementioned challenges. In this
paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report
generation by incorporating diagnostic information as guidance prompts.
Considering the high dimension of CT, we leverage a pre-trained ViT3D with
perceiver to extract the visual information. To tailor the LLM for report
generation and emphasize abnormality, we extract additional diagnostic
information by referring to a disease prototype memory bank, which is updated
during training to capture common disease representations. Furthermore, we
introduce disease-aware attention to enable the model to adjust attention for
different diseases. Experiments on the chest CT dataset demonstrated that our
proposed method outperformed previous methods and achieved state-of-the-art on
both clinical efficacy performance and natural language generation metrics. The
code will be made publically available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators
  for Reasoning-Based Chart VQA <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhuowan, Jasani Bhavan, Tang Peng, Ghadar Shabnam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding data visualizations like charts and plots requires reasoning
about both visual elements and numerics. Although strong in extractive
questions, current chart visual question answering (chart VQA) models suffer on
complex reasoning questions. In this work, we address the lack of reasoning
ability by data augmentation. We leverage Large Language Models (LLMs), which
have shown to have strong reasoning ability, as an automatic data annotator
that generates question-answer annotations for chart images. The key innovation
in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data
generator learns to decompose the complex question into step-by-step
sub-questions (rationales), which are then used to derive the final answer
using external tools, i.e. Python. This step-wise generation procedure is
trained on synthetic data generated using a template-based QA generation
pipeline. Experimental results highlight the significance of the proposed
step-by-step generation. By training with the LLM-augmented data (LAMENDA), we
significantly enhance the chart VQA models, achieving the state-of-the-art
accuracy on the ChartQA and PlotQA datasets. In particular, our approach
improves the accuracy of the previous state-of-the-art approach from 38% to 54%
on the human-written questions in the ChartQA dataset, which needs strong
reasoning. We hope our work underscores the potential of synthetic data and
encourages further exploration of data augmentation using LLMs for
reasoning-heavy tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Residual Dense Swin <span class="highlight-title">Transformer</span> for Continuous Depth-Independent
  Ultrasound Imaging <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintong Hu, Hui Che, Zishuo Li, Wenming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging is crucial for evaluating organ morphology and function,
yet depth adjustment can degrade image quality and field-of-view, presenting a
depth-dependent dilemma. Traditional interpolation-based zoom-in techniques
often sacrifice detail and introduce artifacts. Motivated by the potential of
arbitrary-scale super-resolution to naturally address these inherent
challenges, we present the Residual Dense Swin Transformer Network (RDSTN),
designed to capture the non-local characteristics and long-range dependencies
intrinsic to ultrasound images. It comprises a linear embedding module for
feature enhancement, an encoder with shifted-window attention for modeling
non-locality, and an MLP decoder for continuous detail reconstruction. This
strategy streamlines balancing image quality and field-of-view, which offers
superior textures over traditional methods. Experimentally, RDSTN outperforms
existing approaches while requiring fewer parameters. In conclusion, RDSTN
shows promising potential for ultrasound image enhancement by overcoming the
limitations of conventional interpolation-based methods and achieving
depth-independent imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024, https://ieeexplore.ieee.org/document/10447712</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlashEval: Towards Fast and Accurate Evaluation of Text-to-image
  Diffusion Generative Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been significant progress in the development of
text-to-image generative models. Evaluating the quality of the generative
models is one essential step in the development process. Unfortunately, the
evaluation process could consume a significant amount of computational
resources, making the required periodic evaluation of model performance (e.g.,
monitoring training progress) impractical. Therefore, we seek to improve the
evaluation efficiency by selecting the representative subset of the text-image
dataset. We systematically investigate the design choices, including the
selection criteria (textural features or image-based metrics) and the selection
granularity (prompt-level or set-level). We find that the insights from prior
work on subset selection for training data do not generalize to this problem,
and we propose FlashEval, an iterative search algorithm tailored to evaluation
data selection. We demonstrate the effectiveness of FlashEval on ranking
diffusion models with various configurations, including architectures,
quantization levels, and sampler schedules on COCO and DiffusionDB datasets.
Our searched 50-item subset could achieve comparable evaluation quality to the
randomly sampled 500-item subset for COCO annotations on unseen models,
achieving a 10x evaluation speedup. We release the condensed subset of these
commonly used datasets to help facilitate diffusion algorithm design and
evaluation, and open-source FlashEval as a tool for condensing future datasets,
accessible at https://github.com/thu-nics/FlashEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and
  Distance-Aware Bi-Projection Fusion <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ai, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  360 depth estimation has recently received great attention for 3D
reconstruction owing to its omnidirectional field of view (FoV). Recent
approaches are predominantly focused on cross-projection fusion with
geometry-based re-projection: they fuse 360 images with equirectangular
projection (ERP) and another projection type, e.g., cubemap projection to
estimate depth with the ERP format. However, these methods suffer from 1)
limited local receptive fields, making it hardly possible to capture large FoV
scenes, and 2) prohibitive computational cost, caused by the complex
cross-projection fusion module design. In this paper, we propose Elite360D, a
novel framework that inputs the ERP image and icosahedron projection (ICOSAP)
point set, which is undistorted and spatially continuous. Elite360D is superior
in its capacity in learning a representation from a local-with-global
perspective. With a flexible ERP image encoder, it includes an ICOSAP point
encoder, and a Bi-projection Bi-attention Fusion (B2F) module (totally ~1M
parameters). Specifically, the ERP image encoder can take various perspective
image-trained backbones (e.g., ResNet, Transformer) to extract local features.
The point encoder extracts the global features from the ICOSAP. Then, the B2F
module captures the semantic- and distance-aware dependencies between each
pixel of the ERP feature and the entire ICOSAP feature set. Without specific
backbone design and obvious computational cost increase, Elite360D outperforms
the prior arts on several benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model
  for Distortion-aware Panoramic Semantic Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiming Zhang, Yexin Liu, Xu Zheng, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles a novel yet challenging problem: how to transfer knowledge
from the emerging Segment Anything Model (SAM) -- which reveals impressive
zero-shot instance segmentation capacity -- to learn a compact panoramic
semantic segmentation model, i.e., student, without requiring any labeled data.
This poses considerable challenges due to SAM's inability to provide semantic
labels and the large capacity gap between SAM and the student. To this end, we
propose a novel framework, called GoodSAM, that introduces a teacher assistant
(TA) to provide semantic information, integrated with SAM to generate ensemble
logits to achieve knowledge transfer. Specifically, we propose a
Distortion-Aware Rectification (DAR) module that first addresses the distortion
problem of panoramic images by imposing prediction-level consistency and
boundary enhancement. This subtly enhances TA's prediction capacity on
panoramic images. DAR then incorporates a cross-task complementary fusion block
to adaptively merge the predictions of SAM and TA to obtain more reliable
ensemble logits. Moreover, we introduce a Multi-level Knowledge Adaptation
(MKA) module to efficiently transfer the multi-level feature knowledge from TA
and ensemble logits to learn a compact student model. Extensive experiments on
two benchmarks show that our GoodSAM achieves a remarkable +3.75\% mIoU
improvement over the state-of-the-art (SOTA) domain adaptation methods. Also,
our most lightweight model achieves comparable performance to the SOTA methods
with only 3.7M parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Semantic Priors from SAM to Efficient Image Restoration
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zhang, Xiaoyu Liu, Wei Li, Hanting Chen, Junchao Liu, Jie Hu, Zhiwei Xiong, Chun Yuan, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In image restoration (IR), leveraging semantic priors from segmentation
models has been a common approach to improve performance. The recent segment
anything model (SAM) has emerged as a powerful tool for extracting advanced
semantic priors to enhance IR tasks. However, the computational cost of SAM is
prohibitive for IR, compared to existing smaller IR models. The incorporation
of SAM for extracting semantic priors considerably hampers the model inference
efficiency. To address this issue, we propose a general framework to distill
SAM's semantic knowledge to boost exiting IR models without interfering with
their inference process. Specifically, our proposed framework consists of the
semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD)
scheme. SPF fuses two kinds of information between the restored image predicted
by the original IR model and the semantic mask predicted by SAM for the refined
restored image. SPD leverages a self-distillation manner to distill the fused
semantic priors to boost the performance of original IR models. Additionally,
we design a semantic-guided relation (SGR) module for SPD, which ensures
semantic feature representation space consistency to fully distill the priors.
We demonstrate the effectiveness of our framework across multiple IR models and
tasks, including deraining, deblurring, and denoising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Potent Poisons and Backdoors from Scratch with Guided
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural networks are often trained on massive datasets that are web
scraped with minimal human inspection. As a result of this insecure curation
pipeline, an adversary can poison or backdoor the resulting model by uploading
malicious data to the internet and waiting for a victim to scrape and train on
it. Existing approaches for creating poisons and backdoors start with randomly
sampled clean data, called base samples, and then modify those samples to craft
poisons. However, some base samples may be significantly more amenable to
poisoning than others. As a result, we may be able to craft more potent poisons
by carefully choosing the base samples. In this work, we use guided diffusion
to synthesize base samples from scratch that lead to significantly more potent
poisons and backdoors than previous state-of-the-art attacks. Our Guided
Diffusion Poisoning (GDP) base samples can be combined with any downstream
poisoning or backdoor attack to boost its effectiveness. Our implementation
code is publicly available at: https://github.com/hsouri/GDP .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RSTAR: Rotational Streak Artifact Reduction in 4D CBCT using Separable
  and Circular Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Deng, Hua Chen, Haibo Hu, Zhiyong Xu, Tianling Lyu, Yan Xi, Yang Chen, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Four-dimensional cone-beam computed tomography (4D CBCT) provides
respiration-resolved images and can be used for image-guided radiation therapy.
However, the ability to reveal respiratory motion comes at the cost of image
artifacts. As raw projection data are sorted into multiple respiratory phases,
there is a limited number of cone-beam projections available for image
reconstruction. Consequently, the 4D CBCT images are covered by severe streak
artifacts. Although several deep learning-based methods have been proposed to
address this issue, most algorithms employ ordinary network models, neglecting
the intrinsic structural prior within 4D CBCT images. In this paper, we first
explore the origin and appearance of streak artifacts in 4D CBCT
images.Specifically, we find that streak artifacts exhibit a periodic
rotational motion along with the patient's respiration. This unique motion
pattern inspires us to distinguish the artifacts from the desired anatomical
structures in the spatiotemporal domain. Thereafter, we propose a
spatiotemporal neural network named RSTAR-Net with separable and circular
convolutions for Rotational Streak Artifact Reduction. The specially designed
model effectively encodes dynamic image features, facilitating the recovery of
4D CBCT images. Moreover, RSTAR-Net is also lightweight and computationally
efficient. Extensive experiments substantiate the effectiveness of our proposed
method, and RSTAR-Net shows superior performance to comparison methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChebMixer: Efficient Graph Representation Learning with MLP Mixer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Kui, Haonan Yan, Qinsong Li, Liming Chen, Beiji Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks have achieved remarkable success in learning graph
representations, especially graph Transformer, which has recently shown
superior performance on various graph mining tasks. However, graph Transformer
generally treats nodes as tokens, which results in quadratic complexity
regarding the number of nodes during self-attention computation. The graph MLP
Mixer addresses this challenge by using the efficient MLP Mixer technique from
computer vision. However, the time-consuming process of extracting graph tokens
limits its performance. In this paper, we present a novel architecture named
ChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based
spectral filtering to extract a sequence of tokens. Firstly, we produce
multiscale representations of graph nodes via fast Chebyshev polynomial-based
spectral filtering. Next, we consider each node's multiscale representations as
a sequence of tokens and refine the node representation with an effective MLP
Mixer. Finally, we aggregate the multiscale representations of nodes through
Chebyshev interpolation. Owing to the powerful representation capabilities and
fast computational properties of MLP Mixer, we can quickly extract more
informative node representations to improve the performance of downstream
tasks. The experimental results prove our significant improvements in a variety
of scenarios ranging from graph node classification to medical image
segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-EffiViTCaps: 3D Efficient Vision <span class="highlight-title">Transformer</span> with Capsule for Medical
  Image Segmentation <span class="chip">ICPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwei Gan, Ming Chang, Juan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation (MIS) aims to finely segment various organs. It
requires grasping global information from both parts and the entire image for
better segmenting, and clinically there are often certain requirements for
segmentation efficiency. Convolutional neural networks (CNNs) have made
considerable achievements in MIS. However, they are difficult to fully collect
global context information and their pooling layer may cause information loss.
Capsule networks, which combine the benefits of CNNs while taking into account
additional information such as relative location that CNNs do not, have lately
demonstrated some advantages in MIS. Vision Transformer (ViT) employs
transformers in visual tasks. Transformer based on attention mechanism has
excellent global inductive modeling capabilities and is expected to capture
longrange information. Moreover, there have been resent studies on making ViT
more lightweight to minimize model complexity and increase efficiency. In this
paper, we propose a U-shaped 3D encoder-decoder network named 3D-EffiViTCaps,
which combines 3D capsule blocks with 3D EfficientViT blocks for MIS. Our
encoder uses capsule blocks and EfficientViT blocks to jointly capture local
and global semantic information more effectively and efficiently with less
information loss, while the decoder employs CNN blocks and EfficientViT blocks
to catch ffner details for segmentation. We conduct experiments on various
datasets, including iSeg-2017, Hippocampus and Cardiac to verify the
performance and efficiency of 3D-EffiViTCaps, which performs better than
previous 3D CNN-based, 3D Capsule-based and 3D Transformer-based models. We
further implement a series of ablation experiments on the main blocks. Our code
is available at: https://github.com/HidNeuron/3D-EffiViTCaps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, submitted to ICPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Video Compression Artifacts on Fisheye Camera Visual
  Perception Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhumitha Sakthi, Louis Kerofsky, Varun Ravi Kumar, Senthil Yogamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving systems require extensive data collection schemes to cover
the diverse scenarios needed for building a robust and safe system. The data
volumes are in the order of Exabytes and have to be stored for a long period of
time (i.e., more than 10 years of the vehicle's life cycle). Lossless
compression doesn't provide sufficient compression ratios, hence, lossy video
compression has been explored. It is essential to prove that lossy video
compression artifacts do not impact the performance of the perception
algorithms. However, there is limited work in this area to provide a solid
conclusion. In particular, there is no such work for fisheye cameras, which
have high radial distortion and where compression may have higher artifacts.
Fisheye cameras are commonly used in automotive systems for 3D object detection
task. In this work, we provide the first analysis of the impact of standard
video compression codecs on wide FOV fisheye camera images. We demonstrate that
the achievable compression with negligible impact depends on the dataset and
temporal prediction of the video codec. We propose a radial distortion-aware
zonal metric to evaluate the performance of artifacts in fisheye images. In
addition, we present a novel method for estimating affine mode parameters of
the latest VVC codec, and suggest some areas for improvement in video codecs
for the application to fisheye imagery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEDDAP: Medical <span class="highlight-title">Dataset</span> Enhancement via Diversified Augmentation
  Pipeline <span class="chip">MICCAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasamin Medghalchi, Niloufar Zakariaei, Arman Rahmim, Ilker Hacihaliloglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the
abundance and accuracy of available training data. However, collecting and
annotating data on a large scale is often both costly and time-intensive,
particularly in medical cases where practitioners are already occupied with
their duties. Moreover, ensuring that the model remains robust across various
scenarios of image capture is crucial in medical domains, especially when
dealing with ultrasound images that vary based on the settings of different
devices and the manual operation of the transducer. To address this challenge,
we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion
(SD) models to augment existing small datasets by automatically generating new
informative labeled samples. Pretrained checkpoints for SD are typically based
on natural images, and training them for medical images requires significant
GPU resources due to their heavy parameters. To overcome this challenge, we
introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method
tailored specifically for ultrasound applications. USLoRA allows for selective
fine-tuning of weights within SD, requiring fewer than 0.1\% of parameters
compared to fully fine-tuning only the UNet portion of SD. To enhance dataset
diversity, we incorporate different adjectives into the generation process
prompts, thereby desensitizing the classifiers to intensity changes across
different images. This approach is inspired by clinicians' decision-making
processes regarding breast tumors, where tumor shape often plays a more crucial
role than intensity. In conclusion, our pipeline not only outperforms
classifiers trained on the original dataset but also demonstrates superior
performance when encountering unseen datasets. The source code is available at
https://github.com/yasamin-med/MEDDAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to miccai 2024 submitted to miccai 2024 Submitted to
  MICCAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Xuansong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Model (MLLMs) leverages Large Language Models as a
cognitive framework for diverse visual-language tasks. Recent efforts have been
made to equip MLLMs with visual perceiving and grounding capabilities. However,
there still remains a gap in providing fine-grained pixel-level perceptions and
extending interactions beyond text-specific inputs. In this work, we propose
{\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object
perceptions and natural language descriptions from multi-modality references,
such as texts, boxes, images, or audio. This innovation empowers users with
greater flexibility to engage with the model beyond textual and regional
prompts, without modality-specific designs. Through our proposed refocusing
mechanism, the generated grounding output is guided to better focus on the
referenced object, implicitly incorporating additional pixel-level supervision.
This simple modification utilizes attention scores generated during the
inference of LLM, eliminating the need for extra computations while exhibiting
performance enhancements in both grounding masks and referring expressions.
With only publicly available training data, our model achieves state-of-the-art
results across multiple benchmarks, including diverse modality referring
segmentation and region-level referring expression generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word4Per: Zero-shot Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for specific person has great social benefits and security value,
and it often involves a combination of visual and textual information.
Conventional person retrieval methods, whether image-based or text-based,
usually fall short in effectively harnessing both types of information, leading
to the loss of accuracy. In this paper, a whole new task called Composed Person
Retrieval (CPR) is proposed to jointly utilize both image and text information
for target person retrieval. However, the supervised CPR requires very costly
manual annotation dataset, while there are currently no available resources. To
mitigate this issue, we firstly introduce the Zero-shot Composed Person
Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the
CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we
propose a two-stage learning framework, Word4Per, where a lightweight Textual
Inversion Network (TINet) and a text-based person retrieval model based on
fine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned
without utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed
Person Retrieval (ITCPR) dataset is built as the benchmark to assess the
performance of the proposed Word4Per framework. Extensive experiments under
both Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR
task, surpassing the comparative methods by over 10\%. The code and ITCPR
dataset will be publicly available at
https://github.com/Delong-liu-bupt/Word4Per.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Distillation for Road Detection based on cross-model
  Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanli Ma, Oktay Karakus, Paul L. Rosin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of knowledge distillation has played a crucial role in
enabling the transfer of knowledge from larger teacher models to smaller and
more efficient student models, and is particularly beneficial for online and
resource-constrained applications. The effectiveness of the student model
heavily relies on the quality of the distilled knowledge received from the
teacher. Given the accessibility of unlabelled remote sensing data,
semi-supervised learning has become a prevalent strategy for enhancing model
performance. However, relying solely on semi-supervised learning with smaller
models may be insufficient due to their limited capacity for feature
extraction. This limitation restricts their ability to exploit training data.
To address this issue, we propose an integrated approach that combines
knowledge distillation and semi-supervised learning methods. This hybrid
approach leverages the robust capabilities of large models to effectively
utilise large unlabelled data whilst subsequently providing the small student
model with rich and informative features for enhancement. The proposed
semi-supervised learning-based knowledge distillation (SSLKD) approach
demonstrates a notable improvement in the performance of the student model, in
the application of road segmentation, surpassing the effectiveness of
traditional semi-supervised learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiFi-123: Towards High-fidelity One Image to 3D Content Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu Li, Wenbo Hu, Long Quan, Ying Shan, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have enabled 3D generation from a single
image. However, current methods often produce suboptimal results for novel
views, with blurred textures and deviations from the reference image, limiting
their practical applications. In this paper, we introduce HiFi-123, a method
designed for high-fidelity and multi-view consistent 3D generation. Our
contributions are twofold: First, we propose a Reference-Guided Novel View
Enhancement (RGNV) technique that significantly improves the fidelity of
diffusion-based zero-shot novel view synthesis methods. Second, capitalizing on
the RGNV, we present a novel Reference-Guided State Distillation (RGSD) loss.
When incorporated into the optimization-based image-to-3D pipeline, our method
significantly improves 3D generation quality, achieving state-of-the-art
performance. Comprehensive evaluations demonstrate the effectiveness of our
approach over existing methods, both qualitatively and quantitatively. Video
results are available on the project page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://drexubery.github.io/HiFi-123/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVGDreamer: Text Guided SVG Generation with Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16476v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16476v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
shape over-smoothing, color over-saturation, limited diversity in results, and
slow convergence in existing text-to-SVG generation methods. VPSD models SVGs
as distributions of control points and colors to counteract over-smoothing and
over-saturation. Furthermore, VPSD leverages a reward model to reweight vector
particles, which improves aesthetic appeal and accelerates convergence.
Extensive experiments have been conducted to validate the effectiveness of
SVGDreamer, demonstrating its superiority over baseline methods in terms of
editability, visual quality, and diversity. The code and demo of SVGDreamer can
be found at https://ximinng.github.io/SVGDreamer-project/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. project link:
  https://ximinng.github.io/SVGDreamer-project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Bayes image restoration with compressive autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maud Biquard, Marie Chabert, Thomas Oberlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularization of inverse problems is of paramount importance in
computational imaging. The ability of neural networks to learn efficient image
representations has been recently exploited to design powerful data-driven
regularizers. While state-of-the-art plug-and-play methods rely on an implicit
regularization provided by neural denoisers, alternative Bayesian approaches
consider Maximum A Posteriori (MAP) estimation in the latent space of a
generative model, thus with an explicit regularization. However,
state-of-the-art deep generative models require a huge amount of training data
compared to denoisers. Besides, their complexity hampers the optimization
involved in latent MAP derivation. In this work, we first propose to use
compressive autoencoders instead. These networks, which can be seen as
variational autoencoders with a flexible latent prior, are smaller and easier
to train than state-of-the-art generative models. As a second contribution, we
introduce the Variational Bayes Latent Estimation (VBLE) algorithm, which
performs latent estimation within the framework of variational inference.
Thanks to a simple yet efficient parameterization of the variational posterior,
VBLE allows for fast and easy (approximate) posterior sampling. Experimental
results on image datasets BSD and FFHQ demonstrate that VBLE reaches similar
performance than state-of-the-art plug-and-play methods, while being able to
quantify uncertainties faster than other existing posterior sampling
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask Grounding for Referring Image Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Xien Chng, Henry Zheng, Yizeng Han, Xuchong Qiu, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Image Segmentation (RIS) is a challenging task that requires an
algorithm to segment objects referred by free-form language expressions.
Despite significant progress in recent years, most state-of-the-art (SOTA)
methods still suffer from considerable language-image modality gap at the pixel
and word level. These methods generally 1) rely on sentence-level language
features for language-image alignment and 2) lack explicit training supervision
for fine-grained visual grounding. Consequently, they exhibit weak object-level
correspondence between visual and language features. Without well-grounded
features, prior methods struggle to understand complex expressions that require
strong reasoning over relationships among multiple objects, especially when
dealing with rarely used or ambiguous clauses. To tackle this challenge, we
introduce a novel Mask Grounding auxiliary task that significantly improves
visual grounding within language features, by explicitly teaching the model to
learn fine-grained correspondence between masked textual tokens and their
matching visual objects. Mask Grounding can be directly used on prior RIS
methods and consistently bring improvements. Furthermore, to holistically
address the modality gap, we also design a cross-modal alignment loss and an
accompanying alignment module. These additions work synergistically with Mask
Grounding. With all these techniques, our comprehensive approach culminates in
MagNet (Mask-grounded Network), an architecture that significantly outperforms
prior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating
our method's effectiveness in addressing current limitations of RIS algorithms.
Our code and pre-trained weights will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024; Project page:
  https://yxchng.github.io/projects/mask-grounding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Baldrati, Davide Morelli, Marcella Cornia, Marco Bertini, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fashion illustration is a crucial medium for designers to convey their
creative vision and transform design concepts into tangible representations
that showcase the interplay between clothing and the human body. In the context
of fashion design, computer vision techniques have the potential to enhance and
streamline the design process. Departing from prior research primarily focused
on virtual try-on, this paper tackles the task of multimodal-conditioned
fashion image editing. Our approach aims to generate human-centric fashion
images guided by multimodal prompts, including text, human body poses, garment
sketches, and fabric textures. To address this problem, we propose extending
latent diffusion models to incorporate these multiple modalities and modifying
the structure of the denoising network, taking multimodal prompts as input. To
condition the proposed architecture on fabric textures, we employ textual
inversion techniques and let diverse cross-attention layers of the denoising
network attend to textual and texture information, thus incorporating different
granularity conditioning details. Given the lack of datasets for the task, we
extend two existing fashion datasets, Dress Code and VITON-HD, with multimodal
annotations. Experimental evaluations demonstrate the effectiveness of our
proposed approach in terms of realism and coherence concerning the provided
multimodal inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightIt: Illumination Modeling and Control for Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, Yannick Hold-Geoffroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LightIt, a method for explicit illumination control for image
generation. Recent generative methods lack lighting control, which is crucial
to numerous artistic aspects of image generation such as setting the overall
mood or cinematic appearance. To overcome these limitations, we propose to
condition the generation on shading and normal maps. We model the lighting with
single bounce shading, which includes cast shadows. We first train a shading
estimation module to generate a dataset of real-world images and shading pairs.
Then, we train a control network using the estimated shading and normals as
input. Our method demonstrates high-quality image generation and lighting
control in numerous scenes. Additionally, we use our generated dataset to train
an identity-preserving relighting model, conditioned on an image and a target
shading. Our method is the first that enables the generation of images with
controllable, consistent lighting and performs on par with specialized
relighting state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://peter-kocsis.github.io/LightIt/ Video:
  https://youtu.be/cCfSBD5aPLI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully automated workflow for the design of patient-specific orthopaedic
  implants: application to total knee arthroplasty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziliz Guezou-Philippe, Arnaud Clavé, Ehouarn Maguet, Ludivine Maintier, Charles Garraud, Jean-Rassaire Fouefack, Valérie Burdin, Eric Stindel, Guillaume Dardenne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arthroplasty is commonly performed to treat joint osteoarthritis, reducing
pain and improving mobility. While arthroplasty has known several technical
improvements, a significant share of patients are still unsatisfied with their
surgery. Personalised arthroplasty improves surgical outcomes however current
solutions require delays, making it difficult to integrate in clinical routine.
We propose a fully automated workflow to design patient-specific implants,
presented for total knee arthroplasty, the most widely performed arthroplasty
in the world nowadays.
  The proposed pipeline first uses artificial neural networks to segment the
proximal and distal extremities of the femur and tibia. Then the full bones are
reconstructed using augmented statistical shape models, combining shape and
landmarks information. Finally, 77 morphological parameters are computed to
design patient-specific implants. The developed workflow has been trained using
91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in
terms of accuracy and execution time.
  The workflow accuracy was $0.4\pm0.2mm$ for the segmentation, $1.2\pm0.4mm$
for the full bones reconstruction, and $2.8\pm2.2mm$ for the anatomical
landmarks determination. The custom implants fitted the patients' anatomy with
$0.6\pm0.2mm$ accuracy. The whole process from segmentation to implants' design
lasted about 5 minutes.
  The proposed workflow allows for a fast and reliable personalisation of knee
implants, directly from the patient CT image without requiring any manual
intervention. It establishes a patient-specific pre-operative planning for TKA
in a very short time making it easily available for all patients. Combined with
efficient implant manufacturing techniques, this solution could help answer the
growing number of arthroplasties while reducing complications and improving the
patients' satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ denoiSplit: a method for joint image splitting and unsupervised
  denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashesh Ashesh, Florian Jug
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we present denoiSplit, a method to tackle a new analysis task,
i.e. the challenge of joint semantic image splitting and unsupervised
denoising. This dual approach has important applications in fluorescence
microscopy, where semantic image splitting has important applications but noise
does generally hinder the downstream analysis of image content. Image splitting
involves dissecting an image into its distinguishable semantic structures. We
show that the current state-of-the-art method for this task struggles in the
presence of image noise, inadvertently also distributing the noise across the
predicted outputs. The method we present here can deal with image noise by
integrating an unsupervised denoising sub-task. This integration results in
improved semantic image unmixing, even in the presence of notable and realistic
levels of imaging noise. A key innovation in denoiSplit is the use of
specifically formulated noise models and the suitable adjustment of
KL-divergence loss for the high-dimensional hierarchical latent space we are
training. We showcase the performance of denoiSplit across 4 tasks on
real-world microscopy images. Additionally, we perform qualitative and
quantitative evaluations and compare results to existing benchmarks,
demonstrating the effectiveness of using denoiSplit: a single Variational
Splitting Encoder-Decoder (VSE) Network using two suitable noise models to
jointly perform semantic splitting and denoising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling Instance Associations: A Closer Look for Audio-Visual
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02970v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02970v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhong Chen, Yuyuan Liu, Hu Wang, Fengbei Liu, Chong Wang, Helen Frazer, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual segmentation (AVS) is a challenging task that involves
accurately segmenting sounding objects based on audio-visual cues. The
effectiveness of audio-visual learning critically depends on achieving accurate
cross-modal alignment between sound and visual objects. Successful audio-visual
learning requires two essential components: 1) a challenging dataset with
high-quality pixel-level multi-class annotated images associated with audio
files, and 2) a model that can establish strong links between audio information
and its corresponding visual object. However, these requirements are only
partially addressed by current methods, with training sets containing biased
audio-visual data, and models that generalise poorly beyond this biased
training set. In this work, we propose a new cost-effective strategy to build
challenging and relatively unbiased high-quality audio-visual segmentation
benchmarks. We also propose a new informative sample mining method for
audio-visual supervised contrastive learning to leverage discriminative
contrastive samples to enforce cross-modal understanding. We show empirical
results that demonstrate the effectiveness of our benchmark. Furthermore,
experiments conducted on existing AVS datasets and on our new benchmark show
that our method achieves state-of-the-art (SOTA) segmentation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/cyh-0/CAVP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in
  Human-Centric Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Saif Ullah Khan, Muhammad Ferjad Naeem, Federico Tombari, Luc Van Gool, Didier Stricker, Muhammad Zeshan Afzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose FocusCLIP, integrating subject-level guidance--a specialized
mechanism for target-specific supervision--into the CLIP framework for improved
zero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP
on both the vision and text sides. On the vision side, we incorporate ROI
heatmaps emulating human visual attention mechanisms to emphasize
subject-relevant image regions. On the text side, we introduce human pose
descriptions to provide rich contextual information. For human-centric tasks,
FocusCLIP is trained with images from the MPII Human Pose dataset. The proposed
approach surpassed CLIP by an average of 8.61% across five previously unseen
datasets covering three human-centric tasks. FocusCLIP achieved an average
accuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement
in activity recognition, a 14.78% improvement in age classification, and a
7.06% improvement in emotion recognition. Moreover, using our proposed
single-shot LLM prompting strategy, we release a high-quality MPII Pose
Descriptions dataset to encourage further research in multimodal learning for
human-centric tasks. Furthermore, we also demonstrate the effectiveness of our
subject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%
improvement over CLIP in zero-shot bird classification using the CUB dataset.
Our findings emphasize the potential of integrating subject-level guidance with
general pretraining methods for enhanced downstream performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Power of <span class="highlight-title">Self-Supervised</span> Image Denoising: A Comprehensive
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00247v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00247v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Zhang, Fangfang Zhou, Felix Albu, Yuanzhou Wei, Xiao Yang, Yuan Gu, Qiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of deep learning has brought a revolutionary transformation to
image denoising techniques. However, the persistent challenge of acquiring
noise-clean pairs for supervised methods in real-world scenarios remains
formidable, necessitating the exploration of more practical self-supervised
image denoising. This paper focuses on self-supervised image denoising methods
that offer effective solutions to address this challenge. Our comprehensive
review thoroughly analyzes the latest advancements in self-supervised image
denoising approaches, categorizing them into three distinct classes: General
methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods.
For each class, we provide a concise theoretical analysis along with their
practical applications. To assess the effectiveness of these methods, we
present both quantitative and qualitative experimental results on various
datasets, utilizing classical algorithms as benchmarks. Additionally, we
critically discuss the current limitations of these methods and propose
promising directions for future research. By offering a detailed overview of
recent developments in self-supervised image denoising, this review serves as
an invaluable resource for researchers and practitioners in the field,
facilitating a deeper understanding of this emerging domain and inspiring
further advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands
  from a Single Image <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08262v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08262v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minje Kim, Tae-Kyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating personalized hand avatars is important to offer a realistic
experience to users on AR / VR platforms. While most prior studies focused on
reconstructing 3D hand shapes, some recent work has tackled the reconstruction
of hand textures on top of shapes. However, these methods are often limited to
capturing pixels on the visible side of a hand, requiring diverse views of the
hand in a video or multiple images as input. In this paper, we propose a novel
method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the
first end-to-end trainable method for relightable, pose-free texture
reconstruction of two interacting hands taking only a single RGB image, by
three novel components: 1) bi-directional (left $\leftrightarrow$ right)
texture reconstruction using the texture symmetry of left / right hands, 2)
utilizing a texture parametric model for hand texture recovery, and 3) the
overall coarse-to-fine stage pipeline for reconstructing personalized texture
of two interacting hands. BiTT first estimates the scene light condition and
albedo image from an input image, then reconstructs the texture of both hands
through the texture parametric model and bi-directional texture reconstructor.
In experiments using InterHand2.6M and RGB2Hands datasets, our method
significantly outperforms state-of-the-art hand texture reconstruction methods
quantitatively and qualitatively. The code is available at
https://github.com/yunminjin2/BiTT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Project Page:
  https://yunminjin2.github.io/projects/bitt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toulouse Hyperspectral Data Set: a benchmark data set to assess
  semi-supervised spectral representation learning and pixel-wise
  classification techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08863v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08863v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Thoreau, Laurent Risser, Véronique Achard, Béatrice Berthelot, Xavier Briottet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Airborne hyperspectral images can be used to map the land cover in large
urban areas, thanks to their very high spatial and spectral resolutions on a
wide spectral domain. While the spectral dimension of hyperspectral images is
highly informative of the chemical composition of the land surface, the use of
state-of-the-art machine learning algorithms to map the land cover has been
dramatically limited by the availability of training data. To cope with the
scarcity of annotations, semi-supervised and self-supervised techniques have
lately raised a lot of interest in the community. Yet, the publicly available
hyperspectral data sets commonly used to benchmark machine learning models are
not totally suited to evaluate their generalization performances due to one or
several of the following properties: a limited geographical coverage (which
does not reflect the spectral diversity in metropolitan areas), a small number
of land cover classes and a lack of appropriate standard train / test splits
for semi-supervised and self-supervised learning. Therefore, we release in this
paper the Toulouse Hyperspectral Data Set that stands out from other data sets
in the above-mentioned respects in order to meet key issues in spectral
representation learning and classification over large-scale hyperspectral
images with very few labeled pixels. Besides, we discuss and experiment
self-supervised techniques for spectral representation learning, including the
Masked Autoencoder, and establish a baseline for pixel-wise classification
achieving 85% overall accuracy and 77% F1 score. The Toulouse Hyperspectral
Data Set and our code are publicly available at
https://www.toulouse-hyperspectral-data-set.com and
https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Prior Based Deep Human Point Cloud Geometry Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinju Wu, Pingping Zhang, Meng Wang, Peilin Chen, Shiqi Wang, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of digital avatars has raised an exponential increase in the
demand for human point clouds with realistic and intricate details. The
compression of such data becomes challenging with overwhelming data amounts
comprising millions of points. Herein, we leverage the human geometric prior in
geometry redundancy removal of point clouds, greatly promoting the compression
performance. More specifically, the prior provides topological constraints as
geometry initialization, allowing adaptive adjustments with a compact parameter
set that could be represented with only a few bits. Therefore, we can envisage
high-resolution human point clouds as a combination of geometric priors and
structural deviations. The priors could first be derived with an aligned point
cloud, and subsequently the difference of features is compressed into a compact
latent code. The proposed framework can operate in a play-and-plug fashion with
existing learning based point cloud compression methods. Extensive experimental
results show that our approach significantly improves the compression
performance without deteriorating the quality, demonstrating its promise in a
variety of applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TCSVT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining CLIP's performance disparities on data from blind/low vision
  users <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniela Massiceti, Camilla Longden, Agnieszka Słowik, Samuel Wills, Martin Grayson, Cecily Morrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multi-modal models (LMMs) hold the potential to usher in a new era of
automated visual assistance for people who are blind or low vision (BLV). Yet,
these models have not been systematically evaluated on data captured by BLV
users. We address this by empirically assessing CLIP, a widely-used LMM likely
to underpin many assistive technologies. Testing 25 CLIP variants in a
zero-shot classification task, we find that their accuracy is 15 percentage
points lower on average for images captured by BLV users than web-crawled
images. This disparity stems from CLIP's sensitivities to 1) image content
(e.g. not recognizing disability objects as well as other objects); 2) image
quality (e.g. not being robust to lighting variation); and 3) text content
(e.g. not recognizing objects described by tactile adjectives as well as visual
ones). We delve deeper with a textual analysis of three common pre-training
datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content
is rarely mentioned. We then provide three examples that illustrate how the
performance disparities extend to three downstream models underpinned by CLIP:
OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5
images can mitigate CLIP's quality-of-service disparities for BLV users in some
scenarios, which we discuss alongside a set of other possible mitigations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally Generative Augmentation for Fair Facial Attribute
  Classification <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu, Jun Xiao, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Attribute Classification (FAC) holds substantial promise in widespread
applications. However, FAC models trained by traditional methodologies can be
unfair by exhibiting accuracy inconsistencies across varied data
subpopulations. This unfairness is largely attributed to bias in data, where
some spurious attributes (e.g., Male) statistically correlate with the target
attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the
labels of spurious attributes, which may be unavailable in practice. This work
proposes a novel, generation-based two-stage framework to train a fair FAC
model on biased data without additional annotation. Initially, we identify the
potential spurious attributes based on generative models. Notably, it enhances
interpretability by explicitly showing the spurious attributes in image space.
Following this, for each image, we first edit the spurious attributes with a
random degree sampled from a uniform distribution, while keeping target
attribute unchanged. Then we train a fair FAC model by fostering model
invariance to these augmentation. Extensive experiments on three common
datasets demonstrate the effectiveness of our method in promoting fairness in
FAC without compromising accuracy. Codes are in
https://github.com/heqianpei/DiGA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive <span class="highlight-title">Pre-Train</span>ing with Multi-View Fusion for No-Reference Point
  Cloud Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Jenq-Neng Hwang, Xiaozhong Xu, Shan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-reference point cloud quality assessment (NR-PCQA) aims to automatically
evaluate the perceptual quality of distorted point clouds without available
reference, which have achieved tremendous improvements due to the utilization
of deep neural networks. However, learning-based NR-PCQA methods suffer from
the scarcity of labeled data and usually perform suboptimally in terms of
generalization. To solve the problem, we propose a novel contrastive
pre-training framework tailored for PCQA (CoPA), which enables the pre-trained
model to learn quality-aware representations from unlabeled data. To obtain
anchors in the representation space, we project point clouds with different
distortions into images and randomly mix their local patches to form mixed
images with multiple distortions. Utilizing the generated anchors, we constrain
the pre-training process via a quality-aware contrastive loss following the
philosophy that perceptual quality is closely related to both content and
distortion. Furthermore, in the model fine-tuning stage, we propose a
semantic-guided multi-view fusion module to effectively integrate the features
of projected images from multiple perspectives. Extensive experiments show that
our method outperforms the state-of-the-art PCQA methods on popular benchmarks.
Further investigations demonstrate that CoPA can also benefit existing
learning-based PCQA models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Point-based Inverse Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoon-Gyu Chung, Seokjun Choi, Seung-Hwan Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present differentiable point-based inverse rendering, DPIR, an
analysis-by-synthesis method that processes images captured under diverse
illuminations to estimate shape and spatially-varying BRDF. To this end, we
adopt point-based rendering, eliminating the need for multiple samplings per
ray, typical of volumetric rendering, thus significantly enhancing the speed of
inverse rendering. To realize this idea, we devise a hybrid point-volumetric
representation for geometry and a regularized basis-BRDF representation for
reflectance. The hybrid geometric representation enables fast rendering through
point-based splatting while retaining the geometric details and stability
inherent to SDF-based representations. The regularized basis-BRDF mitigates the
ill-posedness of inverse rendering stemming from limited light-view angular
samples. We also propose an efficient shadow detection method using point-based
shadow map rendering. Our extensive evaluations demonstrate that DPIR
outperforms prior works in terms of reconstruction accuracy, computational
efficiency, and memory footprint. Furthermore, our explicit point-based
representation and rendering enables intuitive geometry and reflectance
editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HallusionBench: An Advanced Diagnostic Suite for Entangled Language
  Hallucination and Visual Illusion in Large Vision-Language Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14566v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14566v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HallusionBench, a comprehensive benchmark designed for the
evaluation of image-context reasoning. This benchmark presents significant
challenges to advanced large visual-language models (LVLMs), such as
GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing
nuanced understanding and interpretation of visual data. The benchmark
comprises 346 images paired with 1129 questions, all meticulously crafted by
human experts. We introduce a novel structure for these visual questions
designed to establish control groups. This structure enables us to conduct a
quantitative analysis of the models' response tendencies, logical consistency,
and various failure modes. In our evaluation on HallusionBench, we benchmarked
15 different models, highlighting a 31.42% question-pair accuracy achieved by
the state-of-the-art GPT-4V. Notably, all other evaluated models achieve
accuracy below 16%. Moreover, our analysis not only highlights the observed
failure modes, including language hallucination and visual illusion, but also
deepens an understanding of these pitfalls. Our comprehensive case studies
within HallusionBench shed light on the challenges of hallucination and
illusion in LVLMs. Based on these insights, we suggest potential pathways for
their future improvement. The benchmark and codebase can be accessed at
https://github.com/tianyi-lab/HallusionBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of
  Altered Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuong Dam, Jihoon Jeong, Anh Tran, Daeyoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study discusses the critical issues of Virtual Try-On in contemporary
e-commerce and the prospective metaverse, emphasizing the challenges of
preserving intricate texture details and distinctive features of the target
person and the clothes in various scenarios, such as clothing texture and
identity characteristics like tattoos or accessories. In addition to the
fidelity of the synthesized images, the efficiency of the synthesis process
presents a significant hurdle. Various existing approaches are explored,
highlighting the limitations and unresolved aspects, e.g., identity information
omission, uncontrollable artifacts, and low synthesis speed. It then proposes a
novel diffusion-based solution that addresses garment texture preservation and
user identity retention during virtual try-on. The proposed network comprises
two primary modules - a warping module aligning clothing with individual
features and a try-on module refining the attire and generating missing parts
integrated with a mask-aware post-processing technique ensuring the integrity
of the individual's identity. It demonstrates impressive results, surpassing
the state-of-the-art in speed by nearly 20 times during inference, with
superior fidelity in qualitative assessments. Quantitative evaluations confirm
comparable performance with the recent SOTA method on the VITON-HD and
Dresscode datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06199v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06199v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have showcased impressive skills in
tasks related to visual understanding and reasoning. Yet, their widespread
application faces obstacles due to the high computational demands during both
the training and inference phases, restricting their use to a limited audience
within the research and user communities. In this paper, we investigate the
design aspects of Multimodal Small Language Models (MSLMs) and propose an
efficient multimodal assistant named Mipha, which is designed to create synergy
among various aspects: visual representation, language models, and optimization
strategies. We show that without increasing the volume of training data, our
Mipha-3B outperforms the state-of-the-art large MLLMs, especially
LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide
insights and guidelines for developing strong MSLMs that rival the capabilities
of MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dispersed Structured Light for Hyperspectral 3D Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhyun Shin, Seokjun Choi, Felix Heide, Seung-Hwan Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral 3D imaging aims to acquire both depth and spectral information
of a scene. However, existing methods are either prohibitively expensive and
bulky or compromise on spectral and depth accuracy. In this work, we present
Dispersed Structured Light (DSL), a cost-effective and compact method for
accurate hyperspectral 3D imaging. DSL modifies a traditional projector-camera
system by placing a sub-millimeter thick diffraction grating film front of the
projector. The grating disperses structured light based on light wavelength. To
utilize the dispersed structured light, we devise a model for dispersive
projection image formation and a per-pixel hyperspectral 3D reconstruction
method. We validate DSL by instantiating a compact experimental prototype. DSL
achieves spectral accuracy of 18.8nm full-width half-maximum (FWHM) and depth
error of 1mm. We demonstrate that DSL outperforms prior work on practical
hyperspectral 3D imaging. DSL promises accurate and practical hyperspectral 3D
imaging for diverse application domains, including computer vision and
graphics, cultural heritage, geology, and biology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIA: Your Personalized Image Animator via Plug-and-Play Modules in
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13964v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13964v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in personalized text-to-image (T2I) models have
revolutionized content creation, empowering non-experts to generate stunning
images with unique styles. While promising, adding realistic motions into these
personalized images by text poses significant challenges in preserving distinct
styles, high-fidelity details, and achieving motion controllability by text. In
this paper, we present PIA, a Personalized Image Animator that excels in
aligning with condition images, achieving motion controllability by text, and
the compatibility with various personalized T2I models without specific tuning.
To achieve these goals, PIA builds upon a base T2I model with well-trained
temporal alignment layers, allowing for the seamless transformation of any
personalized T2I model into an image animation model. A key component of PIA is
the introduction of the condition module, which utilizes the condition frame
and inter-frame affinity as input to transfer appearance information guided by
the affinity hint for individual frame synthesis in the latent space. This
design mitigates the challenges of appearance-related image alignment within
and allows for a stronger focus on aligning with motion-related guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://pi-animator.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-PHYRE: Interactive Physical Reasoning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqian Li, Kewen Wu, Chi Zhang, Yixin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents' abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents' learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving the bongard-logo problem by modeling a probabilistic model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhuo Song, Beiming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract reasoning problems challenge the perceptual and cognitive abilities
of AI algorithms, demanding deeper pattern discernment and inductive reasoning
beyond explicit image features. This study introduces PMoC, a tailored
probability model for the Bongard-Logo problem, achieving high reasoning
accuracy by constructing independent probability models. Additionally, we
present Pose-Transformer, an enhanced Transformer-Encoder designed for complex
abstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.
Pose-Transformer incorporates positional information learning, inspired by
capsule networks' pose matrices, enhancing its focus on local positional
relationships in image data processing. When integrated with PMoC, it further
improves reasoning accuracy. Our approach effectively addresses reasoning
difficulties associated with abstract entities' positional changes,
outperforming previous models on the OIG, D3$\times$3 subsets of RAVEN, and PGM
databases. This research contributes to advancing AI's capabilities in abstract
reasoning and cognitive pattern recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract
  Reasoning process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03190v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03190v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhuo Song, Beiming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract reasoning problems pose significant challenges to artificial
intelligence algorithms, demanding cognitive capabilities beyond those required
for perception tasks. This study introduces the Triple-CFN approach to tackle
the Bongard-Logo problem, achieving notable reasoning accuracy by implicitly
reorganizing the concept space of conflicting instances. Additionally, the
Triple-CFN paradigm proves effective for the RPM problem with necessary
modifications, yielding competitive results. To further enhance performance on
the RPM issue, we develop the Meta Triple-CFN network, which explicitly
structures the problem space while maintaining interpretability on progressive
patterns. The success of Meta Triple-CFN is attributed to its paradigm of
modeling the conceptual space, equivalent to normalizing reasoning information.
Based on this ideology, we introduce the Re-space layer, enhancing the
performance of both Meta Triple-CFN and Triple-CFN. This paper aims to
contribute to advancements in machine intelligence by exploring innovative
network designs for addressing abstract reasoning problems, paving the way for
further breakthroughs in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D4C glove-train: solving the RPM and Bongard-logo problem by
  distributing and Circumscribing concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03452v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03452v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhuo Song, Beiming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper achieves noteworthy progress in the realm of abstract reasoning,
particularly in addressing Raven's Progressive Matrices (RPM) and Bongard-Logo
challenges. Initially, we introduce Lico-Net, a novel baseline model that
resolves RPM problems with remarkable accuracy. Leveraging this foundation, we
advance with the D3C approach, which advocates representing the underlying
concepts in abstract reasoning problems through distributions. This perspective
enhances the performance of both Lico-Net and a baseline model excelling in
Bongard-Logo tasks. To bolster the computational efficiency of D3C, we present
the D3C-cos variant, offering a streamlined yet precise solution. Furthermore,
we propose the D2C method, redefining conceptual boundaries within these
domains and bridging the divide between high-level abstractions and their
lower-dimensional counterparts. Finally, we extend our methodology to D4C,
employing adversarial techniques to refine conceptual boundaries further and
demonstrate substantial improvements in both RPM and Bongard-Logo challenges.
Overall, our contributions present a fresh outlook and practical advancements
in the field of abstract reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 19 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CiPR: An Efficient Framework with Cross-instance Positive Relations for
  Generalized Category Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaozhe Hao, Kai Han, Kwan-Yee K. Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the issue of generalized category discovery (GCD). GCD considers
the open-world problem of automatically clustering a partially labelled
dataset, in which the unlabelled data may contain instances from both novel
categories and labelled classes. In this paper, we address the GCD problem with
an unknown category number for the unlabelled data. We propose a framework,
named CiPR, to bootstrap the representation by exploiting Cross-instance
Positive Relations in the partially labelled data for contrastive learning,
which have been neglected in existing methods. To obtain reliable
cross-instance relations to facilitate representation learning, we introduce a
semi-supervised hierarchical clustering algorithm, named selective neighbor
clustering (SNC), which can produce a clustering hierarchy directly from the
connected components of a graph constructed from selective neighbors. We
further present a method to estimate the unknown class number using SNC with a
joint reference score that considers clustering indexes of both labelled and
unlabelled data, and extend SNC to allow label assignment for the unlabelled
instances with a given class number. We thoroughly evaluate our framework on
public generic image recognition datasets and challenging fine-grained
datasets, and establish a new state-of-the-art. Code:
https://github.com/haoosz/CiPR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR. Code: https://github.com/haoosz/CiPR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction
  Data <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) tuned on machine-generated
instruction-following data have demonstrated remarkable performance in various
multi-modal understanding and generation tasks. However, the hallucinations
inherent in machine-generated data, which could lead to hallucinatory outputs
in MLLMs, remain under-explored. This work aims to investigate various
hallucinations (i.e., object, relation, attribute hallucinations) and mitigate
those hallucinatory toxicities in large-scale machine-generated visual
instruction datasets. Drawing on the human ability to identify factual errors,
we present a novel hallucination detection and elimination framework,
HalluciDoctor, based on the cross-checking paradigm. We use our framework to
identify and eliminate hallucinations in the training data automatically.
Interestingly, HalluciDoctor also indicates that spurious correlations arising
from long-tail object co-occurrences contribute to hallucinations. Based on
that, we execute counterfactual visual instruction expansion to balance data
distribution, thereby enhancing MLLMs' resistance to hallucinations.
Comprehensive experiments on hallucination evaluation benchmarks show that our
method successfully mitigates 44.6% hallucinations relatively and maintains
competitive performance compared to LLaVA. The data and code for this paper are
publicly available. \url{https://github.com/Yuqifan1117/HalluciDoctor}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ W-HMR: Human Mesh Recovery in World Space with Weak-supervised Camera
  Calibration and Orientation Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yao, Hongwen Zhang, Yunlian Sun, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a long time, in reconstructing 3D human bodies from monocular images,
most methods opted to simplify the task by minimizing the influence of the
camera. Using a coarse focal length setting results in the reconstructed bodies
not aligning well with distorted images. Ignoring camera rotation leads to an
unrealistic reconstructed body pose in world space. Consequently, the
application scenarios of existing methods are confined to controlled
environments. When confronted with complex and diverse in-the-wild images, they
struggle to achieve accurate and reasonable reconstruction in world space. To
address the above issues, we propose W-HMR, which decouples global body
recovery into camera calibration, local body recovery, and global body
orientation correction. We design the first weak-supervised camera calibration
method for body distortion, eliminating dependence on focal length labels and
achieving finer mesh-image alignment. We propose a novel orientation correction
module to allow the reconstructed human body to remain normal in world space.
Decoupling body orientation and body pose enables our model to consider the
accuracy in camera coordinate and the reasonableness in world coordinate
simultaneously, expanding the range of applications. As a result, W-HMR
achieves high-quality reconstruction in dual coordinate systems, particularly
in challenging scenes. Codes and demos have been released on the project page
https://yw0208.github.io/w-hmr/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yw0208.github.io/w-hmr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Semantic Segmentation Meets Frequency Aliasing <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09065v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09065v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Chen, Lin Gu, Ying Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in semantic segmentation, where and what pixels
are hard to segment remains largely unexplored. Existing research only
separates an image into easy and hard regions and empirically observes the
latter are associated with object boundaries. In this paper, we conduct a
comprehensive analysis of hard pixel errors, categorizing them into three
types: false responses, merging mistakes, and displacements. Our findings
reveal a quantitative association between hard pixels and aliasing, which is
distortion caused by the overlapping of frequency components in the Fourier
domain during downsampling. To identify the frequencies responsible for
aliasing, we propose using the equivalent sampling rate to calculate the
Nyquist frequency, which marks the threshold for aliasing. Then, we introduce
the aliasing score as a metric to quantify the extent of aliasing. While
positively correlated with the proposed aliasing score, three types of hard
pixels exhibit different patterns. Here, we propose two novel de-aliasing
filter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing
degradation by accurately removing or adjusting frequencies higher than the
Nyquist frequency. The DAF precisely removes the frequencies responsible for
aliasing before downsampling, while the FreqMix dynamically selects
high-frequency components within the encoder block. Experimental results
demonstrate consistent improvements in semantic segmentation and low-light
instance segmentation tasks. The code is available at:
https://github.com/Linwei-Chen/Seg-Aliasing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cell Variational Information Bottleneck Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonghua Zhai, Chen Ju, Jinsong Lan, Shuai Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose Cell Variational Information Bottleneck Network
(cellVIB), a convolutional neural network using information bottleneck
mechanism, which can be combined with the latest feedforward network
architecture in an end-to-end training method. Our Cell Variational Information
Bottleneck Network is constructed by stacking VIB cells, which generate feature
maps with uncertainty. As layers going deeper, the regularization effect will
gradually increase, instead of directly adding excessive regular constraints to
the output layer of the model as in Deep VIB. Under each VIB cell, the
feedforward process learns an independent mean term and an standard deviation
term, and predicts the Gaussian distribution based on them. The feedback
process is based on reparameterization trick for effective training. This work
performs an extensive analysis on MNIST dataset to verify the effectiveness of
each VIB cells, and provides an insightful analysis on how the VIB cells affect
mutual information. Experiments conducted on CIFAR-10 also prove that our
cellVIB is robust against noisy labels during training and against corrupted
images during testing. Then, we validate our method on PACS dataset, whose
results show that the VIB cells can significantly improve the generalization
performance of the basic model. Finally, in a more complex representation
learning task, face recognition, our network structure has also achieved very
competitive results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Judge by the Look: Towards Motion Coherent Video Representation <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current training pipelines in object recognition neglect Hue Jittering when
doing data augmentation as it not only brings appearance changes that are
detrimental to classification, but also the implementation is inefficient in
practice. In this study, we investigate the effect of hue variance in the
context of video understanding and find this variance to be beneficial since
static appearances are less important in videos that contain motion
information. Based on this observation, we propose a data augmentation method
for video understanding, named Motion Coherent Augmentation (MCA), that
introduces appearance variation in videos and implicitly encourages the model
to prioritize motion patterns, rather than static appearances. Concretely, we
propose an operation SwapMix to efficiently modify the appearance of video
samples, and introduce Variation Alignment (VA) to resolve the distribution
shift caused by SwapMix, enforcing the model to learn appearance invariant
representations. Comprehensive empirical evaluation across various
architectures and different datasets solidly validates the effectiveness and
generalization ability of MCA, and the application of VA in other augmentation
methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Text-to-Image (TTI) models have become a common approach for
generating training data in various generative fields. However, visual
hallucinations, which contain perceptually critical defects, remain a concern,
especially in non-photorealistic styles like cartoon characters. We propose a
novel visual hallucination detection system for cartoon character images
generated by TTI models. Our approach leverages pose-aware in-context visual
learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB
images and pose information. By incorporating pose guidance from a fine-tuned
pose estimator, we enable VLMs to make more accurate decisions. Experimental
results demonstrate significant improvements in identifying visual
hallucinations compared to baseline methods relying solely on RGB images. This
research advances TTI models by mitigating visual hallucinations, expanding
their potential in non-photorealistic domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures, 1 table, Project page:
  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMA-Diffusion: MultiModal Attack on Diffusion Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Tsung-Yi Ho, Nan Xu, Qiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Text-to-Image (T2I) models have seen remarkable
advancements, gaining widespread adoption. However, this progress has
inadvertently opened avenues for potential misuse, particularly in generating
inappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces
MMA-Diffusion, a framework that presents a significant and realistic threat to
the security of T2I models by effectively circumventing current defensive
measures in both open-source models and commercial online services. Unlike
previous approaches, MMA-Diffusion leverages both textual and visual modalities
to bypass safeguards like prompt filters and post-hoc safety checkers, thus
exposing and highlighting the vulnerabilities in existing defense mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code is available at
  https://github.com/yangyijune/MMA-Diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noisy-Correspondence Learning for Text-to-Image Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng, Joey Tianyi Zhou, Peng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image person re-identification (TIReID) is a compelling topic in the
cross-modal community, which aims to retrieve the target person based on a
textual query. Although numerous TIReID methods have been proposed and achieved
promising performance, they implicitly assume the training image-text pairs are
correctly aligned, which is not always the case in real-world scenarios. In
practice, the image-text pairs inevitably exist under-correlated or even
false-correlated, a.k.a noisy correspondence (NC), due to the low quality of
the images and annotation errors. To address this problem, we propose a novel
Robust Dual Embedding method (RDE) that can learn robust visual-semantic
associations even with NC. Specifically, RDE consists of two main components:
1) A Confident Consensus Division (CCD) module that leverages the dual-grained
decisions of dual embedding modules to obtain a consensus set of clean training
data, which enables the model to learn correct and reliable visual-semantic
associations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional
Triplet Ranking loss with the hardest negative samples to a log-exponential
upper bound over all negative ones, thus preventing the model collapse under NC
and can also focus on hard-negative samples for promising performance. We
conduct extensive experiments on three public benchmarks, namely CUHK-PEDES,
ICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our
RDE. Our method achieves state-of-the-art results both with and without
synthetic noisy correspondences on all three datasets. Code is available at
https://github.com/QinYang79/RDE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRS-Diff: Controllable Generative Remote Sensing Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Datao Tang, Xiangyong Cao, Xingsong Hou, Zhongyuan Jiang, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of diffusion models has revolutionized the field of image
generation, providing new methods for creating high-quality, high-resolution
images across various applications. However, the potential of these models for
generating domain-specific images, particularly remote sensing (RS) images,
remains largely untapped. RS images that are notable for their high resolution,
extensive coverage, and rich information content, bring new challenges that
general diffusion models may not adequately address. This paper proposes
CRS-Diff, a pioneering diffusion modeling framework specifically tailored for
generating remote sensing imagery, leveraging the inherent advantages of
diffusion models while integrating advanced control mechanisms to ensure that
the imagery is not only visually clear but also enriched with geographic and
temporal information. The model integrates global and local control inputs,
enabling precise combinations of generation conditions to refine the generation
process. A comprehensive evaluation of CRS-Diff has demonstrated its superior
capability to generate RS imagery both in a single condition and multiple
conditions compared with previous methods in terms of image quality and
diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Telling Left from Right: Identifying Geometry-Aware Semantic
  Correspondence <span class="chip">CVPR 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Zhang, Charles Herrmann, Junhwa Hur, Eric Chen, Varun Jampani, Deqing Sun, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pre-trained large-scale vision models have shown significant promise
for semantic correspondence, their features often struggle to grasp the
geometry and orientation of instances. This paper identifies the importance of
being geometry-aware for semantic correspondence and reveals a limitation of
the features of current foundation models under simple post-processing. We show
that incorporating this information can markedly enhance semantic
correspondence performance with simple but effective solutions in both
zero-shot and supervised settings. We also construct a new challenging
benchmark for semantic correspondence built from an existing animal pose
estimation dataset, for both pre-training validating models. Our method
achieves a PCK@0.10 score of 65.4 (zero-shot) and 85.6 (supervised) on the
challenging SPair-71k dataset, outperforming the state of the art by 5.5p and
11.0p absolute gains, respectively. Our code and datasets are publicly
available at: https://telling-left-from-right.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 24, project page:
  https://telling-left-from-right.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VURF: A General-purpose Reasoning and Self-refinement Framework for
  Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14743v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14743v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated the effectiveness of Large Language Models
(LLMs) as reasoning modules that can deconstruct complex tasks into more
manageable sub-tasks, particularly when applied to visual reasoning tasks for
images. In contrast, this paper introduces a Video Understanding and Reasoning
Framework (VURF) based on the reasoning power of LLMs. Ours is a novel approach
to extend the utility of LLMs in the context of video tasks, leveraging their
capacity to generalize from minimal input and output demonstrations within a
contextual framework. By presenting LLMs with pairs of instructions and their
corresponding high-level programs, we harness their contextual learning
capabilities to generate executable visual programs for video understanding. To
enhance program's accuracy and robustness, we implement two important
strategies. Firstly, we employ a feedback-generation approach, powered by
GPT-3.5, to rectify errors in programs utilizing unsupported functions.
Secondly, taking motivation from recent works on self refinement of LLM
outputs, we introduce an iterative procedure for improving the quality of the
in-context examples by aligning the initial outputs to the outputs that would
have been generated had the LLM not been bound by the structure of the
in-context examples. Our results on several video-specific tasks, including
visual QA, video anticipation, pose estimation and multi-video QA illustrate
the efficacy of these enhancements in improving the performance of visual
programming approaches for video tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural
  Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Xu, Ziao Liu, Mengqi Guo, Jiancheng Li, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel rolling shutter bundle adjustment method for neural
radiance fields (NeRF), which utilizes the unordered rolling shutter (RS)
images to obtain the implicit 3D representation. Existing NeRF methods suffer
from low-quality images and inaccurate initial camera poses due to the RS
effect in the image, whereas, the previous method that incorporates the RS into
NeRF requires strict sequential data input, limiting its widespread
applicability. In constant, our method recovers the physical formation of RS
images by estimating camera poses and velocities, thereby removing the input
constraints on sequential data. Moreover, we adopt a coarse-to-fine training
strategy, in which the RS epipolar constraints of the pairwise frames in the
scene graph are used to detect the camera poses that fall into local minima.
The poses detected as outliers are corrected by the interpolation method with
neighboring poses. The experimental results validate the effectiveness of our
method over state-of-the-art works and demonstrate that the reconstruction of
3D representations is not constrained by the requirement of video sequence
input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David D. Nguyen, David Leibowitz, Surya Nepal, Salil S. Kanhere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models with discrete latent representations have recently
demonstrated an impressive ability to learn complex high-dimensional data
distributions. However, their performance relies on a long sequence of tokens
per instance and a large number of codebook entries, resulting in long sampling
times and considerable computation to fit the categorical posterior. To address
these issues, we propose the Masked Vector Quantization (MVQ) framework which
increases the representational capacity of each code vector by learning mask
configurations via a stochastic winner-takes-all training regime called
Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\times$64, MVQ reduces
FID in existing vector quantization architectures by up to $68\%$ at 2 tokens
per instance and $57\%$ at 5 tokens. These improvements widen as codebook
entries is reduced and allows for $7\textit{--}45\times$ speed-up in token
sampling during inference. As an additional benefit, we find that smaller
latent spaces lead to MVQ identifying transferable visual representations where
multiple can be smoothly combined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A newer version of this manuscript was archived under 2312.11735</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProCQA: A Large-scale Community-based Programming Question Answering
  <span class="highlight-title">Dataset</span> for Code Search <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-based code question answering seeks to match user queries in
natural language to relevant code snippets. Previous approaches typically rely
on pretraining models using crafted bi-modal and uni-modal datasets to align
text and code representations. In this paper, we introduce ProCQA, a
large-scale programming question answering dataset extracted from the
StackOverflow community, offering naturally structured mixed-modal QA pairs. To
validate its effectiveness, we propose a modality-agnostic contrastive
pre-training approach to improve the alignment of text and code representations
of current code language models. Compared to previous models that primarily
employ bimodal and unimodal pairs extracted from CodeSearchNet for
pre-training, our model exhibits significant performance improvements across a
wide range of code retrieval benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative analysis of embedding models for patent similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grazia Sveva Ascione, Valerio Sterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Junhua, Tan Yong Keat, Fu Bin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the significant achievements of large language models (LLMs),
researchers have employed in-context learning for text classification tasks.
However, these studies focused on monolingual, single-turn classification
tasks. In this paper, we introduce LARA (Linguistic-Adaptive
Retrieval-Augmented Language Models), designed to enhance accuracy in
multi-turn classification tasks across six languages, accommodating numerous
intents in chatbot interactions. Multi-turn intent classification is notably
challenging due to the complexity and evolving nature of conversational
contexts. LARA tackles these issues by combining a fine-tuned smaller model
with a retrieval-augmented mechanism, integrated within the architecture of
LLMs. This integration allows LARA to dynamically utilize past dialogues and
relevant intents, thereby improving the understanding of the context.
Furthermore, our adaptive retrieval techniques bolster the cross-lingual
capabilities of LLMs without extensive retraining and fine-tune. Comprehensive
experiments demonstrate that LARA achieves state-of-the-art performance on
multi-turn intent classification tasks, enhancing the average accuracy by 3.67%
compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstUPR : Instruction-based Unsupervised Passage Reranking with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Wei Huang, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces InstUPR, an unsupervised passage reranking method based
on large language models (LLMs). Different from existing approaches that rely
on extensive training with query-document pairs or retrieval-specific
instructions, our method leverages the instruction-following capabilities of
instruction-tuned LLMs for passage reranking without any additional
fine-tuning. To achieve this, we introduce a soft score aggregation technique
and employ pairwise reranking for unsupervised passage reranking. Experiments
on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised
baselines as well as an instruction-tuned reranker, highlighting its
effectiveness and superiority. Source code to reproduce all experiments is
open-sourced at https://github.com/MiuLab/InstUPR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. This manuscript was originally written and submitted in
  June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Experiment with the Use of Chat<span class="highlight-title">GPT</span> for LCSH Subject Assignment on
  Electronic Theses and Dissertations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric H. C. Chow, TJ Kao, Xiaoli Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study delves into the potential use of Large Language Models (LLMs) for
generating Library of Congress Subject Headings (LCSH). The authors employed
ChatGPT to generate subject headings for electronic theses and dissertations
(ETDs) based on their titles and summaries. The results revealed that although
some generated subject headings were valid, there were issues regarding
specificity and exhaustiveness. The study showcases that LLMs can serve as a
strategic response to the backlog of items awaiting cataloging in academic
libraries, while also offering a cost-effective approach for promptly
generating LCSH. Nonetheless, human catalogers remain essential for verifying
and enhancing the validity, exhaustiveness, and specificity of LCSH generated
by LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Play to Your Strengths: Collaborative Intelligence of Conventional
  Recommender Models and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjia Xi, Weiwen Liu, Jianghao Lin, Chuhan Wu, Bo Chen, Ruiming Tang, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has opened new opportunities in
Recommender Systems (RSs) by enhancing user behavior modeling and content
understanding. However, current approaches that integrate LLMs into RSs solely
utilize either LLM or conventional recommender model (CRM) to generate final
recommendations, without considering which data segments LLM or CRM excel in.
To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books
datasets, and compare the performance of a representative CRM (DCNv2) and an
LLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that
LLMs excel in data segments where CRMs exhibit lower confidence and precision,
while samples where CRM excels are relatively challenging for LLM, requiring
substantial training data and a long training time for comparable performance.
This suggests potential synergies in the combination between LLM and CRM.
Motivated by these insights, we propose Collaborative Recommendation with
conventional Recommender and Large Language Model (dubbed \textit{CoReLLa}). In
this framework, we first jointly train LLM and CRM and address the issue of
decision boundary shifts through alignment loss. Then, the resource-efficient
CRM, with a shorter inference time, handles simple and moderate samples, while
LLM processes the small subset of challenging samples for CRM. Our experimental
results demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM
methods significantly, underscoring its effectiveness in recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Selective State Space Model's Capabilities in Lifelong
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyuan Yang, Yuanzi Li, Jingyu Zhao, Hanbing Wang, Muyang Ma, Jun Ma, Zhaochun Ren, Mengqi Zhang, Xin Xin, Zhumin Chen, Pengjie Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommenders have been widely applied in various online services,
aiming to model users' dynamic interests from their sequential interactions.
With users increasingly engaging with online platforms, vast amounts of
lifelong user behavioral sequences have been generated. However, existing
sequential recommender models often struggle to handle such lifelong sequences.
The primary challenges stem from computational complexity and the ability to
capture long-range dependencies within the sequence. Recently, a state space
model featuring a selective mechanism (i.e., Mamba) has emerged. In this work,
we investigate the performance of Mamba for lifelong sequential recommendation
(i.e., length>=2k). More specifically, we leverage the Mamba block to model
lifelong user sequences selectively. We conduct extensive experiments to
evaluate the performance of representative sequential recommendation models in
the setting of lifelong sequences. Experiments on two real-world datasets
demonstrate the superiority of Mamba. We found that RecMamba achieves
performance comparable to the representative model while significantly reducing
training duration by approximately 70% and memory costs by 80%. Codes and data
are available at \url{https://github.com/nancheng58/RecMamba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Facet Generation with LLM Editing <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joosung Lee, Jinhong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In information retrieval, facet identification of a user query is an
important task. If a search service can recognize the facets of a user's query,
it has the potential to offer users a much broader range of search results.
Previous studies can enhance facet prediction by leveraging retrieved documents
and related queries obtained through a search engine. However, there are
challenges in extending it to other applications when a search engine operates
as part of the model. First, search engines are constantly updated. Therefore,
additional information may change during training and test, which may reduce
performance. The second challenge is that public search engines cannot search
for internal documents. Therefore, a separate search system needs to be built
to incorporate documents from private domains within the company. We propose
two strategies that focus on a framework that can predict facets by taking only
queries as input without a search engine. The first strategy is multi-task
learning to predict SERP. By leveraging SERP as a target instead of a source,
the proposed model deeply understands queries without relying on external
modules. The second strategy is to enhance the facets by combining Large
Language Model (LLM) and the small model. Overall performance improves when
small model and LLM are combined rather than facet generation individually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EXPLORA: A teacher-apprentice methodology for eliciting natural
  child-computer interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanessa Figueiredo, Catherine Ann Cameron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Investigating child-computer interactions within their contexts is vital for
designing technology that caters to children's needs. However, determining what
aspects of context are relevant for designing child-centric technology remains
a challenge. We introduce EXPLORA, a multimodal, multistage online methodology
comprising three pivotal stages: (1) building a teacher-apprentice
relationship,(2) learning from child-teachers, and (3) assessing and
reinforcing researcher-apprentice learning. Central to EXPLORA is the
collection of attitudinal data through pre-observation interviews, offering
researchers a deeper understanding of children's characteristics and contexts.
This informs subsequent online observations, allowing researchers to focus on
frequent interactions. Furthermore, researchers can validate preliminary
assumptions with children. A means-ends analysis framework aids in the
systematic analysis of data, shedding light on context, agency and
homework-information searching processes children employ in their activities.
To illustrate EXPLORA's capabilities, we present nine single case studies
investigating Brazilian child-caregiver dyads' (children ages 9-11) use of
technology in homework information-searching.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Serbetar Karlo, Dong-Kyu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process
of drug development. DDIs occur when one drug's properties are affected by the
inclusion of other drugs. Detecting favorable DDIs has the potential to pave
the way for creating and advancing innovative medications applicable in
practical settings. However, existing DDI prediction models continue to face
challenges related to generalization in extreme cases, robust feature
extraction, and real-life application possibilities. We aim to address these
challenges by leveraging the effectiveness of context-aware deep graph learning
by introducing a novel framework named CADGL. Based on a customized variational
graph autoencoder (VGAE), we capture critical structural and physio-chemical
information using two context preprocessors for feature extraction from two
different perspectives: local neighborhood and molecular context, in a
heterogeneous graphical structure. Our customized VGAE consists of a graph
encoder, a latent information encoder, and an MLP decoder. CADGL surpasses
other state-of-the-art DDI prediction models, excelling in predicting
clinically valuable novel DDIs, supported by rigorous case studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 4 Figures; In review in IEEE/ACM Transactions on
  Computational Biology and Bioinformatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation of Asset Administration Shell with Large Language Model
  Agents: Interoperability in Digital Twins with Semantic Node 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Xia, Zhewen Xiao, Nasser Jazdi, Michael Weyrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces a novel approach for assisting the creation of Asset
Administration Shell (AAS) instances for digital twin modeling within the
context of Industry 4.0, aiming to enhance interoperability in smart
manufacturing and reduce manual effort. We construct a "semantic node" data
structure to capture the semantic essence of textual data. Then, a system
powered by large language models is designed and implemented to process
"semantic node" and generate AAS instance models from textual technical data.
Our evaluation demonstrates a 62-79% effective generation rate, indicating a
substantial proportion of manual creation effort can be converted into easier
validation effort, thereby reducing the time and cost in creating AAS instance
models. In our evaluation, a comparative analysis of different LLMs and an
in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms
provide insights into the effectiveness of LLM systems for interpreting
technical concepts. Our findings emphasize LLMs' capability in automating AAS
instance creation, enhancing semantic interoperability, and contributing to the
broader field of semantic interoperability for digital twins in industrial
applications. The prototype implementation and evaluation results are released
on our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print, submitted to IEEE ACCESS, under peer-review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI
  collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of ChatGPT and similar large language models (LLMs) has
revolutionized the human-AI interaction and information-seeking process.
Leveraging LLMs as an alternative to search engines, users can now access
summarized information tailored to their queries, significantly reducing the
cognitive load associated with navigating vast information resources. This
shift underscores the potential of LLMs in redefining information access
paradigms. Drawing on the foundation of task-focused information retrieval and
LLMs' task planning ability, this research extends the scope of LLM
capabilities beyond routine task automation to support users in navigating
long-term and significant life tasks. It introduces the GOLF framework
(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability
to assist in significant life decisions through goal orientation and long-term
planning. The methodology encompasses a comprehensive simulation study to test
the framework's efficacy, followed by model and human evaluations to develop a
dataset benchmark for long-term life tasks, and experiments across different
models and settings. By shifting the focus from short-term tasks to the broader
spectrum of long-term life goals, this research underscores the transformative
potential of LLMs in enhancing human decision-making processes and task
management, marking a significant step forward in the evolution of human-AI
collaboration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning-based Recommender Systems with Large Language
  Models for State Reward and Action Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Wang, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL)-based recommender systems have demonstrated
promising performance in meeting user expectations by learning to make accurate
next-item recommendations from historical user-item interactions. However,
existing offline RL-based sequential recommendation methods face the challenge
of obtaining effective user feedback from the environment. Effectively modeling
the user state and shaping an appropriate reward for recommendation remains a
challenge. In this paper, we leverage language understanding capabilities and
adapt large language models (LLMs) as an environment (LE) to enhance RL-based
recommenders. The LE is learned from a subset of user-item interaction data,
thus reducing the need for large training data, and can synthesise user
feedback for offline data by: (i) acting as a state model that produces high
quality states that enrich the user representation, and (ii) functioning as a
reward model to accurately capture nuanced user preferences on actions.
Moreover, the LE allows to generate positive actions that augment the limited
offline training data. We propose a LE Augmentation (LEA) method to further
improve recommendation performance by optimising jointly the supervised
component and the RL policy, using the augmented actions and historical user
signals. We use LEA, the state and reward models in conjunction with
state-of-the-art RL recommenders and report experimental results on two
publicly available datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GloSIS: The Global Soil Information System Web Ontology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raul Palma, Bogusz Janiak, Luís Moreira de Sousa, Kathi Schleidt, Tomáš Řezník, Fenny van Egmond, Johan Leenaars, Dimitrios Moshou, Abdul Mouazen, Peter Wilson, David Medyckyj-Scott, Alistair Ritchie, Yusuf Yigini, Ronald Vargas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Established in 2012 by members of the Food and Agriculture Organisation
(FAO), the Global Soil Partnership (GSP) is a global network of stakeholders
promoting sound land and soil management practices towards a sustainable world
food system. However, soil survey largely remains a local or regional activity,
bound to heterogeneous methods and conventions. Recognising the relevance of
global and trans-national policies towards sustainable land management
practices, the GSP elected data harmonisation and exchange as one of its key
lines of action. Building upon international standards and previous work
towards a global soil data ontology, an improved domain model was eventually
developed within the GSP [54], the basis for a Global Soil Information System
(GloSIS). This work also identified the Semantic Web as a possible avenue to
operationalise the domain model. This article presents the GloSIS web ontology,
an implementation of the GloSIS domain model with the Web Ontology Language
(OWL). Thoroughly employing a host of Semantic Web standards (SOSA, SKOS,
GeoSPARQL, QUDT), GloSIS lays out not only a soil data ontology but also an
extensive set of ready-to-use code-lists for soil description and
physio-chemical analysis. Various examples are provided on the provision and
use of GloSIS-compliant linked data, showcasing the contribution of this
ontology to the discovery, exploration, integration and access of soil data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Augmentation for Recommendation <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianru Zhang, Lianghao Xia, Xuheng Cai, Siuming Yiu, Chao Huang, Christian S. Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph augmentation with contrastive learning has gained significant attention
in the field of recommendation systems due to its ability to learn expressive
user representations, even when labeled data is limited. However, directly
applying existing GCL models to real-world recommendation environments poses
challenges. There are two primary issues to address. Firstly, the lack of
consideration for data noise in contrastive learning can result in noisy
self-supervised signals, leading to degraded performance. Secondly, many
existing GCL approaches rely on graph neural network (GNN) architectures, which
can suffer from over-smoothing problems due to non-adaptive message passing. To
address these challenges, we propose a principled framework called GraphAug.
This framework introduces a robust data augmentor that generates denoised
self-supervised signals, enhancing recommender systems. The GraphAug framework
incorporates a graph information bottleneck (GIB)-regularized augmentation
paradigm, which automatically distills informative self-supervision information
and adaptively adjusts contrastive view generation. Through rigorous
experimentation on real-world datasets, we thoroughly assessed the performance
of our novel GraphAug model. The outcomes consistently unveil its superiority
over existing baseline methods. The source code for our model is publicly
available at: https://github.com/HKUDS/GraphAug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages and accepted by ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word4Per: Zero-shot Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for specific person has great social benefits and security value,
and it often involves a combination of visual and textual information.
Conventional person retrieval methods, whether image-based or text-based,
usually fall short in effectively harnessing both types of information, leading
to the loss of accuracy. In this paper, a whole new task called Composed Person
Retrieval (CPR) is proposed to jointly utilize both image and text information
for target person retrieval. However, the supervised CPR requires very costly
manual annotation dataset, while there are currently no available resources. To
mitigate this issue, we firstly introduce the Zero-shot Composed Person
Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the
CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we
propose a two-stage learning framework, Word4Per, where a lightweight Textual
Inversion Network (TINet) and a text-based person retrieval model based on
fine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned
without utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed
Person Retrieval (ITCPR) dataset is built as the benchmark to assess the
performance of the proposed Word4Per framework. Extensive experiments under
both Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR
task, surpassing the comparative methods by over 10\%. The code and ITCPR
dataset will be publicly available at
https://github.com/Delong-liu-bupt/Word4Per.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the resilience of Collaborative Learning-based Recommender Systems
  Against Community Detection Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yacine Belal, Sonia Ben Mokhtar, Mohamed Maouche, Anthony Simonet-Boulogne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative-learning-based recommender systems emerged following the
success of collaborative learning techniques such as Federated Learning (FL)
and Gossip Learning (GL). In these systems, users participate in the training
of a recommender system while maintaining their history of consumed items on
their devices. While these solutions seemed appealing for preserving the
privacy of the participants at first glance, recent studies have revealed that
collaborative learning can be vulnerable to various privacy attacks. In this
paper, we study the resilience of collaborative learning-based recommender
systems against a novel privacy attack called Community Detection Attack (CDA).
This attack enables an adversary to identify community members based on a
chosen set of items (eg., identifying users interested in specific
points-of-interest). Through experiments on three real recommendation datasets
using two state-of-the-art recommendation models, we evaluate the sensitivity
of an FL-based recommender system as well as two flavors of Gossip
Learning-based recommender systems to CDA. The results show that across all
models and datasets, the FL setting is more vulnerable to CDA compared to
Gossip settings. Furthermore, we assess two off-the-shelf mitigation
strategies, namely differential privacy (DP) and a \emph{Share less} policy,
which consists of sharing a subset of less sensitive model parameters. The
findings indicate a more favorable privacy-utility trade-off for the
\emph{Share less} strategy, particularly in FedRecs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Large Language Models as Generative User Simulators for
  Conversational Recommendation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09738v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09738v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic users are cost-effective proxies for real users in the evaluation
of conversational recommender systems. Large language models show promise in
simulating human-like behavior, raising the question of their ability to
represent a diverse population of users. We introduce a new protocol to measure
the degree to which language models can accurately emulate human behavior in
conversational recommendation. This protocol is comprised of five tasks, each
designed to evaluate a key property that a synthetic user should exhibit:
choosing which items to talk about, expressing binary preferences, expressing
open-ended preferences, requesting recommendations, and giving feedback.
Through evaluation of baseline simulators, we demonstrate these tasks
effectively reveal deviations of language models from human behavior, and offer
insights on how to reduce the deviations with model selection and prompting
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoteLLM: A Retrievable Large Language Model for Note Recommendation <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhang, Shiwei Wu, Haoxin Zhang, Tong Xu, Yan Gao, Yao Hu, Di Wu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People enjoy sharing "notes" including their experiences within online
communities. Therefore, recommending notes aligned with user interests has
become a crucial task. Existing online methods only input notes into BERT-based
models to generate note embeddings for assessing similarity. However, they may
underutilize some important cues, e.g., hashtags or categories, which represent
the key concepts of notes. Indeed, learning to generate hashtags/categories can
potentially enhance note embeddings, both of which compress key note
information into limited content. Besides, Large Language Models (LLMs) have
significantly outperformed BERT in understanding natural languages. It is
promising to introduce LLMs into note recommendation. In this paper, we propose
a novel unified framework called NoteLLM, which leverages LLMs to address the
item-to-item (I2I) note recommendation. Specifically, we utilize Note
Compression Prompt to compress a note into a single special token, and further
learn the potentially related notes' embeddings via a contrastive learning
approach. Moreover, we use NoteLLM to summarize the note and generate the
hashtag/category automatically through instruction tuning. Extensive
validations on real scenarios demonstrate the effectiveness of our proposed
method compared with the online baseline and show major improvements in the
recommendation system of Xiaohongshu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a WWW'24 full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNNUERS: Fairness Explanation in GNNs for Recommendation via
  Counterfactual Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06182v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06182v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Medda, Francesco Fabbri, Mirko Marras, Ludovico Boratto, Gianni Fenu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, research into personalization has been focusing on explainability
and fairness. Several approaches proposed in recent works are able to explain
individual recommendations in a post-hoc manner or by explanation paths.
However, explainability techniques applied to unfairness in recommendation have
been limited to finding user/item features mostly related to biased
recommendations. In this paper, we devised a novel algorithm that leverages
counterfactuality methods to discover user unfairness explanations in the form
of user-item interactions. In our counterfactual framework, interactions are
represented as edges in a bipartite graph, with users and items as nodes. Our
bipartite graph explainer perturbs the topological structure to find an altered
version that minimizes the disparity in utility between the protected and
unprotected demographic groups. Experiments on four real-world graphs coming
from various domains showed that our method can systematically explain user
unfairness on three state-of-the-art GNN-based recommendation models. Moreover,
an empirical evaluation of the perturbed network uncovered relevant patterns
that justify the nature of the unfairness discovered by the generated
explanations. The source code and the preprocessed data sets are available at
https://github.com/jackmedda/RS-BGExplainer.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Domain Incremental Learning <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasushi Esaki, Satoshi Koide, Takuro Kutsuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain incremental learning (DIL) has been discussed in previous studies on
deep neural network models for classification. In DIL, we assume that samples
on new domains are observed over time. The models must classify inputs on all
domains. In practice, however, we may encounter a situation where we need to
perform DIL under the constraint that the samples on the new domain are
observed only infrequently. Therefore, in this study, we consider the extreme
case where we have only one sample from the new domain, which we call one-shot
DIL. We first empirically show that existing DIL methods do not work well in
one-shot DIL. We have analyzed the reason for this failure through various
investigations. According to our analysis, we clarify that the difficulty of
one-shot DIL is caused by the statistics in the batch normalization layers.
Therefore, we propose a technique regarding these statistics and demonstrate
the effectiveness of our technique through experiments on open datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IEEE International Joint Conference on Neural Networks
  (IJCNN) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Performance of Deep Learning for Automated Gleason Grading
  in Prostate Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Daniel Hieber, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Frank Kramer, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer is a dominant health concern calling for advanced diagnostic
tools. Utilizing digital pathology and artificial intelligence, this study
explores the potential of 11 deep neural network architectures for automated
Gleason grading in prostate carcinoma focusing on comparing traditional and
recent architectures. A standardized image classification pipeline, based on
the AUCMEDI framework, facilitated robust evaluation using an in-house dataset
consisting of 34,264 annotated tissue tiles. The results indicated varying
sensitivity across architectures, with ConvNeXt demonstrating the strongest
performance. Notably, newer architectures achieved superior performance, even
though with challenges in differentiating closely related Gleason grades. The
ConvNeXt model was capable of learning a balance between complexity and
generalizability. Overall, this study lays the groundwork for enhanced Gleason
grading systems, potentially improving diagnostic efficiency for prostate
cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synapse: Learning Preferential Concepts from Visual Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of preference learning, which aims to learn
user-specific preferences (e.g., "good parking spot", "convenient drop-off
location") from visual input. Despite its similarity to learning factual
concepts (e.g., "red cube"), preference learning is a fundamentally harder
problem due to its subjective nature and the paucity of person-specific
training data. We address this problem using a new framework called Synapse,
which is a neuro-symbolic approach designed to efficiently learn preferential
concepts from limited demonstrations. Synapse represents preferences as
neuro-symbolic programs in a domain-specific language (DSL) that operates over
images, and leverages a novel combination of visual parsing, large language
models, and program synthesis to learn programs representing individual
preferences. We evaluate Synapse through extensive experimentation including a
user case study focusing on mobility-related concepts in mobile robotics and
autonomous driving. Our evaluation demonstrates that Synapse significantly
outperforms existing baselines as well as its own ablations. The code and other
details can be found on the project website https://amrl.cs.utexas.edu/synapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures; Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A note on generalization bounds for losses with finite moments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borja Rodríguez-Gálvez, Omar Rivasplata, Ragnar Thobaben, Mikael Skoglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the truncation method from Alquier [1] to derive
high-probability PAC-Bayes bounds for unbounded losses with heavy tails.
Assuming that the $p$-th moment is bounded, the resulting bounds interpolate
between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p
\to \infty$ and the loss is essentially bounded. Moreover, the paper derives a
high-probability PAC-Bayes bound for losses with a bounded variance. This bound
has an exponentially better dependence on the confidence parameter and the
dependency measure than previous bounds in the literature. Finally, the paper
extends all results to guarantees in expectation and single-draw PAC-Bayes. In
order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded
losses from [2] in these settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages: 5 of main text, 1 of references, and 3 of appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rene Winchenbach, Nils Thuerey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning physical simulations has been an essential and central aspect of
many recent research efforts in machine learning, particularly for
Navier-Stokes-based fluid mechanics. Classic numerical solvers have
traditionally been computationally expensive and challenging to use in inverse
problems, whereas Neural solvers aim to address both concerns through machine
learning. We propose a general formulation for continuous convolutions using
separable basis functions as a superset of existing methods and evaluate a
large set of basis functions in the context of (a) a compressible 1D SPH
simulation, (b) a weakly compressible 2D SPH simulation, and (c) an
incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries
included in the basis functions are key aspects of stability and accuracy. Our
broad evaluation shows that Fourier-based continuous convolutions outperform
all other architectures regarding accuracy and generalization. Finally, using
these Fourier-based networks, we show that prior inductive biases, such as
window functions, are no longer necessary. An implementation of our approach,
as well as complete datasets and solver implementations, is available at
https://github.com/tum-pbs/SFBC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Learning Representation
  (ICLR) 2024, 54 pages, 39 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepGleason: a System for Automated Gleason Grading of Prostate Cancer
  using Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in digital pathology and artificial intelligence (AI) offer
promising opportunities for clinical decision support and enhancing diagnostic
workflows. Previous studies already demonstrated AI's potential for automated
Gleason grading, but lack state-of-the-art methodology and model reusability.
To address this issue, we propose DeepGleason: an open-source deep neural
network based image classification system for automated Gleason grading using
whole-slide histopathology images from prostate tissue sections. Implemented
with the standardized AUCMEDI framework, our tool employs a tile-wise
classification approach utilizing fine-tuned image preprocessing techniques in
combination with a ConvNeXt architecture which was compared to various
state-of-the-art architectures. The neural network model was trained and
validated on an in-house dataset of 34,264 annotated tiles from 369 prostate
carcinoma slides. We demonstrated that DeepGleason is capable of highly
accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,
AUC of 0.991, and Accuracy of 0.974. The internal architecture comparison
revealed that the ConvNeXt model was superior performance-wise on our dataset
to established and other modern architectures like transformers. Furthermore,
we were able to outperform the current state-of-the-art in tile-wise
fine-classification with a sensitivity and specificity of 0.94 and 0.98 for
benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs
Gleason 4 & 5 classification, respectively. Our tool contributes to the wider
adoption of AI-based Gleason grading within the research community and paves
the way for broader clinical application of deep learning models in digital
pathology. DeepGleason is open-source and publicly available for research
application in the following Git repository:
https://github.com/frankkramer-lab/DeepGleason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOOL: Addressing the Downlink Bottleneck in Satellite Computing with
  Neural Feature Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanosatellite constellations equipped with sensors capturing large geographic
regions provide unprecedented opportunities for Earth observation. As
constellation sizes increase, network contention poses a downlink bottleneck.
Orbital Edge Computing (OEC) leverages limited onboard compute resources to
reduce transfer costs by processing the raw captures at the source. However,
current solutions have limited practicability due to reliance on crude
filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression
method that preserves prediction performance. FOOL partitions high-resolution
satellite imagery to maximize throughput. Further, it embeds context and
leverages inter-tile dependencies to lower transfer costs with negligible
overhead. While FOOL is a feature compressor, it can recover images with
competitive scores on perceptual quality measures at lower bitrates. We
extensively evaluate transfer cost reduction by including the peculiarity of
intermittently available network connections in low earth orbit. Lastly, we
test the feasibility of our system for standardized nanosatellite form factors.
We demonstrate that FOOL permits downlinking over 100x the data volume without
relying on prior information on the downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, double column, 19 figures, 7 tables, Initial Submission to
  IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Functional Roles of Modelling Components in Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huifeng Yin, Hanle Zheng, Jiayi Mao, Siyuan Ding, Xing Liu, Mingkun Xu, Yifan Hu, Jing Pei, Lei Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,
are promising in achieving high computational efficiency with biological
fidelity. Nevertheless, it is quite difficult to optimize SNNs because the
functional roles of their modelling components remain unclear. By designing and
evaluating several variants of the classic model, we systematically investigate
the functional roles of key modelling components, leakage, reset, and
recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive
experiments, we demonstrate how these components influence the accuracy,
generalization, and robustness of SNNs. Specifically, we find that the leakage
plays a crucial role in balancing memory retention and robustness, the reset
mechanism is essential for uninterrupted temporal processing and computational
efficiency, and the recurrence enriches the capability to model complex
dynamics at a cost of robustness degradation. With these interesting
observations, we provide optimization suggestions for enhancing the performance
of SNNs in different scenarios. This work deepens the understanding of how SNNs
work, which offers valuable guidance for the development of more effective and
robust neuromorphic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Augmentation for Recommendation <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianru Zhang, Lianghao Xia, Xuheng Cai, Siuming Yiu, Chao Huang, Christian S. Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph augmentation with contrastive learning has gained significant attention
in the field of recommendation systems due to its ability to learn expressive
user representations, even when labeled data is limited. However, directly
applying existing GCL models to real-world recommendation environments poses
challenges. There are two primary issues to address. Firstly, the lack of
consideration for data noise in contrastive learning can result in noisy
self-supervised signals, leading to degraded performance. Secondly, many
existing GCL approaches rely on graph neural network (GNN) architectures, which
can suffer from over-smoothing problems due to non-adaptive message passing. To
address these challenges, we propose a principled framework called GraphAug.
This framework introduces a robust data augmentor that generates denoised
self-supervised signals, enhancing recommender systems. The GraphAug framework
incorporates a graph information bottleneck (GIB)-regularized augmentation
paradigm, which automatically distills informative self-supervision information
and adaptively adjusts contrastive view generation. Through rigorous
experimentation on real-world datasets, we thoroughly assessed the performance
of our novel GraphAug model. The outcomes consistently unveil its superiority
over existing baseline methods. The source code for our model is publicly
available at: https://github.com/HKUDS/GraphAug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages and accepted by ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Loss Function-based Support Vector Machine for Binary
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Liping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss
SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the
degree of penalty for the correctly classified samples within the margin. This
oversight affects the generalization ability of the SVM classifier to some
extent. To address this limitation, from the perspective of confidence margin,
we propose a novel Slide loss function ($\ell_s$) to construct the support
vector machine classifier($\ell_s$-SVM). By introducing the concept of proximal
stationary point, and utilizing the property of Lipschitz continuity, we derive
the first-order optimality conditions for $\ell_s$-SVM. Based on this, we
define the $\ell_s$ support vectors and working set of $\ell_s$-SVM. To
efficiently handle $\ell_s$-SVM, we devise a fast alternating direction method
of multipliers with the working set ($\ell_s$-ADMM), and provide the
convergence analysis. The numerical experiments on real world datasets confirm
the robustness and effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Sim-to-Real Gap with Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Rothfuss, Bhavya Sukhija, Lenart Treven, Florian Dörfler, Stelian Coros, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SIM-FSVGD for learning robot dynamics from data. As opposed to
traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in
the form of simulators, to regularize the training of neural network models.
While learning accurate dynamics already in the low data regime, SIM-FSVGD
scales and excels also when more data is available. We empirically show that
learning with implicit physical priors results in accurate mean model
estimation as well as precise uncertainty quantification. We demonstrate the
effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a
high-performance RC racecar system. Using model-based RL, we demonstrate a
highly dynamic parking maneuver with drifting, using less than half the data
compared to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Texture Loss for CT denoising with GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Feola, Lorenzo Tronchin, Valerio Guarrasi, Paolo Soda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have proved as a powerful framework
for denoising applications in medical imaging. However, GAN-based denoising
algorithms still suffer from limitations in capturing complex relationships
within the images. In this regard, the loss function plays a crucial role in
guiding the image generation process, encompassing how much a synthetic image
differs from a real image. To grasp highly complex and non-linear textural
relationships in the training process, this work presents a loss function that
leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence
Matrix (GLCM). Although the recent advances in deep learning have demonstrated
superior performance in classification and detection tasks, we hypothesize that
its information content can be valuable when integrated into GANs' training. To
this end, we propose a differentiable implementation of the GLCM suited for
gradient-based optimization. Our approach also introduces a self-attention
layer that dynamically aggregates the multi-scale texture information extracted
from the images. We validate our approach by carrying out extensive experiments
in the context of low-dose CT denoising, a challenging application that aims to
enhance the quality of noisy CT scans. We utilize three publicly available
datasets, including one simulated and two real datasets. The results are
promising as compared to other well-established loss functions, being also
consistent across three different GAN architectures. The code is available at:
https://github.com/FrancescoDiFeola/DenoTextureLoss
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative analysis of embedding models for patent similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grazia Sveva Ascione, Valerio Sterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Busra Asan, Abdullah Akgul, Alper Unal, Melih Kandemir, Gozde Unal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seasonal forecasting is a crucial task when it comes to detecting the extreme
heat and colds that occur due to climate change. Confidence in the predictions
should be reliable since a small increase in the temperatures in a year has a
big impact on the world. Calibration of the neural networks provides a way to
ensure our confidence in the predictions. However, calibrating regression
models is an under-researched topic, especially in forecasters. We calibrate a
UNet++ based architecture, which was shown to outperform physics-based models
in temperature anomalies. We show that with a slight trade-off between
prediction error and calibration error, it is possible to get more reliable and
sharper forecasts. We believe that calibration should be an important part of
safety-critical machine learning applications such as weather forecasters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a workshop paper at "ICLR 2024 Tackling Climate Change
  with Machine Learning"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed collaborative anomalous sound detection by embedding sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Dohi, Yohei Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To develop a machine sound monitoring system, a method for detecting
anomalous sound is proposed. In this paper, we explore a method for multiple
clients to collaboratively learn an anomalous sound detection model while
keeping their raw data private from each other. In the context of industrial
machine anomalous sound detection, each client possesses data from different
machines or different operational states, making it challenging to learn
through federated learning or split learning. In our proposed method, each
client calculates embeddings using a common pre-trained model developed for
sound data classification, and these calculated embeddings are aggregated on
the server to perform anomalous sound detection through outlier exposure.
Experiments showed that our proposed method improves the AUC of anomalous sound
detection by an average of 6.8%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction
  and Defect-Focus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Li, Ruijie Ma, Xiang Qian, Xiaohao Wang, Xinghui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenge of data scarcity in industrial domains, transfer
learning emerges as a pivotal paradigm. This work introduces Style Filter, a
tailored methodology for industrial contexts. By selectively filtering source
domain data before knowledge transfer, Style Filter reduces the quantity of
data while maintaining or even enhancing the performance of transfer learning
strategy. Offering label-free operation, minimal reliance on prior knowledge,
independence from specific models, and re-utilization, Style Filter is
evaluated on authentic industrial datasets, highlighting its effectiveness when
employed before conventional transfer strategies in the deep learning domain.
The results underscore the effectiveness of Style Filter in real-world
industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures,4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kudaibergen Abutalip, Numan Saeed, Ikboljon Sobirov, Vincent Andrearczyk, Adrien Depeursinge, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying deep learning (DL) models in medical applications relies on
predictive performance and other critical factors, such as conveying
trustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide
potential solutions for evaluating prediction reliability and improving the
model confidence calibration. Despite increasing interest in UE, challenges
persist, such as the need for explicit methods to capture aleatoric uncertainty
and align uncertainty estimates with real-life disagreements among domain
experts. This paper proposes an Expert Disagreement-Guided Uncertainty
Estimation (EDUE) for medical image segmentation. By leveraging variability in
ground-truth annotations from multiple raters, we guide the model during
training and incorporate random sampling-based strategies to enhance
calibration confidence. Our method achieves 55% and 23% improvement in
correlation on average with expert disagreements at the image and pixel levels,
respectively, better calibration, and competitive segmentation performance
compared to the state-of-the-art deep ensembles, requiring only a single
forward pass.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering the Interplay between Local Differential Privacy, Average
  Bayesian Privacy, and Maximum Bayesian Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojin Zhang, Yulin Fei, Wei Chen, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The swift evolution of machine learning has led to emergence of various
definitions of privacy due to the threats it poses to privacy, including the
concept of local differential privacy (LDP). Although widely embraced and
utilized across numerous domains, this conventional approach to measure privacy
still exhibits certain limitations, spanning from failure to prevent
inferential disclosure to lack of consideration for the adversary's background
knowledge. In this comprehensive study, we introduce Bayesian privacy and delve
into the intricate relationship between local differential privacy and its
Bayesian counterparts, unveiling novel insights into utility-privacy
trade-offs. We introduce a framework that encapsulates both attack and defense
strategies, highlighting their interplay and effectiveness. Our theoretical
contributions are anchored in the rigorous definitions and relationships
between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),
encapsulated by equations $\epsilon_{p,a} \leq
\frac{1}{\sqrt{2}}\sqrt{(\epsilon_{p,m} + \epsilon)\cdot(e^{\epsilon_{p,m} +
\epsilon} - 1)}$ and the equivalence between $\xi$-MBP and $2\xi$-LDP
established under uniform prior distribution. These relationships fortify our
understanding of the privacy guarantees provided by various mechanisms, leading
to the realization that a mechanism satisfying $\xi$-LDP also confers
$\xi$-MBP, and vice versa. Our work not only lays the groundwork for future
empirical exploration but also promises to enhance the design of
privacy-preserving algorithms that do not compromise on utility, thereby
fostering the development of trustworthy machine learning solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In the Search for Optimal Multi-view Learning Models for Crop
  Classification with Global Remote Sensing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mena, Diego Arenas, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop classification is of critical importance due to its role in studying
crop pattern changes, resource management, and carbon sequestration. When
employing data-driven techniques for its prediction, utilizing various temporal
data sources is necessary. Deep learning models have proven to be effective for
this task by mapping time series data to high-level representation for
prediction. However, they face substantial challenges when dealing with
multiple input patterns. The literature offers limited guidance for Multi-View
Learning (MVL) scenarios, as it has primarily focused on exploring fusion
strategies with specific encoders and validating them in local regions. In
contrast, we investigate the impact of simultaneous selection of the fusion
strategy and the encoder architecture evaluated on a global-scale cropland and
crop-type classifications. We use a range of five fusion strategies (Input,
Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures
(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The
validation is on the CropHarvest dataset that provides optical, radar, and
weather time series, and topographic information as input data. We found that
in scenarios with a limited number of labeled samples, a unique configuration
is insufficient for all the cases. Instead, a specialized combination,
including encoder and fusion strategy, should be meticulously sought. To
streamline this search process, we suggest initially identifying the optimal
encoder architecture tailored for a particular fusion strategy, and then
determining the most suitable fusion strategy for the classification task. We
provide a technical framework for researchers exploring crop classification or
related tasks through a MVL approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Antigen-Specific Antibody Design via Direct Energy-based Preference
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibody design, a crucial task with significant implications across various
disciplines such as therapeutics and biology, presents considerable challenges
due to its intricate nature. In this paper, we tackle antigen-specific antibody
design as a protein sequence-structure co-design problem, considering both
rationality and functionality. Leveraging a pre-trained conditional diffusion
model that jointly models sequences and structures of
complementarity-determining regions (CDR) in antibodies with equivariant neural
networks, we propose direct energy-based preference optimization to guide the
generation of antibodies with both rational structures and considerable binding
affinities to given antigens. Our method involves fine-tuning the pre-trained
diffusion model using a residue-level decomposed energy preference.
Additionally, we employ gradient surgery to address conflicts between various
types of energy, such as attraction and repulsion. Experiments on RAbD
benchmark show that our approach effectively optimizes the energy of generated
antibodies and achieves state-of-the-art performance in designing high-quality
antibodies with low total energy and high binding affinity, demonstrating the
superiority of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NSINA: A News Corpus for Sinhala <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu Ranasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of large language models (LLMs) has advanced natural
language processing (NLP), but their effectiveness is largely dependent on
pre-training resources. This is especially evident in low-resource languages,
such as Sinhala, which face two primary challenges: the lack of substantial
training data and limited benchmarking datasets. In response, this study
introduces NSINA, a comprehensive news corpus of over 500,000 articles from
popular Sinhala news websites, along with three NLP tasks: news media
identification, news category prediction, and news headline generation. The
release of NSINA aims to provide a solution to challenges in adapting LLMs to
Sinhala, offering valuable resources and benchmarks for improving NLP in the
Sinhala language. NSINA is the largest news corpus for Sinhala, available up to
date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Vulnerabilities of Neural Networks in Parameter Learning and
  Defense Against Explanation-Aware Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) strategies play a crucial part in
increasing the understanding and trustworthiness of neural networks.
Nonetheless, these techniques could potentially generate misleading
explanations. Blinding attacks can drastically alter a machine learning
algorithm's prediction and explanation, providing misleading information by
adding visually unnoticeable artifacts into the input, while maintaining the
model's accuracy. It poses a serious challenge in ensuring the reliability of
XAI methods. To ensure the reliability of XAI methods poses a real challenge,
we leverage statistical analysis to highlight the changes in CNN weights within
a CNN following blinding attacks. We introduce a method specifically designed
to limit the effectiveness of such attacks during the evaluation phase,
avoiding the need for extra training. The method we suggest defences against
most modern explanation-aware adversarial attacks, achieving an approximate
decrease of ~99\% in the Attack Success Rate (ASR) and a ~91\% reduction in the
Mean Square Error (MSE) between the original explanation and the defended
(post-attack) explanation across three unique types of attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Ji, Zhaowei Zhu, Wei Xi, Olga Gadyatskaya, Zilong Song, Yong Cai, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) heavily depends on label quality for its performance.
However, the label distribution among individual clients is always both noisy
and heterogeneous. The high loss incurred by client-specific samples in
heterogeneous label noise poses challenges for distinguishing between
client-specific and noisy label samples, impacting the effectiveness of
existing label noise learning approaches. To tackle this issue, we propose
FedFixer, where the personalized model is introduced to cooperate with the
global model to effectively select clean client-specific samples. In the dual
models, updating the personalized model solely at a local level can lead to
overfitting on noisy data due to limited samples, consequently affecting both
the local and global models' performance. To mitigate overfitting, we address
this concern from two perspectives. Firstly, we employ a confidence regularizer
to alleviate the impact of unconfident predictions caused by label noise.
Secondly, a distance regularizer is implemented to constrain the disparity
between the personalized and global models. We validate the effectiveness of
FedFixer through extensive experiments on benchmark datasets. The results
demonstrate that FedFixer can perform well in filtering noisy label samples on
different clients, especially in highly heterogeneous label noise scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAA24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Federated Learning by Selecting Beneficial Herd of Local
  Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Luo, Xiaoge Deng, Ziqing Wen, Tao Sun, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed machine learning framework in
communication network systems. However, the systems' Non-Independent and
Identically Distributed (Non-IID) data negatively affect the convergence
efficiency of the global model, since only a subset of these data samples are
beneficial for model convergence. In pursuit of this subset, a reliable
approach involves determining a measure of validity to rank the samples within
the dataset. In this paper, We propose the BHerd strategy which selects a
beneficial herd of local gradients to accelerate the convergence of the FL
model. Specifically, we map the distribution of the local dataset to the local
gradients and use the Herding strategy to obtain a permutation of the set of
gradients, where the more advanced gradients in the permutation are closer to
the average of the set of gradients. These top portion of the gradients will be
selected and sent to the server for global aggregation. We conduct experiments
on different datasets, models and scenarios by building a prototype system, and
experimental results demonstrate that our BHerd strategy is effective in
selecting beneficial local gradients to mitigate the effects brought by the
Non-IID dataset, thus accelerating model convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Online Federated Learning with Correlated Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaojiao Zhang, Linglingzhi Zhu, Mikael Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel differentially private algorithm for online federated
learning that employs temporally correlated noise to improve the utility while
ensuring the privacy of the continuously released models. To address challenges
stemming from DP noise and local updates with streaming noniid data, we develop
a perturbed iterate analysis to control the impact of the DP noise on the
utility. Moreover, we demonstrate how the drift errors from local updates can
be effectively managed under a quasi-strong convexity condition. Subject to an
$(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the
entire time horizon that quantifies the impact of key parameters and the
intensity of changes in dynamic environments. Numerical experiments validate
the efficacy of the proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Discovery from Poisson Branching Structural Causal Model Using
  High-Order Cumulant with Path Analysis <span class="chip">AAAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Qiao, Yu Xiang, Zhengming Chen, Ruichu Cai, Zhifeng Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Count data naturally arise in many fields, such as finance, neuroscience, and
epidemiology, and discovering causal structure among count data is a crucial
task in various scientific and industrial scenarios. One of the most common
characteristics of count data is the inherent branching structure described by
a binomial thinning operator and an independent Poisson distribution that
captures both branching and noise. For instance, in a population count
scenario, mortality and immigration contribute to the count, where survival
follows a Bernoulli distribution, and immigration follows a Poisson
distribution. However, causal discovery from such data is challenging due to
the non-identifiability issue: a single causal pair is Markov equivalent, i.e.,
$X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately,
in this work, we found that the causal order from $X$ to its child $Y$ is
identifiable if $X$ is a root vertex and has at least two directed paths to
$Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed
path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching
Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using
high-order cumulants. Theoretical results establish the connection between the
path and cumulant and demonstrate that the path information can be obtained
from the cumulant. With the path information, causal order is identifiable
under some graphical conditions. A practical algorithm for learning causal
structure under PB-SCM is proposed and the experiments demonstrate and verify
the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Understanding AI Paper Challenge 2024 -- <span class="highlight-title">Dataset</span> Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se Won Oh, Hyuntae Jeong, Jeong Mook Lim, Seungeun Chung, Kyoung Ju Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 2024, we will hold a research paper competition (the third Human
Understanding AI Paper Challenge) for the research and development of
artificial intelligence technologies to understand human daily life. This
document introduces the datasets that will be provided to participants in the
competition, and summarizes the issues to consider in data processing and
learning model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathoTune: Adapting Visual Foundation Model to Pathological Specialists <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Lu, Fang Yan, Xiaofan Zhang, Yue Gao, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As natural image understanding moves towards the pretrain-finetune era,
research in pathology imaging is concurrently evolving. Despite the predominant
focus on pretraining pathological foundation models, how to adapt foundation
models to downstream tasks is little explored. For downstream adaptation, we
propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the
Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework
designed to efficiently adapt pathological or even visual foundation models to
pathology-specific tasks via multi-modal prompt tuning. The proposed framework
leverages Task-specific Visual Prompts and Task-specific Textual Prompts to
identify task-relevant features, along with Instance-specific Visual Prompts
for encoding single pathological image features. Results across multiple
datasets at both patch-level and WSI-level demonstrate its superior performance
over single-modality prompt tuning approaches. Significantly, PathoTune
facilitates the direct adaptation of natural visual foundation models to
pathological tasks, drastically outperforming pathological foundation models
with simple linear probing. The code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSTTN: A Long-Short Term <span class="highlight-title">Transformer</span>-based Spatio-temporal Neural
  Network for Traffic Flow Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinyao Luo, Silu He, Xing Han, Yuhan Wang, Haifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate traffic forecasting is a fundamental problem in intelligent
transportation systems and learning long-range traffic representations with key
information through spatiotemporal graph neural networks (STGNNs) is a basic
assumption of current traffic flow prediction models. However, due to
structural limitations, existing STGNNs can only utilize short-range traffic
flow data; therefore, the models cannot adequately learn the complex trends and
periodic features in traffic flow. Besides, it is challenging to extract the
key temporal information from the long historical traffic series and obtain a
compact representation. To solve the above problems, we propose a novel LSTTN
(Long-Short Term Transformer-based Network) framework comprehensively
considering the long- and short-term features in historical traffic flow.
First, we employ a masked subseries Transformer to infer the content of masked
subseries from a small portion of unmasked subseries and their temporal context
in a pretraining manner, forcing the model to efficiently learn compressed and
contextual subseries temporal representations from long historical series.
Then, based on the learned representations, long-term trend is extracted by
using stacked 1D dilated convolution layers, and periodic features are
extracted by dynamic graph convolution layers. For the difficulties in making
time-step level prediction, LSTTN adopts a short-term trend extractor to learn
fine-grained short-term temporal features. Finally, LSTTN fuses the long-term
trend, periodic features and short-term features to obtain the prediction
results. Experiments on four real-world datasets show that in 60-minute-ahead
long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\%
and a maximum improvement of 16.78\% over baseline models. The source code is
available at https://github.com/GeoX-Lab/LSTTN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Determined Multi-Label Learning via Similarity-Based <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Wei, Zhongnian Li, Peng Ying, Yong Zhou, Xinzheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-label classification, each training instance is associated with
multiple class labels simultaneously. Unfortunately, collecting the fully
precise class labels for each training instance is time- and labor-consuming
for real-world applications. To alleviate this problem, a novel labeling
setting termed \textit{Determined Multi-Label Learning} (DMLL) is proposed,
aiming to effectively alleviate the labeling cost inherent in multi-label
tasks. In this novel labeling setting, each training instance is associated
with a \textit{determined label} (either "Yes" or "No"), which indicates
whether the training instance contains the provided class label. The provided
class label is randomly and uniformly selected from the whole candidate labels
set. Besides, each training instance only need to be determined once, which
significantly reduce the annotation cost of the labeling task for multi-label
datasets. In this paper, we theoretically derive an risk-consistent estimator
to learn a multi-label classifier from these determined-labeled training data.
Additionally, we introduce a similarity-based prompt learning method for the
first time, which minimizes the risk-consistent loss of large-scale pre-trained
models to learn a supplemental prompt with richer semantic information.
Extensive experimental validation underscores the efficacy of our approach,
demonstrating superior performance compared to existing state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Reduced Labels for Long-Tailed Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Wei, Zhongnian Li, Yong Zhou, Xinzheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-tailed data is prevalent in real-world classification tasks and heavily
relies on supervised information, which makes the annotation process
exceptionally labor-intensive and time-consuming. Unfortunately, despite being
a common approach to mitigate labeling costs, existing weakly supervised
learning methods struggle to adequately preserve supervised information for
tail samples, resulting in a decline in accuracy for the tail classes. To
alleviate this problem, we introduce a novel weakly supervised labeling setting
called Reduced Label. The proposed labeling setting not only avoids the decline
of supervised information for the tail samples, but also decreases the labeling
costs associated with long-tailed data. Additionally, we propose an
straightforward and highly efficient unbiased framework with strong theoretical
guarantees to learn from these Reduced Labels. Extensive experiments conducted
on benchmark datasets including ImageNet validate the effectiveness of our
approach, surpassing the performance of state-of-the-art weakly supervised
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Generative Adversarial Network-Based Vocoder with Limited Data
  Using Augmentation-Conditional Discriminator <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generative adversarial network (GAN)-based vocoder trained with an
adversarial discriminator is commonly used for speech synthesis because of its
fast, lightweight, and high-quality characteristics. However, this data-driven
model requires a large amount of training data incurring high data-collection
costs. This fact motivates us to train a GAN-based vocoder on limited data. A
promising solution is to augment the training data to avoid overfitting.
However, a standard discriminator is unconditional and insensitive to
distributional changes caused by data augmentation. Thus, augmented speech
(which can be extraordinary) may be considered real speech. To address this
issue, we propose an augmentation-conditional discriminator (AugCondD) that
receives the augmentation state as input in addition to speech, thereby
assessing the input speech according to the augmentation state, without
inhibiting the learning of the original non-augmented distribution.
Experimental results indicate that AugCondD improves speech quality under
limited data conditions while achieving comparable speech quality under
sufficient data conditions. Audio samples are available at
https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedAC: A Adaptive Clustered Federated Learning Framework for
  Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustered federated learning (CFL) is proposed to mitigate the performance
deterioration stemming from data heterogeneity in federated learning (FL) by
grouping similar clients for cluster-wise model training. However, current CFL
methods struggle due to inadequate integration of global and intra-cluster
knowledge and the absence of an efficient online model similarity metric, while
treating the cluster count as a fixed hyperparameter limits flexibility and
robustness. In this paper, we propose an adaptive CFL framework, named FedAC,
which (1) efficiently integrates global knowledge into intra-cluster learning
by decoupling neural networks and utilizing distinct aggregation methods for
each submodule, significantly enhancing performance; (2) includes a
costeffective online model similarity metric based on dimensionality reduction;
(3) incorporates a cluster number fine-tuning module for improved adaptability
and scalability in complex, heterogeneous environments. Extensive experiments
show that FedAC achieves superior empirical performance, increasing the test
accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,
respectively, under different non-IID settings compared to SOTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the rates of convergence for learning with convolutional neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfei Yang, Han Feng, Ding-Xuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the approximation and learning capacities of convolutional neural
networks (CNNs). Our first result proves a new approximation bound for CNNs
with certain constraint on the weights. Our second result gives a new analysis
on the covering number of feed-forward neural networks, which include CNNs as
special cases. The analysis carefully takes into account the size of the
weights and hence gives better bounds than existing literature in some
situations. Using these two results, we are able to derive rates of convergence
for estimators based on CNNs in many learning problems. In particular, we
establish minimax optimal convergence rates of the least squares based on CNNs
for learning smooth functions in the nonparametric regression setting. For
binary classification, we derive convergence rates for CNN classifiers with
hinge loss and logistic loss. It is also shown that the obtained rates are
minimax optimal in several settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ If CLIP Could Talk: Understanding Vision-Language Model Representations
  Through Their Preferred Concept Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works often assume that Vision-Language Model (VLM) representations
are based on visual attributes like shape. However, it is unclear to what
extent VLMs prioritize this information to represent concepts. We propose
Extract and Explore (EX2), a novel approach to characterize important textual
features for VLMs. EX2 uses reinforcement learning to align a large language
model with VLM preferences and generates descriptions that incorporate the
important features for the VLM. Then, we inspect the descriptions to identify
the features that contribute to VLM representations. We find that spurious
descriptions have a major role in VLM representations despite providing no
helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,
among informative descriptions, VLMs rely significantly on non-visual
attributes like habitat to represent visual concepts. Also, our analysis
reveals that different VLMs prioritize different attributes in their
representations. Overall, we show that VLMs do not simply match images to scene
descriptions and that non-visual or even spurious descriptions significantly
influence their representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/BatsResearch/ex2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Producing and Leveraging Online Map Uncertainty in Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, Boris Ivanovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) maps have played an integral role in the development of
modern autonomous vehicle (AV) stacks, albeit with high associated labeling and
maintenance costs. As a result, many recent works have proposed methods for
estimating HD maps online from sensor data, enabling AVs to operate outside of
previously-mapped regions. However, current online map estimation approaches
are developed in isolation of their downstream tasks, complicating their
integration in AV stacks. In particular, they do not produce uncertainty or
confidence estimates. In this work, we extend multiple state-of-the-art online
map estimation methods to additionally estimate uncertainty and show how this
enables more tightly integrating online mapping with trajectory forecasting. In
doing so, we find that incorporating uncertainty yields up to 50% faster
training convergence and up to 15% better prediction performance on the
real-world nuScenes driving dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures, 6 tables. CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An incremental MaxSAT-based model to learn balanced rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antônio Carlos Souza Ferreira Júnior, Thiago Alves Rocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing advancements in the field of machine learning have led to the
development of numerous applications that effectively address a wide range of
problems with accurate predictions. However, in certain cases, accuracy alone
may not be sufficient. Many real-world problems also demand explanations and
interpretability behind the predictions. One of the most popular interpretable
models that are classification rules. This work aims to propose an incremental
model for learning interpretable and balanced rules based on MaxSAT, called
IMLIB. This new model was based on two other approaches, one based on SAT and
the other on MaxSAT. The one based on SAT limits the size of each generated
rule, making it possible to balance them. We suggest that such a set of rules
seem more natural to be understood compared to a mixture of large and small
rules. The approach based on MaxSAT, called IMLI, presents a technique to
increase performance that involves learning a set of rules by incrementally
applying the model in a dataset. Finally, IMLIB and IMLI are compared using
diverse databases. IMLIB obtained results comparable to IMLI in terms of
accuracy, generating more balanced rules with smaller sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 tables, submitted to BRACIS 2023 (Brazilian Conference on
  Intelligent Systems), accepted version published in Intelligent Systems,
  LNCS, vol 14195</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble Adversarial Defense via Integration of Multiple Dispersed Low
  Curvature Models <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaikang Zhao, Xi Chen, Wei Huang, Liuxin Ding, Xianglong Kong, Fan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of an ensemble of deep learning models has been extensively
explored to enhance defense against adversarial attacks. The diversity among
sub-models increases the attack cost required to deceive the majority of the
ensemble, thereby improving the adversarial robustness. While existing
approaches mainly center on increasing diversity in feature representations or
dispersion of first-order gradients with respect to input, the limited
correlation between these diversity metrics and adversarial robustness
constrains the performance of ensemble adversarial defense. In this work, we
aim to enhance ensemble diversity by reducing attack transferability. We
identify second-order gradients, which depict the loss curvature, as a key
factor in adversarial robustness. Computing the Hessian matrix involved in
second-order gradients is computationally expensive. To address this, we
approximate the Hessian-vector product using differential approximation. Given
that low curvature provides better robustness, our ensemble model was designed
to consider the influence of curvature among different sub-models. We introduce
a novel regularizer to train multiple more-diverse low-curvature network
models. Extensive experiments across various datasets demonstrate that our
ensemble model exhibits superior robustness against a range of attacks,
underscoring the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The 2024 International Joint Conference on Neural
  Networks (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking the Representation in Federated Unsupervised Learning with
  Non-IID Data <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Fengyuan Yu, Huabin Zhu, Binhui Yao, Tao Wang, Xiaolin Zheng, Yanchao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning achieves effective performance in modeling decentralized
data. In practice, client data are not well-labeled, which makes it potential
for federated unsupervised learning (FUSL) with non-IID data. However, the
performance of existing FUSL methods suffers from insufficient representations,
i.e., (1) representation collapse entanglement among local and global models,
and (2) inconsistent representation spaces among local models. The former
indicates that representation collapse in local model will subsequently impact
the global model and other local models. The latter means that clients model
data representation with inconsistent parameters due to the deficiency of
supervision signals. In this work, we propose FedU2 which enhances generating
uniform and unified representation in FUSL with non-IID data. Specifically,
FedU2 consists of flexible uniform regularizer (FUR) and efficient unified
aggregator (EUA). FUR in each client avoids representation collapse via
dispersing samples uniformly, and EUA in server promotes unified representation
by constraining consistent client model updating. To extensively validate the
performance of FedU2, we conduct both cross-device and cross-silo evaluation
experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concurrent Linguistic Error Detection (CLED) for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhua Zhu, Javier Conde, Zhen Gao, Pedro Reviriego, Shanshan Liu, Fabrizio Lombardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wide adoption of Large language models (LLMs) makes their dependability a
pressing concern. Detection of errors is the first step to mitigating their
impact on a system and thus, efficient error detection for LLMs is an important
issue. In many settings, the LLM is considered as a black box with no access to
the internal nodes; this prevents the use of many error detection schemes that
need access to the model's internal nodes. An interesting observation is that
the output of LLMs in error-free operation should be valid and normal text.
Therefore, when the text is not valid or differs significantly from normal
text, it is likely that there is an error. Based on this observation we propose
to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts
some linguistic features of the text generated by the LLM and feeds them to a
concurrent classifier that detects errors. Since the proposed error detection
mechanism only relies on the outputs of the model, then it can be used on LLMs
in which there is no access to the internal nodes. The proposed CLED scheme has
been evaluated on the T5 model when used for news summarization and on the
OPUS-MT model when used for translation. In both cases, the same set of
linguistic features has been used for error detection to illustrate the
applicability of the proposed scheme beyond a specific case. The results show
that CLED can detect most of the errors at a low overhead penalty. The use of
the concurrent classifier also enables a trade-off between error detection
effectiveness and its associated overhead, so providing flexibility to a
designer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 30 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed RL for Maximal Safety Probability Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hikaru Hoshino, Yorie Nakahira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate risk quantification and reachability analysis are crucial for safe
control and learning, but sampling from rare events, risky states, or long-term
trajectories can be prohibitively costly. Motivated by this, we study how to
estimate the long-term safety probability of maximally safe actions without
sufficient coverage of samples from risky states and long-term trajectories.
The use of maximal safety probability in control and learning is expected to
avoid conservative behaviors due to over-approximation of risk. Here, we first
show that long-term safety probability, which is multiplicative in time, can be
converted into additive costs and be solved using standard reinforcement
learning methods. We then derive this probability as solutions of partial
differential equations (PDEs) and propose Physics-Informed Reinforcement
Learning (PIRL) algorithm. The proposed method can learn using sparse rewards
because the physics constraints help propagate risk information through
neighbors. This suggests that, for the purpose of extracting more information
for efficient learning, physics constraints can serve as an alternative to
reward shaping. The proposed method can also estimate long-term risk using
short-term samples and deduce the risk of unsampled states. This feature is in
stark contrast with the unconstrained deep RL that demands sufficient data
coverage. These merits of the proposed method are demonstrated in numerical
simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Adaptation for Condition Monitoring Signal Prediction using
  Label-aware Neural Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyun Chung, Raed Al Kontar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building a predictive model that rapidly adapts to real-time condition
monitoring (CM) signals is critical for engineering systems/units.
Unfortunately, many current methods suffer from a trade-off between
representation power and agility in online settings. For instance, parametric
methods that assume an underlying functional form for CM signals facilitate
efficient online prediction updates. However, this simplification leads to
vulnerability to model specifications and an inability to capture complex
signals. On the other hand, approaches based on over-parameterized or
non-parametric models can excel at explaining complex nonlinear signals, but
real-time updates for such models pose a challenging task. In this paper, we
propose a neural process-based approach that addresses this trade-off. It
encodes available observations within a CM signal into a representation space
and then reconstructs the signal's history and evolution for prediction. Once
trained, the model can encode an arbitrary number of observations without
requiring retraining, enabling on-the-spot real-time predictions along with
quantified uncertainty and can be readily updated as more online data is
gathered. Furthermore, our model is designed to incorporate qualitative
information (i.e., labels) from individual units. This integration not only
enhances individualized predictions for each unit but also enables joint
inference for both signals and their associated labels. Numerical studies on
both synthetic and real-world data in reliability engineering highlight the
advantageous features of our model in real-time adaptation, enhanced signal
prediction with uncertainty quantification, and joint prediction for labels and
signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProIn: Learning to Predict Trajectory Based on Progressive Interactions
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinke Dong, Haifeng Yuan, Hongkun Liu, Wei Jing, Fangzhen Li, Hongmin Liu, Bin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate motion prediction of pedestrians, cyclists, and other surrounding
vehicles (all called agents) is very important for autonomous driving. Most
existing works capture map information through an one-stage interaction with
map by vector-based attention, to provide map constraints for social
interaction and multi-modal differentiation. However, these methods have to
encode all required map rules into the focal agent's feature, so as to retain
all possible intentions' paths while at the meantime to adapt to potential
social interaction. In this work, a progressive interaction network is proposed
to enable the agent's feature to progressively focus on relevant maps, in order
to better learn agents' feature representation capturing the relevant map
constraints. The network progressively encode the complex influence of map
constraints into the agent's feature through graph convolutions at the
following three stages: after historical trajectory encoder, after social
interaction, and after multi-modal differentiation. In addition, a weight
allocation mechanism is proposed for multi-modal training, so that each mode
can obtain learning opportunities from a single-mode ground truth. Experiments
have validated the superiority of progressive interactions to the existing
one-stage interaction, and demonstrate the effectiveness of each component.
Encouraging results were obtained in the challenging benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SignSGD with Federated Voting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanho Park, H. Vincent Poor, Namyoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed learning is commonly used for accelerating model training by
harnessing the computational capabilities of multiple-edge devices. However, in
practical applications, the communication delay emerges as a bottleneck due to
the substantial information exchange required between workers and a central
parameter server. SignSGD with majority voting (signSGD-MV) is an effective
distributed learning algorithm that can significantly reduce communication
costs by one-bit quantization. However, due to heterogeneous computational
capabilities, it fails to converge when the mini-batch sizes differ among
workers. To overcome this, we propose a novel signSGD optimizer with
\textit{federated voting} (signSGD-FV). The idea of federated voting is to
exploit learnable weights to perform weighted majority voting. The server
learns the weights assigned to the edge devices in an online fashion based on
their computational capabilities. Subsequently, these weights are employed to
decode the signs of the aggregated local gradients in such a way to minimize
the sign decoding error probability. We provide a unified convergence rate
analysis framework applicable to scenarios where the estimated weights are
known to the parameter server either perfectly or imperfectly. We demonstrate
that the proposed signSGD-FV algorithm has a theoretical convergence guarantee
even when edge devices use heterogeneous mini-batch sizes. Experimental results
show that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence
rate, especially in heterogeneous mini-batch sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Action-based Representations Using Invariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Rudolph, Caleb Chuck, Kevin Black, Misha Lvovsky, Scott Niekum, Amy Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust reinforcement learning agents using high-dimensional observations must
be able to identify relevant state features amidst many exogeneous distractors.
A representation that captures controllability identifies these state elements
by determining what affects agent control. While methods such as inverse
dynamics and mutual information capture controllability for a limited number of
timesteps, capturing long-horizon elements remains a challenging problem.
Myopic controllability can capture the moment right before an agent crashes
into a wall, but not the control-relevance of the wall while the agent is still
some distance away. To address this we introduce action-bisimulation encoding,
a method inspired by the bisimulation invariance pseudometric, that extends
single-step controllability with a recursive invariance constraint. By doing
this, action-bisimulation learns a multi-step controllability metric that
smoothly discounts distant state features that are relevant for control. We
demonstrate that action-bisimulation pretraining on reward-free, uniformly
random data improves sample efficiency in several environments, including a
photorealistic 3D simulation domain, Habitat. Additionally, we provide
theoretical analysis and qualitative results demonstrating the information
captured by action-bisimulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Potent Poisons and Backdoors from Scratch with Guided
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural networks are often trained on massive datasets that are web
scraped with minimal human inspection. As a result of this insecure curation
pipeline, an adversary can poison or backdoor the resulting model by uploading
malicious data to the internet and waiting for a victim to scrape and train on
it. Existing approaches for creating poisons and backdoors start with randomly
sampled clean data, called base samples, and then modify those samples to craft
poisons. However, some base samples may be significantly more amenable to
poisoning than others. As a result, we may be able to craft more potent poisons
by carefully choosing the base samples. In this work, we use guided diffusion
to synthesize base samples from scratch that lead to significantly more potent
poisons and backdoors than previous state-of-the-art attacks. Our Guided
Diffusion Poisoning (GDP) base samples can be combined with any downstream
poisoning or backdoor attack to boost its effectiveness. Our implementation
code is publicly available at: https://github.com/hsouri/GDP .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChatDBG: An AI-Powered Debugging Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyla Levin, Nicolas van Kempen, Emery D. Berger, Stephen N. Freund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ChatDBG, the first AI-powered debugging assistant.
ChatDBG integrates large language models (LLMs) to significantly enhance the
capabilities and user-friendliness of conventional debuggers. ChatDBG lets
programmers engage in a collaborative dialogue with the debugger, allowing them
to pose complex questions about program state, perform root cause analysis for
crashes or assertion failures, and explore open-ended queries like "why is x
null?". To handle these queries, ChatDBG grants the LLM autonomy to take the
wheel and drive debugging by issuing commands to navigate through stacks and
inspect program state; it then reports its findings and yields back control to
the programmer. Our ChatDBG prototype integrates with standard debuggers
including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our
evaluation across a diverse set of code, including C/C++ code with known bugs
and a suite of Python code including standalone scripts and Jupyter notebooks,
demonstrates that ChatDBG can successfully analyze root causes, explain bugs,
and generate accurate fixes for a wide range of real-world errors. For the
Python programs, a single query led to an actionable bug fix 67% of the time;
one additional follow-up query increased the success rate to 85%. ChatDBG has
seen rapid uptake; it has already been downloaded nearly 30,000 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat<span class="highlight-title">GPT</span> Incorrectness Detection in Software <span class="highlight-title">Review</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minaoar Hossain Tanzil, Junaed Younus Khan, Gias Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conducted a survey of 135 software engineering (SE) practitioners to
understand how they use Generative AI-based chatbots like ChatGPT for SE tasks.
We find that they want to use ChatGPT for SE tasks like software library
selection but often worry about the truthfulness of ChatGPT responses. We
developed a suite of techniques and a tool called CID (ChatGPT Incorrectness
Detector) to automatically test and detect the incorrectness in ChatGPT
responses. CID is based on the iterative prompting to ChatGPT by asking it
contextually similar but textually divergent questions (using an approach that
utilizes metamorphic relationships in texts). The underlying principle in CID
is that for a given question, a response that is different from other responses
(across multiple incarnations of the question) is likely an incorrect response.
In a benchmark study of library selection, we show that CID can detect
incorrect responses from ChatGPT with an F1-score of 0.74 - 0.75.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Inference in Multi-environment Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John C. Duchi, Suyash Gupta, Kuanhao Jiang, Pragya Sur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of constructing valid confidence intervals and sets
in problems of prediction across multiple environments. We investigate two
types of coverage suitable for these problems, extending the jackknife and
split-conformal methods to show how to obtain distribution-free coverage in
such non-traditional, hierarchical data-generating scenarios. Our contributions
also include extensions for settings with non-real-valued responses and a
theory of consistency for predictive inference in these general problems. We
demonstrate a novel resizing method to adapt to problem difficulty, which
applies both to existing approaches for predictive inference with hierarchical
data and the methods we develop; this reduces prediction set sizes using
limited information from the test environment, a key to the methods' practical
performance, which we evaluate through neurochemical sensing and species
classification datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEDDAP: Medical <span class="highlight-title">Dataset</span> Enhancement via Diversified Augmentation
  Pipeline <span class="chip">MICCAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasamin Medghalchi, Niloufar Zakariaei, Arman Rahmim, Ilker Hacihaliloglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the
abundance and accuracy of available training data. However, collecting and
annotating data on a large scale is often both costly and time-intensive,
particularly in medical cases where practitioners are already occupied with
their duties. Moreover, ensuring that the model remains robust across various
scenarios of image capture is crucial in medical domains, especially when
dealing with ultrasound images that vary based on the settings of different
devices and the manual operation of the transducer. To address this challenge,
we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion
(SD) models to augment existing small datasets by automatically generating new
informative labeled samples. Pretrained checkpoints for SD are typically based
on natural images, and training them for medical images requires significant
GPU resources due to their heavy parameters. To overcome this challenge, we
introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method
tailored specifically for ultrasound applications. USLoRA allows for selective
fine-tuning of weights within SD, requiring fewer than 0.1\% of parameters
compared to fully fine-tuning only the UNet portion of SD. To enhance dataset
diversity, we incorporate different adjectives into the generation process
prompts, thereby desensitizing the classifiers to intensity changes across
different images. This approach is inspired by clinicians' decision-making
processes regarding breast tumors, where tumor shape often plays a more crucial
role than intensity. In conclusion, our pipeline not only outperforms
classifiers trained on the original dataset but also demonstrates superior
performance when encountering unseen datasets. The source code is available at
https://github.com/yasamin-med/MEDDAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to miccai 2024 submitted to miccai 2024 Submitted to
  MICCAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graphs Generalization under Distribution Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Tian, Wenjun Wang, Chen Zhao, Minglai Shao, Wang Zhang, Dong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional machine learning methods heavily rely on the independent and
identically distribution assumption, which imposes limitations when the test
distribution deviates from the training distribution. To address this crucial
issue, out-of-distribution (OOD) generalization, which aims to achieve
satisfactory generalization performance when faced with unknown distribution
shifts, has made a significant process. However, the OOD method for
graph-structured data currently lacks clarity and remains relatively unexplored
due to two primary challenges. Firstly, distribution shifts on graphs often
occur simultaneously on node attributes and graph topology. Secondly, capturing
invariant information amidst diverse distribution shifts proves to be a
formidable challenge. To overcome these obstacles, in this paper, we introduce
a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER).
The goal is to (1) diversify variations across domains by modeling the
potential seen or unseen variations of attribute distribution and topological
structure and (2) minimize the discrepancy of the variation in a representation
space where the target is to predict semantic labels. Extensive experiment
results indicate that our model outperforms baseline methods on node-level OOD
generalization across domains in distribution shift on node features and
topological structures simultaneously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Second Look on BASS -- Boosting Abstractive Summarization with Unified
  Semantic Graphs -- A Replication Study <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osman Alperen Koraş, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a detailed replication study of the BASS framework, an abstractive
summarization system based on the notion of Unified Semantic Graphs. Our
investigation includes challenges in replicating key components and an ablation
study to systematically isolate error sources rooted in replicating novel
components. Our findings reveal discrepancies in performance compared to the
original work. We highlight the significance of paying careful attention even
to reasonably omitted details for replicating advanced frameworks like BASS,
and emphasize key practices for writing replicable papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Advances in Information Retrieval, 46th European Conference on
  Information Retrieval, ECIR 2024. 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Analysis of Linear Time Series Forecasting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Toner, Luke Darlow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their simplicity, linear models perform well at time series
forecasting, even when pitted against deeper and more expensive models. A
number of variations to the linear model have been proposed, often including
some form of feature normalisation that improves model generalisation. In this
paper we analyse the sets of functions expressible using these linear model
architectures. In so doing we show that several popular variants of linear
models for time series forecasting are equivalent and functionally
indistinguishable from standard, unconstrained linear regression. We
characterise the model classes for each linear variant. We demonstrate that
each model can be reinterpreted as unconstrained linear regression over a
suitably augmented feature set, and therefore admit closed-form solutions when
using a mean-squared loss function. We provide experimental evidence that the
models under inspection learn nearly identical solutions, and finally
demonstrate that the simpler closed form solutions are superior forecasters
across 72% of test settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatio-Temporal Few-Shot Learning via Diffusive Neural Network
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal modeling is foundational for smart city applications, yet it
is often hindered by data scarcity in many cities and regions. To bridge this
gap, we propose a novel generative pre-training framework, GPD, for
spatio-temporal few-shot learning with urban knowledge transfer. Unlike
conventional approaches that heavily rely on common feature extraction or
intricate few-shot learning designs, our solution takes a novel approach by
performing generative pre-training on a collection of neural network parameters
optimized with data from source cities. We recast spatio-temporal few-shot
learning as pre-training a generative diffusion model, which generates tailored
neural networks guided by prompts, allowing for adaptability to diverse data
distributions and city-specific characteristics. GPD employs a
Transformer-based denoising diffusion model, which is model-agnostic to
integrate with powerful spatio-temporal neural networks. By addressing
challenges arising from data gaps and the complexity of generalizing knowledge
across cities, our framework consistently outperforms state-of-the-art
baselines on multiple real-world datasets for tasks such as traffic speed
prediction and crowd flow prediction. The implementation of our approach is
available: https://github.com/tsinghua-fib-lab/GPD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the forecast accuracy of wind power by leveraging multiple
  hierarchical structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas English, Mahdi Abolghasemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Renewable energy generation is of utmost importance for global
decarbonization. Forecasting renewable energies, particularly wind energy, is
challenging due to the inherent uncertainty in wind energy generation, which
depends on weather conditions. Recent advances in hierarchical forecasting
through reconciliation have demonstrated a significant increase in the quality
of wind energy forecasts for short-term periods. We leverage the
cross-sectional and temporal hierarchical structure of turbines in wind farms
and build cross-temporal hierarchies to further investigate how integrated
cross-sectional and temporal dimensions can add value to forecast accuracy in
wind farms. We found that cross-temporal reconciliation was superior to
individual cross-sectional reconciliation at multiple temporal aggregations.
Additionally, machine learning based forecasts that were cross-temporally
reconciled demonstrated high accuracy at coarser temporal granularities, which
may encourage adoption for short-term wind forecasts. Empirically, we provide
insights for decision-makers on the best methods for forecasting high-frequency
wind data across different forecasting horizons and levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightIt: Illumination Modeling and Control for Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, Yannick Hold-Geoffroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LightIt, a method for explicit illumination control for image
generation. Recent generative methods lack lighting control, which is crucial
to numerous artistic aspects of image generation such as setting the overall
mood or cinematic appearance. To overcome these limitations, we propose to
condition the generation on shading and normal maps. We model the lighting with
single bounce shading, which includes cast shadows. We first train a shading
estimation module to generate a dataset of real-world images and shading pairs.
Then, we train a control network using the estimated shading and normals as
input. Our method demonstrates high-quality image generation and lighting
control in numerous scenes. Additionally, we use our generated dataset to train
an identity-preserving relighting model, conditioned on an image and a target
shading. Our method is the first that enables the generation of images with
controllable, consistent lighting and performs on par with specialized
relighting state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://peter-kocsis.github.io/LightIt/ Video:
  https://youtu.be/cCfSBD5aPLI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BatteryML:An Open-source platform for Machine Learning on Battery
  Degradation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14714v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14714v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Xiaofan Gui, Shun Zheng, Ziheng Lu, Yuqi Li, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Battery degradation remains a pivotal concern in the energy storage domain,
with machine learning emerging as a potent tool to drive forward insights and
solutions. However, this intersection of electrochemical science and machine
learning poses complex challenges. Machine learning experts often grapple with
the intricacies of battery science, while battery researchers face hurdles in
adapting intricate models tailored to specific datasets. Beyond this, a
cohesive standard for battery degradation modeling, inclusive of data formats
and evaluative benchmarks, is conspicuously absent. Recognizing these
impediments, we present BatteryML - a one-step, all-encompass, and open-source
platform designed to unify data preprocessing, feature extraction, and the
implementation of both traditional and state-of-the-art models. This
streamlined approach promises to enhance the practicality and efficiency of
research applications. BatteryML seeks to fill this void, fostering an
environment where experts from diverse specializations can collaboratively
contribute, thus elevating the collective understanding and advancement of
battery research.The code for our project is publicly available on GitHub at
https://github.com/microsoft/BatteryML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Question Answering with Reinforcement Learning <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Blübaum, Stefan Heindorf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal questions inquire about causal relationships between different events
or phenomena. They are important for a variety of use cases, including virtual
assistants and search engines. However, many current approaches to causal
question answering cannot provide explanations or evidence for their answers.
Hence, in this paper, we aim to answer causal questions with a causality graph,
a large-scale dataset of causal relations between noun phrases along with the
relations' provenance data. Inspired by recent, successful applications of
reinforcement learning to knowledge graph tasks, such as link prediction and
fact-checking, we explore the application of reinforcement learning on a
causality graph for causal question answering. We introduce an
Actor-Critic-based agent which learns to search through the graph to answer
causal questions. We bootstrap the agent with a supervised learning procedure
to deal with large action spaces and sparse rewards. Our evaluation shows that
the agent successfully prunes the search space to answer binary causal
questions by visiting less than 30 nodes per question compared to over 3,000
nodes by a naive breadth-first search. Our ablation study indicates that our
supervised learning strategy provides a strong foundation upon which our
reinforcement learning agent improves. The paths returned by our agent explain
the mechanisms by which a cause produces an effect. Moreover, for each edge on
a path, our causality graph provides its original source allowing for easy
verification of paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Adversarial Capabilities of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09132v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09132v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of large language models (LLMs) has sparked widespread and
general interest due to their strong language generation capabilities, offering
great potential for both industry and research. While previous research delved
into the security and privacy issues of LLMs, the extent to which these models
can exhibit adversarial behavior remains largely unexplored. Addressing this
gap, we investigate whether common publicly available LLMs have inherent
capabilities to perturb text samples to fool safety measures, so-called
adversarial examples resp.~attacks. More specifically, we investigate whether
LLMs are inherently able to craft adversarial examples out of benign samples to
fool existing safe rails. Our experiments, which focus on hate speech
detection, reveal that LLMs succeed in finding adversarial perturbations,
effectively undermining hate speech detection systems. Our findings carry
significant implications for (semi-)autonomous systems relying on LLMs,
highlighting potential challenges in their interaction with existing systems
and safety measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the resilience of Collaborative Learning-based Recommender Systems
  Against Community Detection Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yacine Belal, Sonia Ben Mokhtar, Mohamed Maouche, Anthony Simonet-Boulogne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative-learning-based recommender systems emerged following the
success of collaborative learning techniques such as Federated Learning (FL)
and Gossip Learning (GL). In these systems, users participate in the training
of a recommender system while maintaining their history of consumed items on
their devices. While these solutions seemed appealing for preserving the
privacy of the participants at first glance, recent studies have revealed that
collaborative learning can be vulnerable to various privacy attacks. In this
paper, we study the resilience of collaborative learning-based recommender
systems against a novel privacy attack called Community Detection Attack (CDA).
This attack enables an adversary to identify community members based on a
chosen set of items (eg., identifying users interested in specific
points-of-interest). Through experiments on three real recommendation datasets
using two state-of-the-art recommendation models, we evaluate the sensitivity
of an FL-based recommender system as well as two flavors of Gossip
Learning-based recommender systems to CDA. The results show that across all
models and datasets, the FL setting is more vulnerable to CDA compared to
Gossip settings. Furthermore, we assess two off-the-shelf mitigation
strategies, namely differential privacy (DP) and a \emph{Share less} policy,
which consists of sharing a subset of less sensitive model parameters. The
findings indicate a more favorable privacy-utility trade-off for the
\emph{Share less} strategy, particularly in FedRecs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-agent reinforcement learning using echo-state network and its
  application to pedestrian dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisato Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, simulations of pedestrians using the multi-agent
reinforcement learning (MARL) have been studied. This study considered the
roads on a grid-world environment, and implemented pedestrians as MARL agents
using an echo-state network and the least squares policy iteration method.
Under this environment, the ability of these agents to learn to move forward by
avoiding other agents was investigated. Specifically, we considered two types
of tasks: the choice between a narrow direct route and a broad detour, and the
bidirectional pedestrian flow in a corridor. The simulations results indicated
that the learning was successful when the density of the agents was not that
high.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally Generative Augmentation for Fair Facial Attribute
  Classification <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu, Jun Xiao, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Attribute Classification (FAC) holds substantial promise in widespread
applications. However, FAC models trained by traditional methodologies can be
unfair by exhibiting accuracy inconsistencies across varied data
subpopulations. This unfairness is largely attributed to bias in data, where
some spurious attributes (e.g., Male) statistically correlate with the target
attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the
labels of spurious attributes, which may be unavailable in practice. This work
proposes a novel, generation-based two-stage framework to train a fair FAC
model on biased data without additional annotation. Initially, we identify the
potential spurious attributes based on generative models. Notably, it enhances
interpretability by explicitly showing the spurious attributes in image space.
Following this, for each image, we first edit the spurious attributes with a
random degree sampled from a uniform distribution, while keeping target
attribute unchanged. Then we train a fair FAC model by fostering model
invariance to these augmentation. Extensive experiments on three common
datasets demonstrate the effectiveness of our method in promoting fairness in
FAC without compromising accuracy. Codes are in
https://github.com/heqianpei/DiGA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference as Reward, Maximum Preference Optimization with Importance
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16430v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16430v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaifan Jiang, Xing Huang, Chao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a
model-based algorithm to optimize preference learning, which first fits a
reward model for preference scores and then optimizes the generating policy
with an on-policy PPO algorithm to maximize the reward. The processing of RLHF
is complex, time-consuming, and unstable. The Direct Preference Optimization
(DPO) algorithm uses an off-policy algorithm to directly optimize the
generating policy and eliminates the need for a reward model. DPO is more
data-efficient and stable. However, DPO has a drawback of overfitting to the
preference data and ignoring the KL-regularization term when the preference is
deterministic. Identity mapping Preference Optimization(IPO) uses a
root-finding MSE loss to incorporate KL-regularization. However, both DPO and
IPO fail to properly address the KL-regularization term because the support of
the preference distribution is not equal to the reference distribution. In this
paper, we propose a simple and intuitive off-policy preference optimization
algorithm from an importance sampling view, which we call Maximum Preference
Optimization (MPO). MPO incorporates the off-policy KL-regularization term,
making regularization truly effective. MPO achieves the best of both worlds by
combining the objectives of RLHF and IPO while being an off-policy algorithm.
Furthermore, MPO eliminates the need for a reward model and reference policy,
simplifying the learning process and reducing memory usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data driven modeling for self-similar dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08282v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08282v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyi Tao, Ningning Tao, Yi-zhuang You, Jiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiscale modeling of complex systems is crucial for understanding their
intricacies. Data-driven multiscale modeling has emerged as a promising
approach to tackle challenges associated with complex systems. On the other
hand, self-similarity is prevalent in complex systems, hinting that large-scale
complex systems can be modeled at a reduced cost. In this paper, we introduce a
multiscale neural network framework that incorporates self-similarity as prior
knowledge, facilitating the modeling of self-similar dynamical systems. For
deterministic dynamics, our framework can discern whether the dynamics are
self-similar. For uncertain dynamics, it can compare and determine which
parameter set is closer to self-similarity. The framework allows us to extract
scale-invariant kernels from the dynamics for modeling at any scale. Moreover,
our method can identify the power law exponents in self-similar systems.
Preliminary tests on the Ising model yielded critical exponents consistent with
theoretical expectations, providing valuable insights for addressing critical
phase transitions in non-equilibrium systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,7 figures,1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-PHYRE: Interactive Physical Reasoning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqian Li, Kewen Wu, Chi Zhang, Yixin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents' abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents' learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral methods for Neural Integral Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05654v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05654v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Zappala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural integral equations are deep learning models based on the theory of
integral equations, where the model consists of an integral operator and the
corresponding equation (of the second kind) which is learned through an
optimization procedure. This approach allows to leverage the nonlocal
properties of integral operators in machine learning, but it is computationally
expensive. In this article, we introduce a framework for neural integral
equations based on spectral methods that allows us to learn an operator in the
spectral domain, resulting in a cheaper computational cost, as well as in high
interpolation accuracy. We study the properties of our methods and show various
theoretical guarantees regarding the approximation capabilities of the model,
and convergence to solutions of the numerical methods. We provide numerical
experiments to demonstrate the practical effectiveness of the resulting model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures and 2 tables. v3: Missing hypotheses for the
  framework have been now added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Developing and Deploying Industry Standards for Artificial Intelligence
  in Education (AIED): Challenges, Strategies, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Tong, Haoyang Li, Joleen Liang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of Artificial Intelligence in Education (AIED) holds the promise
of revolutionizing educational practices by offering personalized learning
experiences, automating administrative and pedagogical tasks, and reducing the
cost of content creation. However, the lack of standardized practices in the
development and deployment of AIED solutions has led to fragmented ecosystems,
which presents challenges in interoperability, scalability, and ethical
governance. This article aims to address the critical need to develop and
implement industry standards in AIED, offering a comprehensive analysis of the
current landscape, challenges, and strategic approaches to overcome these
obstacles. We begin by examining the various applications of AIED in various
educational settings and identify key areas lacking in standardization,
including system interoperability, ontology mapping, data integration,
evaluation, and ethical governance. Then, we propose a multi-tiered framework
for establishing robust industry standards for AIED. In addition, we discuss
methodologies for the iterative development and deployment of standards,
incorporating feedback loops from real-world applications to refine and adapt
standards over time. The paper also highlights the role of emerging
technologies and pedagogical theories in shaping future standards for AIED.
Finally, we outline a strategic roadmap for stakeholders to implement these
standards, fostering a cohesive and ethical AIED ecosystem. By establishing
comprehensive industry standards, such as those by IEEE Artificial Intelligence
Standards Committee (AISC) and International Organization for Standardization
(ISO), we can accelerate and scale AIED solutions to improve educational
outcomes, ensuring that technological advances align with the principles of
inclusivity, fairness, and educational excellence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEA: Sparse Linear Attention with Estimated Attention Mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer architecture has driven breakthroughs in recent years on
tasks which require modeling pairwise relationships between sequential
elements, as is the case in natural language understanding. However, long
seqeuences pose a problem due to the quadratic complexity of the attention
operation. Previous research has aimed to lower the complexity by sparsifying
or linearly approximating the attention matrix. Yet, these approaches cannot
straightforwardly distill knowledge from a teacher's attention matrix and often
require complete retraining from scratch. Furthermore, previous sparse and
linear approaches lose interpretability if they cannot produce full attention
matrices. To address these challenges, we propose SEA: Sparse linear attention
with an Estimated Attention mask. SEA estimates the attention matrix with
linear complexity via kernel-based linear attention, then subsequently creates
a sparse attention matrix with a top-k selection to perform a sparse attention
operation. For language modeling tasks (Wikitext2), previous linear and sparse
attention methods show roughly two-fold worse perplexity scores over the
quadratic OPT-1.3B baseline, while SEA achieves better perplexity than
OPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable
attention matrix. We believe that our work will have a large practical impact,
as it opens the possibility of running large transformers on resource-limited
devices with less memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial
  Network <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Shibuya, Yuhta Takida, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative adversarial network (GAN)-based vocoders have been intensively
studied because they can synthesize high-fidelity audio waveforms faster than
real-time. However, it has been reported that most GANs fail to obtain the
optimal projection for discriminating between real and fake data in the feature
space. In the literature, it has been demonstrated that slicing adversarial
network (SAN), an improved GAN training framework that can find the optimal
projection, is effective in the image generation task. In this paper, we
investigate the effectiveness of SAN in the vocoding task. For this purpose, we
propose a scheme to modify least-squares GAN, which most GAN-based vocoders
adopt, so that their loss functions satisfy the requirements of SAN. Through
our experiments, we demonstrate that SAN can improve the performance of
GAN-based vocoders, including BigVGAN, with small modifications. Our code is
available at https://github.com/sony/bigvsan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024. Equation (5) in the previous version is
  wrong. We modified it</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Transfer Attack to Image Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermark has been widely deployed by industry to detect AI-generated images.
The robustness of such watermark-based detector against evasion attacks in the
white-box and black-box settings is well understood in the literature. However,
the robustness in the no-box setting is much less understood. In particular,
multiple studies claimed that image watermark is robust in such setting. In
this work, we propose a new transfer evasion attack to image watermark in the
no-box setting. Our transfer attack adds a perturbation to a watermarked image
to evade multiple surrogate watermarking models trained by the attacker itself,
and the perturbed watermarked image also evades the target watermarking model.
Our major contribution is to show that, both theoretically and empirically,
watermark-based AI-generated image detector is not robust to evasion attacks
even if the attacker does not have access to the watermarking model nor the
detection API.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Judge by the Look: Towards Motion Coherent Video Representation <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current training pipelines in object recognition neglect Hue Jittering when
doing data augmentation as it not only brings appearance changes that are
detrimental to classification, but also the implementation is inefficient in
practice. In this study, we investigate the effect of hue variance in the
context of video understanding and find this variance to be beneficial since
static appearances are less important in videos that contain motion
information. Based on this observation, we propose a data augmentation method
for video understanding, named Motion Coherent Augmentation (MCA), that
introduces appearance variation in videos and implicitly encourages the model
to prioritize motion patterns, rather than static appearances. Concretely, we
propose an operation SwapMix to efficiently modify the appearance of video
samples, and introduce Variation Alignment (VA) to resolve the distribution
shift caused by SwapMix, enforcing the model to learn appearance invariant
representations. Comprehensive empirical evaluation across various
architectures and different datasets solidly validates the effectiveness and
generalization ability of MCA, and the application of VA in other augmentation
methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVOTER: Evolution of Transparent Explainable Rule-sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10438v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10438v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hormoz Shahrzad, Babak Hodjat, Risto Miikkulainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most AI systems are black boxes generating reasonable outputs for given
inputs. Some domains, however, have explainability and trustworthiness
requirements that cannot be directly met by these approaches. Various methods
have therefore been developed to interpret black-box models after training.
This paper advocates an alternative approach where the models are transparent
and explainable to begin with. This approach, EVOTER, evolves rule-sets based
on simple logical expressions. The approach is evaluated in several
prediction/classification and prescription/policy search domains with and
without a surrogate. It is shown to discover meaningful rule sets that perform
similarly to black-box models. The rules can provide insight into the domain,
and make biases hidden in the data explicit. It may also be possible to edit
them directly to remove biases and add constraints. EVOTER thus forms a
promising foundation for building trustworthy AI systems for real-world
applications in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Text-to-Image (TTI) models have become a common approach for
generating training data in various generative fields. However, visual
hallucinations, which contain perceptually critical defects, remain a concern,
especially in non-photorealistic styles like cartoon characters. We propose a
novel visual hallucination detection system for cartoon character images
generated by TTI models. Our approach leverages pose-aware in-context visual
learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB
images and pose information. By incorporating pose guidance from a fine-tuned
pose estimator, we enable VLMs to make more accurate decisions. Experimental
results demonstrate significant improvements in identifying visual
hallucinations compared to baseline methods relying solely on RGB images. This
research advances TTI models by mitigating visual hallucinations, expanding
their potential in non-photorealistic domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures, 1 table, Project page:
  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Learning on Graphs: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimeng Guo, Teng Xiao, Zongyu Wu, Charu Aggarwal, Hui Liu, Suhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-structured data are pervasive in the real-world such as social
networks, molecular graphs and transaction networks. Graph neural networks
(GNNs) have achieved great success in representation learning on graphs,
facilitating various downstream tasks. However, GNNs have several drawbacks
such as lacking interpretability, can easily inherit the bias of data and
cannot model casual relations. Recently, counterfactual learning on graphs has
shown promising results in alleviating these drawbacks. Various approaches have
been proposed for counterfactual fairness, explainability, link prediction and
other applications on graphs. To facilitate the development of this promising
direction, in this survey, we categorize and comprehensively review papers on
graph counterfactual learning. We divide existing methods into four categories
based on problems studied. For each category, we provide background and
motivating examples, a general framework summarizing existing works and a
detailed review of these works. We point out promising future research
directions at the intersection of graph-structured data, counterfactual
learning, and real-world applications. To offer a comprehensive view of
resources for future studies, we compile a collection of open-source
implementations, public datasets, and commonly-used evaluation metrics. This
survey aims to serve as a ``one-stop-shop'' for building a unified
understanding of graph counterfactual learning categories and current
resources. We also maintain a repository for papers and resources and will keep
updating the repository
https://github.com/TimeLovercc/Awesome-Graph-Causal-Learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universality of almost periodicity in bounded discrete time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00290v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00290v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chikara Nakayama, Tsuyoshi Yoneda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider arbitrary bounded discrete time series. From its statistical
feature, without any use of the Fourier transform, we find an almost periodic
function which suitably characterizes the corresponding time series.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David D. Nguyen, David Leibowitz, Surya Nepal, Salil S. Kanhere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models with discrete latent representations have recently
demonstrated an impressive ability to learn complex high-dimensional data
distributions. However, their performance relies on a long sequence of tokens
per instance and a large number of codebook entries, resulting in long sampling
times and considerable computation to fit the categorical posterior. To address
these issues, we propose the Masked Vector Quantization (MVQ) framework which
increases the representational capacity of each code vector by learning mask
configurations via a stochastic winner-takes-all training regime called
Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\times$64, MVQ reduces
FID in existing vector quantization architectures by up to $68\%$ at 2 tokens
per instance and $57\%$ at 5 tokens. These improvements widen as codebook
entries is reduced and allows for $7\textit{--}45\times$ speed-up in token
sampling during inference. As an additional benefit, we find that smaller
latent spaces lead to MVQ identifying transferable visual representations where
multiple can be smoothly combined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A newer version of this manuscript was archived under 2312.11735</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> PhyloGFN: Phylogenetic inference with generative flow networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, <span class="highlight-author">Yoshua Bengio</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phylogenetics is a branch of computational biology that studies the
evolutionary relationships among biological entities. Its long history and
numerous applications notwithstanding, inference of phylogenetic trees from
sequence data remains challenging: the high complexity of tree space poses a
significant obstacle for the current combinatorial and probabilistic
techniques. In this paper, we adopt the framework of generative flow networks
(GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and
Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling
complex combinatorial structures, they are a natural choice for exploring and
sampling from the multimodal posterior distribution over tree topologies and
evolutionary distances. We demonstrate that our amortized posterior sampler,
PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real
benchmark datasets. PhyloGFN is competitive with prior works in marginal
likelihood estimation and achieves a closer fit to the target distribution than
state-of-the-art variational inference methods. Our code is available at
https://github.com/zmy1116/phylogfn.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIP: Temporal Residual Learning with Image Noise Prior for
  Image-to-Video Diffusion Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video generation have demonstrated the utility of
powerful diffusion models. Nevertheless, the problem is not trivial when
shaping diffusion models to animate static image (i.e., image-to-video
generation). The difficulty originates from the aspect that the diffusion
process of subsequent animated frames should not only preserve the faithful
alignment with the given image but also pursue temporal coherence among
adjacent frames. To alleviate this, we present TRIP, a new recipe of
image-to-video diffusion paradigm that pivots on image noise prior derived from
static image to jointly trigger inter-frame relational reasoning and ease the
coherent temporal modeling via temporal residual learning. Technically, the
image noise prior is first attained through one-step backward diffusion process
based on both static image and noised video latent codes. Next, TRIP executes a
residual-like dual-path scheme for noise prediction: 1) a shortcut path that
directly takes image noise prior as the reference noise of each frame to
amplify the alignment between the first frame and subsequent frames; 2) a
residual path that employs 3D-UNet over noised video and static image latent
codes to enable inter-frame relational reasoning, thereby easing the learning
of the residual noise for each frame. Furthermore, both reference and residual
noise of each frame are dynamically merged via attention mechanism for final
video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT
datasets demonstrate the effectiveness of our TRIP for image-to-video
generation. Please see our project page at https://trip-i2v.github.io/TRIP/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; Project page: https://trip-i2v.github.io/TRIP/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SD-DiT: Unleashing the Power of <span class="highlight-title">Self-supervised</span> Discrimination in
  Diffusion <span class="highlight-title">Transformer</span> <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, Chang Wen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformer (DiT) has emerged as the new trend of generative
diffusion models on image generation. In view of extremely slow convergence in
typical DiT, recent breakthroughs have been driven by mask strategy that
significantly improves the training efficiency of DiT with additional
intra-image contextual learning. Despite this progress, mask strategy still
suffers from two inherent limitations: (a) training-inference discrepancy and
(b) fuzzy relations between mask reconstruction & generative diffusion process,
resulting in sub-optimal training of DiT. In this work, we address these
limitations by novelly unleashing the self-supervised discrimination knowledge
to boost DiT training. Technically, we frame our DiT in a teacher-student
manner. The teacher-student discriminative pairs are built on the diffusion
noises along the same Probability Flow Ordinary Differential Equation (PF-ODE).
Instead of applying mask reconstruction loss over both DiT encoder and decoder,
we decouple DiT encoder and decoder to separately tackle discriminative and
generative objectives. In particular, by encoding discriminative pairs with
student and teacher DiT encoders, a new discriminative loss is designed to
encourage the inter-image alignment in the self-supervised embedding space.
After that, student samples are fed into student DiT decoder to perform the
typical generative diffusion task. Extensive experiments are conducted on
ImageNet dataset, and our method achieves a competitive balance between
training cost and generative capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VP3D: Unleashing 2D Visual <span class="highlight-title">Prompt</span> for Text-to-3D Generation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent innovations on text-to-3D generation have featured Score Distillation
Sampling (SDS), which enables the zero-shot learning of implicit 3D models
(NeRF) by directly distilling prior knowledge from 2D diffusion models.
However, current SDS-based models still struggle with intricate text prompts
and commonly result in distorted 3D models with unrealistic textures or
cross-view inconsistency issues. In this work, we introduce a novel Visual
Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the
visual appearance knowledge in 2D visual prompt to boost text-to-3D generation.
Instead of solely supervising SDS with text prompt, VP3D first capitalizes on
2D diffusion model to generate a high-quality image from input text, which
subsequently acts as visual prompt to strengthen SDS optimization with explicit
visual appearance. Meanwhile, we couple the SDS optimization with additional
differentiable reward function that encourages rendering images of 3D models to
better visually align with 2D visual prompt and semantically match with text
prompt. Through extensive experiments, we show that the 2D Visual Prompt in our
VP3D significantly eases the learning of visual appearance of 3D models and
thus leads to higher visual fidelity with more detailed textures. It is also
appealing in view that when replacing the self-generating visual prompt with a
given reference image, VP3D is able to trigger a new task of stylized
text-to-3D generation. Our project page is available at
https://vp3d-cvpr24.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; Project page: https://vp3d-cvpr24.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Spatial Adaptation and Temporal Coherence in Diffusion Models
  for Video Super-Resolution <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are just at a tipping point for image super-resolution task.
Nevertheless, it is not trivial to capitalize on diffusion models for video
super-resolution which necessitates not only the preservation of visual
appearance from low-resolution to high-resolution videos, but also the temporal
consistency across video frames. In this paper, we propose a novel approach,
pursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video
super-resolution. SATeCo pivots on learning spatial-temporal guidance from
low-resolution videos to calibrate both latent-space high-resolution video
denoising and pixel-space video reconstruction. Technically, SATeCo freezes all
the parameters of the pre-trained UNet and VAE, and only optimizes two
deliberately-designed spatial feature adaptation (SFA) and temporal feature
alignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame
features via adaptively estimating affine parameters for each pixel,
guaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA
delves into feature interaction within a 3D local window (tubelet) through
self-attention, and executes cross-attention between tubelet and its
low-resolution counterpart to guide temporal feature alignment. Extensive
experiments conducted on the REDS4 and Vid4 datasets demonstrate the
effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Low-Latency and Energy-Efficient Hybrid P2P-CDN Live Video
  Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Farahani, Christian Timmerer, Hermann Hellwagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an
increasingly popular approach in both live and video-on-demand (VoD)
applications. However, designing a scalable and adaptable framework that
reduces servers energy consumption and supports low latency and high quality
services, particularly for live video streaming scenarios, is still challenging
for Over-The-Top (OTT) service providers. To address such challenges, this
paper introduces a new hybrid P2P-CDN framework that leverages new networking
and computing paradigms, i.e., Network Function Virtualization (NFV) and edge
computing for live video streaming. The proposed framework introduces a
multi-layer architecture and a tree of possible actions therein (an action
tree), taking into account all available resources from peers, edge, and CDN
servers to efficiently distribute video fetching and transcoding tasks across a
hybrid P2P-CDN network, consequently enhancing the users latency and video
quality. We also discuss our testbed designed to validate the framework and
compare it with baseline methods. The experimental results indicate that the
proposed framework improves user Quality of Experience (QoE), reduces client
serving latency, and improves edge server energy consumption compared to
baseline approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, Special Issue on Sustainable Multimedia
  Communications and Services, IEEE MMTC Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network-Assisted Delivery of Adaptive Video Streaming Services through
  CDN, SDN, and MEC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Farahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia applications, mainly video streaming services, are currently the
dominant source of network load worldwide. In recent Video-on-Demand (VoD) and
live video streaming services, traditional streaming delivery techniques have
been replaced by adaptive solutions based on the HTTP protocol. Current trends
toward high-resolution (e.g., 8K) and/or low-latency VoD and live video
streaming pose new challenges to end-to-end (E2E) bandwidth demand and have
stringent delay requirements. To do this, video providers typically rely on
Content Delivery Networks (CDNs) to ensure that they provide scalable video
streaming services. To support future streaming scenarios involving millions of
users, it is necessary to increase the CDNs' efficiency. It is widely agreed
that these requirements may be satisfied by adopting emerging networking
techniques to present Network-Assisted Video Streaming (NAVS) methods.
Motivated by this, this thesis goes one step beyond traditional pure
client-based HAS algorithms by incorporating (an) in-network component(s) with
a broader view of the network to present completely transparent NAVS solutions
for HAS clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis defended in 22.08.2023
  (https://netlibrary.aau.at/obvuklhs/content/titleinfo/9173622)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling Instance Associations: A Closer Look for Audio-Visual
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02970v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02970v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhong Chen, Yuyuan Liu, Hu Wang, Fengbei Liu, Chong Wang, Helen Frazer, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual segmentation (AVS) is a challenging task that involves
accurately segmenting sounding objects based on audio-visual cues. The
effectiveness of audio-visual learning critically depends on achieving accurate
cross-modal alignment between sound and visual objects. Successful audio-visual
learning requires two essential components: 1) a challenging dataset with
high-quality pixel-level multi-class annotated images associated with audio
files, and 2) a model that can establish strong links between audio information
and its corresponding visual object. However, these requirements are only
partially addressed by current methods, with training sets containing biased
audio-visual data, and models that generalise poorly beyond this biased
training set. In this work, we propose a new cost-effective strategy to build
challenging and relatively unbiased high-quality audio-visual segmentation
benchmarks. We also propose a new informative sample mining method for
audio-visual supervised contrastive learning to leverage discriminative
contrastive samples to enforce cross-modal understanding. We show empirical
results that demonstrate the effectiveness of our benchmark. Furthermore,
experiments conducted on existing AVS datasets and on our new benchmark show
that our method achieves state-of-the-art (SOTA) segmentation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/cyh-0/CAVP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive <span class="highlight-title">Pre-Train</span>ing with Multi-View Fusion for No-Reference Point
  Cloud Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Jenq-Neng Hwang, Xiaozhong Xu, Shan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-reference point cloud quality assessment (NR-PCQA) aims to automatically
evaluate the perceptual quality of distorted point clouds without available
reference, which have achieved tremendous improvements due to the utilization
of deep neural networks. However, learning-based NR-PCQA methods suffer from
the scarcity of labeled data and usually perform suboptimally in terms of
generalization. To solve the problem, we propose a novel contrastive
pre-training framework tailored for PCQA (CoPA), which enables the pre-trained
model to learn quality-aware representations from unlabeled data. To obtain
anchors in the representation space, we project point clouds with different
distortions into images and randomly mix their local patches to form mixed
images with multiple distortions. Utilizing the generated anchors, we constrain
the pre-training process via a quality-aware contrastive loss following the
philosophy that perceptual quality is closely related to both content and
distortion. Furthermore, in the model fine-tuning stage, we propose a
semantic-guided multi-view fusion module to effectively integrate the features
of projected images from multiple perspectives. Extensive experiments show that
our method outperforms the state-of-the-art PCQA methods on popular benchmarks.
Further investigations demonstrate that CoPA can also benefit existing
learning-based PCQA models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Interaction Modeling via <span class="highlight-title">Self-Supervised</span> Multi-Task Learning
  for <span class="highlight-title">Review</span> Helpfulness Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HongLin Gong, Mengzhao Jia, Liqiang Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In line with the latest research, the task of identifying helpful reviews
from a vast pool of user-generated textual and visual data has become a
prominent area of study. Effective modal representations are expected to
possess two key attributes: consistency and differentiation. Current methods
designed for Multimodal Review Helpfulness Prediction (MRHP) face limitations
in capturing distinctive information due to their reliance on uniform
multimodal annotation. The process of adding varied multimodal annotations is
not only time-consuming but also labor-intensive. To tackle these challenges,
we propose an auto-generated scheme based on multi-task learning to generate
pseudo labels. This approach allows us to simultaneously train for the global
multimodal interaction task and the separate cross-modal interaction subtasks,
enabling us to learn and leverage both consistency and differentiation
effectively. Subsequently, experimental results validate the effectiveness of
pseudo labels, and our approach surpasses previous textual and multimodal
baseline models on two widely accessible benchmark datasets, providing a
solution to the MRHP problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Text-to-Image (TTI) models have become a common approach for
generating training data in various generative fields. However, visual
hallucinations, which contain perceptually critical defects, remain a concern,
especially in non-photorealistic styles like cartoon characters. We propose a
novel visual hallucination detection system for cartoon character images
generated by TTI models. Our approach leverages pose-aware in-context visual
learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB
images and pose information. By incorporating pose guidance from a fine-tuned
pose estimator, we enable VLMs to make more accurate decisions. Experimental
results demonstrate significant improvements in identifying visual
hallucinations compared to baseline methods relying solely on RGB images. This
research advances TTI models by mitigating visual hallucinations, expanding
their potential in non-photorealistic domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures, 1 table, Project page:
  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noisy-Correspondence Learning for Text-to-Image Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng, Joey Tianyi Zhou, Peng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image person re-identification (TIReID) is a compelling topic in the
cross-modal community, which aims to retrieve the target person based on a
textual query. Although numerous TIReID methods have been proposed and achieved
promising performance, they implicitly assume the training image-text pairs are
correctly aligned, which is not always the case in real-world scenarios. In
practice, the image-text pairs inevitably exist under-correlated or even
false-correlated, a.k.a noisy correspondence (NC), due to the low quality of
the images and annotation errors. To address this problem, we propose a novel
Robust Dual Embedding method (RDE) that can learn robust visual-semantic
associations even with NC. Specifically, RDE consists of two main components:
1) A Confident Consensus Division (CCD) module that leverages the dual-grained
decisions of dual embedding modules to obtain a consensus set of clean training
data, which enables the model to learn correct and reliable visual-semantic
associations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional
Triplet Ranking loss with the hardest negative samples to a log-exponential
upper bound over all negative ones, thus preventing the model collapse under NC
and can also focus on hard-negative samples for promising performance. We
conduct extensive experiments on three public benchmarks, namely CUHK-PEDES,
ICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our
RDE. Our method achieves state-of-the-art results both with and without
synthetic noisy correspondences on all three datasets. Code is available at
https://github.com/QinYang79/RDE.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cedric Perauer, Laurenz Adrian Heidrich, Haifan Zhang, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, progress in acquisition equipment such as LiDAR sensors has enabled
sensing increasingly spacious outdoor 3D environments. Making sense of such 3D
acquisitions requires fine-grained scene understanding, such as constructing
instance-based 3D scene segmentations. Commonly, a neural network is trained
for this task; however, this requires access to a large, densely annotated
dataset, which is widely known to be challenging to obtain. To address this
issue, in this work we propose to predict instance segmentations for 3D scenes
in an unsupervised way, without relying on ground-truth annotations. To this
end, we construct a learning framework consisting of two components: (1) a
pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and
(2) a self-training algorithm for instance segmentation to fit robust, accurate
instances from initial noisy proposals. To enable generating 3D instance mask
proposals, we construct a weighted proxy-graph by connecting 3D points with
edges integrating multi-modal image- and point-based self-supervised features,
and perform graph-cuts to isolate individual pseudo-instances. We then build on
a state-of-the-art point-based architecture and train a 3D instance
segmentation model, resulting in significant refinement of initial proposals.
To scale to arbitrary complexity 3D scenes, we design our algorithm to operate
on local 3D point chunks and construct a merging step to generate scene-level
instance segmentations. Experiments on the challenging SemanticKITTI benchmark
demonstrate the potential of our approach, where it attains 13.3% higher
Average Precision and 9.1% higher F1 score compared to the best-performing
baseline. The code will be made publicly available at
https://github.com/artonson/autoinst.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ latentSplat: Autoencoding Variational Gaussians for Fast Generalizable
  3D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present latentSplat, a method to predict semantic Gaussians in a 3D latent
space that can be splatted and decoded by a light-weight generative 2D
architecture. Existing methods for generalizable 3D reconstruction either do
not enable fast inference of high resolution novel views due to slow volume
rendering, or are limited to interpolation of close input views, even in
simpler settings with a single central object, where 360-degree generalization
is possible. In this work, we combine a regression-based approach with a
generative model, moving towards both of these capabilities within the same
method, trained purely on readily available real video data. The core of our
method are variational 3D Gaussians, a representation that efficiently encodes
varying uncertainty within a latent space consisting of 3D feature Gaussians.
From these Gaussians, specific instances can be sampled and rendered via
efficient Gaussian splatting and a fast, generative decoder network. We show
that latentSplat outperforms previous works in reconstruction quality and
generalization, while being fast and scalable to high-resolution data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://geometric-rl.mpi-inf.mpg.de/latentsplat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HemoSet: The First Blood Segmentation <span class="highlight-title">Dataset</span> for Automation of
  Hemostasis Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert J. Miao Shan Lin, Jingpei Lu, Florian Richter, Benjamin Ostrander, Emily K. Funk, Ryan K. Orosco, Michael C. Yip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hemorrhaging occurs in surgeries of all types, forcing surgeons to quickly
adapt to the visual interference that results from blood rapidly filling the
surgical field. Introducing automation into the crucial surgical task of
hemostasis management would offload mental and physical tasks from the surgeon
and surgical assistants while simultaneously increasing the efficiency and
safety of the operation. The first step in automation of hemostasis management
is detection of blood in the surgical field. To propel the development of blood
detection algorithms in surgeries, we present HemoSet, the first blood
segmentation dataset based on bleeding during a live animal robotic surgery.
Our dataset features vessel hemorrhage scenarios where turbulent flow leads to
abnormal pooling geometries in surgical fields. These pools are formed in
conditions endemic to surgical procedures -- uneven heterogeneous tissue, under
glossy lighting conditions and rapid tool movement. We benchmark several
state-of-the-art segmentation models and provide insight into the difficulties
specific to blood detection. We intend for HemoSet to spur development of
autonomous blood suction tools by providing a platform for training and
refining blood segmentation models, addressing the precision needed for such
robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary
  Alignment for Temporal Referential Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Tang, Daiki Shimada, Jing Bi, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In everyday communication, humans frequently use speech and gestures to refer
to specific areas or objects, a process known as Referential Dialogue (RD).
While prior studies have investigated RD through Large Language Models (LLMs)
or Large Multimodal Models (LMMs) in static contexts, the exploration of
Temporal Referential Dialogue (TRD) within audio-visual media remains limited.
Two primary challenges hinder progress in this field: (1) the absence of
comprehensive, untrimmed audio-visual video datasets with precise temporal
annotations, and (2) the need for methods to integrate complex temporal
auditory and visual cues effectively. To address these challenges, we introduce
a novel framework to generate PU-VALOR, an extensive audio-visual dataset
comprising over 114,000 untrimmed videos with accurate temporal demarcations.
We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI)
that ensures the temporal alignment of audio-visual information. Additionally,
we develop the A5-222K dataset, encompassing more than 200,000 audio-text
pairings, to facilitate the audio and text alignments. Our experiments
demonstrate that AVicuna can effectively handle TRD in audio-visual videos and
achieve state-of-the-art performance on various audio-visual video
understanding tasks, particularly in untrimmed videos. We further investigate
the optimal audio-interleaving rate for interleaved audio-visual inputs, which
maximizes performance on the Audio-Visual Event Dense Localization task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L-MAE: Longitudinal masked auto-encoder with time and severity-aware
  encoding for diabetic retinopathy progression prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li, Alireza Rezaei, Hugo Le Boité, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Ikram Brahim, Gwenolé Quellec, Mathieu Lamard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training strategies based on self-supervised learning (SSL) have proven
to be effective pretext tasks for many downstream tasks in computer vision. Due
to the significant disparity between medical and natural images, the
application of typical SSL is not straightforward in medical imaging.
Additionally, those pretext tasks often lack context, which is critical for
computer-aided clinical decision support. In this paper, we developed a
longitudinal masked auto-encoder (MAE) based on the well-known
Transformer-based MAE. In particular, we explored the importance of time-aware
position embedding as well as disease progression-aware masking. Taking into
account the time between examinations instead of just scheduling them offers
the benefit of capturing temporal changes and trends. The masking strategy, for
its part, evolves during follow-up to better capture pathological changes,
ensuring a more accurate assessment of disease progression. Using OPHDIAT, a
large follow-up screening dataset targeting diabetic retinopathy (DR), we
evaluated the pre-trained weights on a longitudinal task, which is to predict
the severity label of the next visit within 3 years based on the past time
series examinations. Our results demonstrated the relevancy of both time-aware
position embedding and masking strategies based on disease progression
knowledge. Compared to popular baseline models and standard longitudinal
Transformers, these simple yet effective extensions significantly enhance the
predictive ability of deep classification models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Detectors in the Open Environment:Challenges, Solutions, and
  Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Liang, Wei Wang, Ruoyu Chen, Aishan Liu, Boxi Wu, Ee-Chien Chang, Xiaochun Cao, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of foundation models, deep learning-based object detectors
have shown practical usability in closed set scenarios. However, for real-world
tasks, object detectors often operate in open environments, where crucial
factors (\eg, data distribution, objective) that influence model learning are
often changing. The dynamic and intricate nature of the open environment poses
novel and formidable challenges to object detectors. Unfortunately, current
research on object detectors in open environments lacks a comprehensive
analysis of their distinctive characteristics, challenges, and corresponding
solutions, which hinders their secure deployment in critical real-world
scenarios. This paper aims to bridge this gap by conducting a comprehensive
review and analysis of object detectors in open environments. We initially
identified limitations of key structural components within the existing
detection pipeline and propose the open environment object detector challenge
framework that includes four quadrants (\ie, out-of-domain, out-of-category,
robust learning, and incremental learning) based on the dimensions of the data
/ target changes. For each quadrant of challenges in the proposed framework, we
present a detailed description and systematic analysis of the overarching goals
and core difficulties, systematically review the corresponding solutions, and
benchmark their performance over multiple widely adopted datasets. In addition,
we engage in a discussion of open problems and potential avenues for future
research. This paper aims to provide a fresh, comprehensive, and systematic
understanding of the challenges and solutions associated with open-environment
object detectors, thus catalyzing the development of more solid applications in
real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constricting Normal Latent Space for Anomaly Detection with Normal-only
  Training Data <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcella Astrid, Muhammad Zaigham Zaheer, Seung-Ik Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to devise an anomaly detection model using only normal training
data, an autoencoder (AE) is typically trained to reconstruct the data. As a
result, the AE can extract normal representations in its latent space. During
test time, since AE is not trained using real anomalies, it is expected to
poorly reconstruct the anomalous data. However, several researchers have
observed that it is not the case. In this work, we propose to limit the
reconstruction capability of AE by introducing a novel latent constriction
loss, which is added to the existing reconstruction loss. By using our method,
no extra computational cost is added to the AE during test time. Evaluations
using three video anomaly detection benchmark datasets, i.e., Ped2, Avenue, and
ShanghaiTech, demonstrate the effectiveness of our method in limiting the
reconstruction capability of AE, which leads to a better anomaly detection
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR Workshop 2024 (PML4LRS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Recognition from the perspective of Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savinay Nagendra, Prapti Panigrahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applications of an efficient emotion recognition system can be found in
several domains such as medicine, driver fatigue surveillance, social robotics,
and human-computer interaction. Appraising human emotional states, behaviors,
and reactions displayed in real-world settings can be accomplished using latent
continuous dimensions. Continuous dimensional models of human affect, such as
those based on valence and arousal are more accurate in describing a broad
range of spontaneous everyday emotions than more traditional models of discrete
stereotypical emotion categories (e.g. happiness, surprise). Most of the prior
work on estimating valence and arousal considers laboratory settings and acted
data. But, for emotion recognition systems to be deployed and integrated into
real-world mobile and computing devices, we need to consider data collected in
the world. Action recognition is a domain of Computer Vision that involves
capturing complementary information on appearance from still frames and motion
between frames. In this paper, we treat emotion recognition from the
perspective of action recognition by exploring the application of deep learning
architectures specifically designed for action recognition, for continuous
affect recognition. We propose a novel three-stream end-to-end deep learning
regression pipeline with an attention mechanism, which is an ensemble design
based on sub-modules of multiple state-of-the-art action recognition systems.
The pipeline constitutes a novel data pre-processing approach with a spatial
self-attention mechanism to extract keyframes. The optical flow of
high-attention regions of the face is extracted to capture temporal context.
AFEW-VA in-the-wild dataset has been used to conduct comparative experiments.
Quantitative analysis shows that the proposed model outperforms multiple
standard baselines of both emotion recognition and action recognition models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhui Xu, Fuxun Yu, Zirui Xu, Nathan Inkawhich, Xiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research underscores the pivotal role of the Out-of-Distribution (OOD)
feature representation field scale in determining the efficacy of models in OOD
detection. Consequently, the adoption of model ensembles has emerged as a
prominent strategy to augment this feature representation field, capitalizing
on anticipated model diversity.
  However, our introduction of novel qualitative and quantitative model
ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and
the Self-Coupling Index, reveals a critical drawback in existing ensemble
methods. We find that these methods incorporate weights that are
affine-transformable, exhibiting limited variability and thus failing to
achieve the desired diversity in feature representation.
  To address this limitation, we elevate the dimensions of traditional model
ensembles, incorporating various factors such as different weight
initializations, data holdout, etc., into distinct supervision tasks. This
innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages
diverse training tasks to generate distinct comprehensions of the data and
labels, thereby extending the feature representation field.
  Our experimental results demonstrate the superior performance of the MC
Ensemble strategy in OOD detection compared to both the naive Deep Ensemble
method and a standalone model of comparable size. This underscores the
effectiveness of our proposed approach in enhancing the model's capability to
detect instances outside its training distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated
  Synthesis <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atefeh Khoshkhahtinat, Ali Zafari, Piyush M. Mehta, Nasser M. Nasrabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While replacing Gaussian decoders with a conditional diffusion model enhances
the perceptual quality of reconstructions in neural image compression, their
lack of inductive bias for image data restricts their ability to achieve
state-of-the-art perceptual levels. To address this limitation, we adopt a
non-isotropic diffusion model at the decoder side. This model imposes an
inductive bias aimed at distinguishing between frequency contents, thereby
facilitating the generation of high-quality images. Moreover, our framework is
equipped with a novel entropy model that accurately models the probability
distribution of latent representation by exploiting spatio-channel correlations
in latent space, while accelerating the entropy decoding step. This
channel-wise entropy model leverages both local and global spatial contexts
within each channel chunk. The global spatial context is built upon the
Transformer, which is specifically designed for image compression tasks. The
designed Transformer employs a Laplacian-shaped positional encoding, the
learnable parameters of which are adaptively adjusted for each channel cluster.
Our experiments demonstrate that our proposed framework yields better
perceptual quality compared to cutting-edge generative-based codecs, and the
proposed entropy model contributes to notable bitrate savings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal
  Contrastive Learning via Local Token Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal contrastive learning has emerged as a powerful paradigm for
building high-quality features using the complementary strengths of various
data modalities. However, the open nature of such systems inadvertently
increases the possibility of backdoor attacks. These attacks subtly embed
malicious behaviors within the model during training, which can be activated by
specific triggers in the inference phase, posing significant security risks.
Despite existing countermeasures through fine-tuning that reduce the adverse
impacts of such attacks, these defenses often degrade the clean accuracy and
necessitate the construction of extensive clean training pairs. In this paper,
we explore the possibility of a less-cost defense from the perspective of model
unlearning, that is, whether the model can be made to quickly \textbf{u}nlearn
\textbf{b}ackdoor \textbf{t}hreats (UBT) by constructing a small set of
poisoned samples. Specifically, we strengthen the backdoor shortcuts to
discover suspicious samples through overfitting training prioritized by weak
similarity samples. Building on the initial identification of suspicious
samples, we introduce an innovative token-based localized forgetting training
regime. This technique specifically targets the poisoned aspects of the model,
applying a focused effort to unlearn the backdoor associations and trying not
to damage the integrity of the overall model. Experimental results show that
our method not only ensures a minimal success rate for attacks, but also
preserves the model's high clean accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partially Blinded Unlearning: Class Unlearning for Deep Networks a
  Bayesian Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhodip Panda, Shashwat Sourav, Prathosh A. P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to adhere to regulatory standards governing individual data privacy
and safety, machine learning models must systematically eliminate information
derived from specific subsets of a user's training data that can no longer be
utilized. The emerging discipline of Machine Unlearning has arisen as a pivotal
area of research, facilitating the process of selectively discarding
information designated to specific sets or classes of data from a pre-trained
model, thereby eliminating the necessity for extensive retraining from scratch.
The principal aim of this study is to formulate a methodology tailored for the
purposeful elimination of information linked to a specific class of data from a
pre-trained classification network. This intentional removal is crafted to
degrade the model's performance specifically concerning the unlearned data
class while concurrently minimizing any detrimental impacts on the model's
performance in other classes. To achieve this goal, we frame the class
unlearning problem from a Bayesian perspective, which yields a loss function
that minimizes the log-likelihood associated with the unlearned data with a
stability regularization in parameter space. This stability regularization
incorporates Mohalanobis distance with respect to the Fisher Information matrix
and $l_2$ distance from the pre-trained model parameters. Our novel approach,
termed \textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing
state-of-the-art class unlearning methods, demonstrating superior
effectiveness. Notably, PBU achieves this efficacy without requiring awareness
of the entire training dataset but only to the unlearned data points, marking a
distinctive feature of its performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Equivalency, Substitutability, and Flexibility of Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che-Jui Chang, Danrui Li, Seonghyeon Moon, Mubbasir Kapadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study, from an empirical standpoint, the efficacy of synthetic data in
real-world scenarios. Leveraging synthetic data for training perception models
has become a key strategy embraced by the community due to its efficiency,
scalability, perfect annotations, and low costs. Despite proven advantages, few
studies put their stress on how to efficiently generate synthetic datasets to
solve real-world problems and to what extent synthetic data can reduce the
effort for real-world data collection. To answer the questions, we
systematically investigate several interesting properties of synthetic data --
the equivalency of synthetic data to real-world data, the substitutability of
synthetic data for real data, and the flexibility of synthetic data generators
to close up domain gaps. Leveraging the M3Act synthetic data generator, we
conduct experiments on DanceTrack and MOT17. Our results suggest that synthetic
data not only enhances model performance but also demonstrates substitutability
for real data, with 60% to 80% replacement without performance loss. In
addition, our study of the impact of synthetic data distributions on downstream
performance reveals the importance of flexible data generators in narrowing
domain gaps for improved model adaptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarially Masked Video Consistency for Unsupervised Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhu, Junwei Liang, Po-Yao Huang, Alex Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of unsupervised domain adaptation for egocentric videos.
We propose a transformer-based model to learn class-discriminative and
domain-invariant feature representations. It consists of two novel designs. The
first module is called Generative Adversarial Domain Alignment Network with the
aim of learning domain-invariant representations. It simultaneously learns a
mask generator and a domain-invariant encoder in an adversarial way. The
domain-invariant encoder is trained to minimize the distance between the source
and target domain. The masking generator, conversely, aims at producing
challenging masks by maximizing the domain distance. The second is a Masked
Consistency Learning module to learn class-discriminative representations. It
enforces the prediction consistency between the masked target videos and their
full forms. To better evaluate the effectiveness of domain adaptation methods,
we construct a more challenging benchmark for egocentric videos, U-Ego4D. Our
method achieves state-of-the-art performance on the Epic-Kitchen and the
proposed U-Ego4D benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low Rank Groupwise Deformations for Motion Tracking in Cardiac Cine MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Rendell, Jinming Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffeomorphic image registration is a commonly used method to deform one
image to resemble another. While warping a single image to another is useful,
it can be advantageous to warp multiple images simultaneously, such as in
tracking the motion of the heart across a sequence of images. In this paper,
our objective is to propose a novel method capable of registering a group or
sequence of images to a target image, resulting in registered images that
appear identical and therefore have a low rank. Moreover, we aim for these
registered images to closely resemble the target image. Through experimental
evidence, we will demonstrate our method's superior efficacy in producing
low-rank groupwise deformations compared to other state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A thesis submitted to the University of Birmingham for MSc Degree</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-modal Prior Semantic Guided Infrared and Visible Image Fusion for
  Intelligent Transportation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Li, Lu Bai, Bin Yang, Chang Li, Lingfei Ma, Lixin Cui, Edwin R. Hancock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared and visible image fusion (IVF) plays an important role in
intelligent transportation system (ITS). The early works predominantly focus on
boosting the visual appeal of the fused result, and only several recent
approaches have tried to combine the high-level vision task with IVF. However,
they prioritize the design of cascaded structure to seek unified suitable
features and fit different tasks. Thus, they tend to typically bias toward to
reconstructing raw pixels without considering the significance of semantic
features. Therefore, we propose a novel prior semantic guided image fusion
method based on the dual-modality strategy, improving the performance of IVF in
ITS. Specifically, to explore the independent significant semantic of each
modality, we first design two parallel semantic segmentation branches with a
refined feature adaptive-modulation (RFaM) mechanism. RFaM can perceive the
features that are semantically distinct enough in each semantic segmentation
branch. Then, two pilot experiments based on the two branches are conducted to
capture the significant prior semantic of two images, which then is applied to
guide the fusion task in the integration of semantic segmentation branches and
fusion branches. In addition, to aggregate both high-level semantics and
impressive visual effects, we further investigate the frequency response of the
prior semantics, and propose a multi-level representation-adaptive fusion
(MRaF) module to explicitly integrate the low-frequent prior semantic with the
high-frequent details. Extensive experiments on two public datasets demonstrate
the superiority of our method over the state-of-the-art image fusion
approaches, in terms of either the visual appeal or the high-level semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse Rendering of Glossy Objects via the Neural Plenoptic Function
  and Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse rendering aims at recovering both geometry and materials of objects.
It provides a more compatible reconstruction for conventional rendering
engines, compared with the neural radiance fields (NeRFs). On the other hand,
existing NeRF-based inverse rendering methods cannot handle glossy objects with
local light interactions well, as they typically oversimplify the illumination
as a 2D environmental map, which assumes infinite lights only. Observing the
superiority of NeRFs in recovering radiance fields, we propose a novel 5D
Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more
accurate lighting-object interactions can be formulated via the rendering
equation. We also design a material-aware cone sampling strategy to efficiently
integrate lights inside the BRDF lobes with the help of pre-filtered radiance
fields. Our method has two stages: the geometry of the target object and the
pre-filtered environmental radiance fields are reconstructed in the first
stage, and materials of the target object are estimated in the second stage
with the proposed NeP and material-aware cone sampling strategy. Extensive
experiments on the proposed real-world and synthetic datasets demonstrate that
our method can reconstruct high-fidelity geometry/materials of challenging
glossy objects with complex lighting interactions from nearby objects. Project
webpage: https://whyy.site/paper/nep
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 paper. Project webpage https://whyy.site/paper/nep</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exemplar-Free Class Incremental Learning via Incremental Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Huang, Zhulin An, Yan Zeng, Chuanguang Yang, Xinqiang Yu, Yongjun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exemplar-Free Class Incremental Learning (efCIL) aims to continuously
incorporate the knowledge from new classes while retaining previously learned
information, without storing any old-class exemplars (i.e., samples). For this
purpose, various efCIL methods have been proposed over the past few years,
generally with elaborately constructed old pseudo-features, increasing the
difficulty of model development and interpretation. In contrast, we propose a
\textbf{simple Incremental Representation (IR) framework} for efCIL without
constructing old pseudo-features. IR utilizes dataset augmentation to cover a
suitable feature space and prevents the model from forgetting by using a single
L2 space maintenance loss. We discard the transient classifier trained on each
one of the sequence tasks and instead replace it with a 1-near-neighbor
classifier for inference, ensuring the representation is incrementally updated
during CIL. Extensive experiments demonstrate that our proposed IR achieves
comparable performance while significantly preventing the model from forgetting
on CIFAR100, TinyImageNet, and ImageNetSubset datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI
  Classification in Alzheimer Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaojie Li, Haichen Qu, Xinqi Dong, Bo Dang, Hengyi Zang, Yulu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring the application of deep learning technologies in the field of
medical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique
perspective for observing and diagnosing complex neurodegenerative diseases
such as Alzheimer Disease (AD). With advancements in deep learning,
particularly in Convolutional Neural Networks (CNNs) and the Xception network
architecture, we are now able to analyze and classify vast amounts of MRI data
with unprecedented accuracy. The progress of this technology not only enhances
our understanding of brain structural changes but also opens up new avenues for
monitoring disease progression through non-invasive means and potentially
allows for precise diagnosis in the early stages of the disease.
  This study aims to classify MRI images using deep learning models to identify
different stages of Alzheimer Disease through a series of innovative data
processing and model construction steps. Our experimental results show that the
deep learning framework based on the Xception model achieved a 99.6% accuracy
rate in the multi-class MRI image classification task, demonstrating its
potential application value in assistive diagnosis. Future research will focus
on expanding the dataset, improving model interpretability, and clinical
validation to further promote the application of deep learning technology in
the medical field, with the hope of bringing earlier diagnosis and more
personalized treatment plans to Alzheimer Disease patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frankenstein: Generating Semantic-Compositional 3D Scenes in One
  Tri-Plane 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Frankenstein, a diffusion-based framework that can generate
semantic-compositional 3D scenes in a single pass. Unlike existing methods that
output a single, unified 3D shape, Frankenstein simultaneously generates
multiple separated shapes, each corresponding to a semantically meaningful
part. The 3D scene information is encoded in one single tri-plane tensor, from
which multiple Singed Distance Function (SDF) fields can be decoded to
represent the compositional shapes. During training, an auto-encoder compresses
tri-planes into a latent space, and then the denoising diffusion process is
employed to approximate the distribution of the compositional scenes.
Frankenstein demonstrates promising results in generating room interiors as
well as human avatars with automatically separated parts. The generated scenes
facilitate many downstream applications, such as part-wise re-texturing, object
rearrangement in the room or avatar cloth re-targeting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video: https://youtu.be/lRn-HqyCrLI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Captioning in news report scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianrui Liu, Qi Cai, Changxin Xu, Zhanxin Zhou, Jize Xiong, Yuxin Qiao, Tsungwei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning strives to generate pertinent captions for specified images,
situating itself at the crossroads of Computer Vision (CV) and Natural Language
Processing (NLP). This endeavor is of paramount importance with far-reaching
applications in recommendation systems, news outlets, social media, and beyond.
Particularly within the realm of news reporting, captions are expected to
encompass detailed information, such as the identities of celebrities captured
in the images. However, much of the existing body of work primarily centers
around understanding scenes and actions. In this paper, we explore the realm of
image captioning specifically tailored for celebrity photographs, illustrating
its broad potential for enhancing news industry practices. This exploration
aims to augment automated news content generation, thereby facilitating a more
nuanced dissemination of information. Our endeavor shows a broader horizon,
enriching the narrative in news reporting through a more intuitive image
captioning framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqing Liang, Congyi Zhang, Junli Zhao, Wenping Wang, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deducing the 3D face from a skull is an essential but challenging task in
forensic science and archaeology. Existing methods for automated facial
reconstruction yield inaccurate results, suffering from the non-determinative
nature of the problem that a skull with a sparse set of tissue depth cannot
fully determine the skinned face. Additionally, their texture-less results
require further post-processing stages to achieve a photo-realistic appearance.
This paper proposes an end-to-end 3D face reconstruction and exploration tool,
providing textured 3D faces for reference. With the help of state-of-the-art
text-to-image diffusion models and image-based facial reconstruction
techniques, we generate an initial reference 3D face, whose biological profile
aligns with the given skull. We then adapt these initial faces to meet the
statistical expectations of extruded anatomical landmarks on the skull through
an optimization process. The joint statistical distribution of tissue depths is
learned on a small set of anatomical landmarks on the skull. To support further
adjustment, we propose an efficient face adaptation tool to assist users in
tuning tissue depths, either globally or at local regions, while observing
plausible visual feedback. Experiments conducted on a real skull-face dataset
demonstrated the effectiveness of our proposed pipeline in terms of
reconstruction accuracy, diversity, and stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown
  Domains <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang-Dang Pham, Phong Tran, Anh Tran, Cuong Pham, Rang Nguyen, Minh Hoai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative framework designed to train an image
deblurring algorithm tailored to a specific camera device. This algorithm works
by transforming a blurry input image, which is challenging to deblur, into
another blurry image that is more amenable to deblurring. The transformation
process, from one blurry state to another, leverages unpaired data consisting
of sharp and blurry images captured by the target camera device. Learning this
blur-to-blur transformation is inherently simpler than direct blur-to-sharp
conversion, as it primarily involves modifying blur patterns rather than the
intricate task of reconstructing fine image details. The efficacy of the
proposed approach has been demonstrated through comprehensive experiments on
various benchmarks, where it significantly outperforms state-of-the-art methods
both quantitatively and qualitatively. Our code and data are available at
https://zero1778.github.io/blur2blur/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FH-SSTNet: Forehead Creases based User Verification using Spatio-Spatial
  Temporal Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geetanjali Sharma, Gaurav Jaswal, Aditya Nigam, Raghavendra Ramachandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biometric authentication, which utilizes contactless features, such as
forehead patterns, has become increasingly important for identity verification
and access management. The proposed method is based on learning a 3D
spatio-spatial temporal convolution to create detailed pictures of forehead
patterns. We introduce a new CNN model called the Forehead Spatio-Spatial
Temporal Network (FH-SSTNet), which utilizes a 3D CNN architecture with triplet
loss to capture distinguishing features. We enhance the model's discrimination
capability using Arcloss in the network's head. Experimentation on the Forehead
Creases version 1 (FH-V1) dataset, containing 247 unique subjects, demonstrates
the superior performance of FH-SSTNet compared to existing methods and
pre-trained CNNs like ResNet50, especially for forehead-based user
verification. The results demonstrate the superior performance of FH-SSTNet for
forehead-based user verification, confirming its effectiveness in identity
authentication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 Figure, IWBF conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Discrete to Continuous: Deep Fair Clustering With Transferable
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of deep fair clustering, which partitions data into
clusters via the representations extracted by deep neural networks while hiding
sensitive data attributes. To achieve fairness, existing methods present a
variety of fairness-related objective functions based on the group fairness
criterion. However, these works typically assume that the sensitive attributes
are discrete and do not work for continuous sensitive variables, such as the
proportion of the female population in an area. Besides, the potential of the
representations learned from clustering tasks to improve performance on other
tasks is ignored by existing works. In light of these limitations, we propose a
flexible deep fair clustering method that can handle discrete and continuous
sensitive attributes simultaneously. Specifically, we design an information
bottleneck style objective function to learn fair and clustering-friendly
representations. Furthermore, we explore for the first time the transferability
of the extracted representations to other downstream tasks. Unlike existing
works, we impose fairness at the representation level, which could guarantee
fairness for the transferred task regardless of clustering results. To verify
the effectiveness of the proposed method, we perform extensive experiments on
datasets with discrete and continuous sensitive attributes, demonstrating the
advantage of our method in comparison with state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Model is a Good Pose Estimator from 3D RF-Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqiao Fan, Jianfei Yang, Yuecong Xu, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human pose estimation (HPE) from Radio Frequency vision (RF-vision) performs
human sensing using RF signals that penetrate obstacles without revealing
privacy (e.g., facial information). Recently, mmWave radar has emerged as a
promising RF-vision sensor, providing radar point clouds by processing RF
signals. However, the mmWave radar has a limited resolution with severe noise,
leading to inaccurate and inconsistent human pose estimation. This work
proposes mmDiff, a novel diffusion-based pose estimator tailored for noisy
radar data. Our approach aims to provide reliable guidance as conditions to
diffusion models. Two key challenges are addressed by mmDiff: (1)
miss-detection of parts of human bodies, which is addressed by a module that
isolates feature extraction from different body parts, and (2) signal
inconsistency due to environmental interference, which is tackled by
incorporating prior knowledge of body structure and motion. Several modules are
designed to achieve these goals, whose features work as the conditions for the
subsequent diffusion model, eliminating the miss-detection and instability of
HPE based on RF-vision. Extensive experiments demonstrate that mmDiff
outperforms existing methods significantly, achieving state-of-the-art
performances on public datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised
  Landmark Discovery <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Tourani, Ahmed Alwheibi, Arif Mahmood, Muhammad Haris Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised landmarks discovery (ULD) for an object category is a
challenging computer vision problem. In pursuit of developing a robust ULD
framework, we explore the potential of a recent paradigm of self-supervised
learning algorithms, known as diffusion models. Some recent works have shown
that these models implicitly contain important correspondence cues. Towards
harnessing the potential of diffusion models for the ULD task, we make the
following core contributions. First, we propose a ZeroShot ULD baseline based
on simple clustering of random pixel locations with nearest neighbour matching.
It delivers better results than existing ULD methods. Second, motivated by the
ZeroShot performance, we develop a ULD algorithm based on diffusion features
using self-training and clustering which also outperforms prior methods by
notable margins. Third, we introduce a new proxy task based on generating
latent pose codes and also propose a two-stage clustering mechanism to
facilitate effective pseudo-labeling, resulting in a significant performance
improvement. Overall, our approach consistently outperforms state-of-the-art
methods on four challenging benchmarks AFLW, MAFL, CatHeads and LS3D by
significant margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ghost on the Shell: An Expressive Representation of General 3D Shapes <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15168v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15168v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of photorealistic virtual worlds requires the accurate modeling
of 3D surface geometry for a wide range of objects. For this, meshes are
appealing since they 1) enable fast physics-based rendering with realistic
material and lighting, 2) support physical simulation, and 3) are
memory-efficient for modern graphics pipelines. Recent work on reconstructing
and statistically modeling 3D shape, however, has critiqued meshes as being
topologically inflexible. To capture a wide range of object shapes, any 3D
representation must be able to model solid, watertight, shapes as well as thin,
open, surfaces. Recent work has focused on the former, and methods for
reconstructing open surfaces do not support fast reconstruction with material
and lighting or unconditional generative modelling. Inspired by the observation
that open surfaces can be seen as islands floating on watertight surfaces, we
parameterize open surfaces by defining a manifold signed distance field on
watertight templates. With this parameterization, we further develop a
grid-based and differentiable representation that parameterizes both watertight
and non-watertight meshes of arbitrary topology. Our new representation, called
Ghost-on-the-Shell (G-Shell), enables two important applications:
differentiable rasterization-based reconstruction from multiview images and
generative modelling of non-watertight meshes. We empirically demonstrate that
G-Shell achieves state-of-the-art performance on non-watertight mesh
reconstruction and generation tasks, while also performing effectively for
watertight meshes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Oral (v3: 30 pages, 19 figures, Project Page:
  https://gshell3d.github.io/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VQPy: An Object-Oriented Approach to Modern Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Yu, Zhenting Zhu, Yu Chen, Hanchen Xu, Pengzhan Zhao, Yang Wang, Arthi Padmanabhan, Hugo Latapie, Harry Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video analytics is widely used in contemporary systems and services. At the
forefront of video analytics are video queries that users develop to find
objects of particular interest. Building upon the insight that video objects
(e.g., human, animals, cars, etc.), the center of video analytics, are similar
in spirit to objects modeled by traditional object-oriented languages, we
propose to develop an object-oriented approach to video analytics. This
approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant
with constructs that make it easy for users to express video objects and their
interactions$\unicode{x2015}$as well as an extensible backend that can
automatically construct and optimize pipelines based on video objects. We have
implemented and open-sourced VQPy, which has been productized in Cisco as part
of its DeepVision framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MLSys'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent <span class="highlight-title">Dataset</span> Distillation with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficacy of machine learning has traditionally relied on the availability
of increasingly larger datasets. However, large datasets pose storage
challenges and contain non-influential samples, which could be ignored during
training without impacting the final accuracy of the model. In response to
these limitations, the concept of distilling the information on a dataset into
a condensed set of (synthetic) samples, namely a distilled dataset, emerged.
One crucial aspect is the selected architecture (usually ConvNet) for linking
the original and synthetic datasets. However, the final accuracy is lower if
the employed model architecture differs from the model used during
distillation. Another challenge is the generation of high-resolution images,
e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation
with Diffusion Models (LD3M) that combine diffusion in latent space with
dataset distillation to tackle both challenges. LD3M incorporates a novel
diffusion process tailored for dataset distillation, which improves the
gradient norms for learning synthetic images. By adjusting the number of
diffusion steps, LD3M also offers a straightforward way of controlling the
trade-off between speed and accuracy. We evaluate our approach in several
ImageNet subsets and for high-resolution images (128x128 and 256x256). As a
result, LD3M consistently outperforms state-of-the-art distillation techniques
by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li, Rama Chellappa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent efforts in using 3D Gaussians for scene reconstruction and novel view
synthesis can achieve impressive results on curated benchmarks; however, images
captured in real life are often blurry. In this work, we analyze the robustness
of Gaussian-Splatting-based methods against various image blur, such as motion
blur, defocus blur, downscaling blur, \etc. Under these degradations,
Gaussian-Splatting-based methods tend to overfit and produce worse results than
Neural-Radiance-Field-based methods. To address this issue, we propose Blur
Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling
capacities such that a 3D-consistent and high quality scene can be
reconstructed despite image-wise blur. Specifically, we model blur by
estimating per-pixel convolution kernels from a Blur Proposal Network (BPN).
BPN is designed to consider spatial, color, and depth variations of the scene
to maximize modeling capacity. Additionally, BPN also proposes a
quality-assessing mask, which indicates regions where blur occur. Finally, we
introduce a coarse-to-fine kernel optimization scheme; this optimization scheme
is fast and avoids sub-optimal solutions due to a sparse point cloud
initialization, which often occurs when we apply Structure-from-Motion on
blurry images. We demonstrate that BAGS achieves photorealistic renderings
under various challenging blur conditions and imaging geometry, while
significantly improving upon existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection of diabetic retinopathy using longitudinal <span class="highlight-title">self-supervised</span>
  learning <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Gwenolé Quellec, Mathieu Lamard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Longitudinal imaging is able to capture both static anatomical structures and
dynamic changes in disease progression towards earlier and better
patient-specific pathology management. However, conventional approaches for
detecting diabetic retinopathy (DR) rarely take advantage of longitudinal
information to improve DR analysis. In this work, we investigate the benefit of
exploiting self-supervised learning with a longitudinal nature for DR diagnosis
purposes. We compare different longitudinal self-supervised learning (LSSL)
methods to model the disease progression from longitudinal retinal color fundus
photographs (CFP) to detect early DR severity changes using a pair of
consecutive exams. The experiments were conducted on a longitudinal DR
screening dataset with or without those trained encoders (LSSL) acting as a
longitudinal pretext task. Results achieve an AUC of 0.875 for the baseline
(model trained from scratch) and an AUC of 0.96 (95% CI: 0.9593-0.9655 DeLong
test) with a p-value < 2.2e-16 on early fusion using a simple ResNet alike
architecture with frozen LSSL weights, suggesting that the LSSL latent space
enables to encode the dynamic of DR progression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted preprint for presentation at MICCAI-OMIA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influencer Backdoor Attack on Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a small number of poisoned samples are injected into the training
dataset of a deep neural network, the network can be induced to exhibit
malicious behavior during inferences, which poses potential threats to
real-world applications. While they have been intensively studied in
classification, backdoor attacks on semantic segmentation have been largely
overlooked. Unlike classification, semantic segmentation aims to classify every
pixel within a given image. In this work, we explore backdoor attacks on
segmentation models to misclassify all pixels of a victim class by injecting a
specific trigger on non-victim pixels during inferences, which is dubbed
Influencer Backdoor Attack (IBA). IBA is expected to maintain the
classification accuracy of non-victim pixels and mislead classifications of all
victim pixels in every single inference and could be easily applied to
real-world scenes. Based on the context aggregation ability of segmentation
models, we proposed a simple, yet effective, Nearest-Neighbor trigger injection
strategy. We also introduce an innovative Pixel Random Labeling strategy which
maintains optimal performance even when the trigger is placed far from the
victim pixels. Our extensive experiments reveal that current segmentation
models do suffer from backdoor attacks, demonstrate IBA real-world
applicability, and show that our proposed techniques can further increase
attack performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with
  Global-Local Depth Normalization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06912v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06912v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiance fields have demonstrated impressive performance in synthesizing
novel views from sparse input views, yet prevailing methods suffer from high
training costs and slow inference speed. This paper introduces DNGaussian, a
depth-regularized framework based on 3D Gaussian radiance fields, offering
real-time and high-quality few-shot novel view synthesis at low costs. Our
motivation stems from the highly efficient representation and surprising
quality of the recent 3D Gaussian Splatting, despite it will encounter a
geometry degradation when input views decrease. In the Gaussian radiance
fields, we find this degradation in scene geometry primarily lined to the
positioning of Gaussian primitives and can be mitigated by depth constraint.
Consequently, we propose a Hard and Soft Depth Regularization to restore
accurate scene geometry under coarse monocular depth supervision while
maintaining a fine-grained color appearance. To further refine detailed
geometry reshaping, we introduce Global-Local Depth Normalization, enhancing
the focus on small local depth changes. Extensive experiments on LLFF, DTU, and
Blender datasets demonstrate that DNGaussian outperforms state-of-the-art
methods, achieving comparable or better results with significantly reduced
memory cost, a $25 \times$ reduction in training time, and over $3000 \times$
faster rendering speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024. Project page:
  https://fictionarry.github.io/DNGaussian/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DGC-GNN: Leveraging Geometry and Color Cues for Visual Descriptor-Free
  2D-3D Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhe Wang, Juho Kannala, Daniel Barath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matching 2D keypoints in an image to a sparse 3D point cloud of the scene
without requiring visual descriptors has garnered increased interest due to its
low memory requirements, inherent privacy preservation, and reduced need for
expensive 3D model maintenance compared to visual descriptor-based methods.
However, existing algorithms often compromise on performance, resulting in a
significant deterioration compared to their descriptor-based counterparts. In
this paper, we introduce DGC-GNN, a novel algorithm that employs a
global-to-local Graph Neural Network (GNN) that progressively exploits
geometric and color cues to represent keypoints, thereby improving matching
accuracy. Our procedure encodes both Euclidean and angular relations at a
coarse level, forming the geometric embedding to guide the point matching. We
evaluate DGC-GNN on both indoor and outdoor datasets, demonstrating that it not
only doubles the accuracy of the state-of-the-art visual descriptor-free
algorithm but also substantially narrows the performance gap between
descriptor-based and descriptor-free methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DemoCaricature: Democratising Caricature Generation with a Rough Sketch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dar-Yen Chen, Ayan Kumar Bhunia, Subhadeep Koley, Aneeshan Sain, Pinaki Nath Chowdhury, Yi-Zhe Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we democratise caricature generation, empowering individuals
to effortlessly craft personalised caricatures with just a photo and a
conceptual sketch. Our objective is to strike a delicate balance between
abstraction and identity, while preserving the creativity and subjectivity
inherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing
alongside single-image personalisation, selectively applying nuanced edits to
cross-attention layers for a seamless merge of identity and style.
Additionally, we propose Random Mask Reconstruction to enhance robustness,
directing the model to focus on distinctive identity and style features.
Crucially, our aim is not to replace artists but to eliminate accessibility
barriers, allowing enthusiasts to engage in the artistry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on
  Scene Graphs <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object rearrangement is pivotal in robotic-environment interactions,
representing a significant capability in embodied AI. In this paper, we present
SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme
with a scene graph as the scene representation. Unlike previous methods that
rely on either known goal priors or zero-shot large models, SG-Bot exemplifies
lightweight, real-time, and user-controllable characteristics, seamlessly
blending the consideration of commonsense knowledge with automatic generation
capabilities. SG-Bot employs a three-fold procedure--observation, imagination,
and execution--to adeptly address the task. Initially, objects are discerned
and extracted from a cluttered scene during the observation. These objects are
first coarsely organized and depicted within a scene graph, guided by either
commonsense or user-defined criteria. Then, this scene graph subsequently
informs a generative model, which forms a fine-grained goal scene considering
the shape information from the initial scene and object semantics. Finally, for
execution, the initial and envisioned goal scenes are matched to formulate
robotic action policies. Experimental results demonstrate that SG-Bot
outperforms competitors by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024 accepted. Project website:
  https://sites.google.com/view/sg-bot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-TPT: Calibrated Test-Time <span class="highlight-title">Prompt</span> Tuning for Vision-Language Models via
  Text Feature Dispersion <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep learning, test-time adaptation has gained attention as a method for
model fine-tuning without the need for labeled data. A prime exemplification is
the recently proposed test-time prompt tuning for large-scale vision-language
models such as CLIP. Unfortunately, these prompts have been mainly developed to
improve accuracy, overlooking the importance of calibration, which is a crucial
aspect for quantifying prediction uncertainty. However, traditional calibration
methods rely on substantial amounts of labeled data, making them impractical
for test-time scenarios. To this end, this paper explores calibration during
test-time prompt tuning by leveraging the inherent properties of CLIP. Through
a series of observations, we find that the prompt choice significantly affects
the calibration in CLIP, where the prompts leading to higher text feature
dispersion result in better-calibrated predictions. Introducing the Average
Text Feature Dispersion (ATFD), we establish its relationship with calibration
error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),
for optimizing prompts during test-time with enhanced calibration. Through
extensive experiments on different CLIP architectures and datasets, we show
that C-TPT can effectively improve the calibration of test-time prompt tuning
without needing labeled data. The code is publicly accessible at
https://github.com/hee-suk-yoon/C-TPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UCM-Net: A Lightweight and Efficient Solution for Skin Lesion
  Segmentation using MLP and CNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Yuan, Dongfang Zhao, Sos S. Agaian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is a significant public health problem, and computer-aided
diagnosis can help to prevent and treat it. A crucial step for computer-aided
diagnosis is accurately segmenting skin lesions in images, which allows for
lesion detection, classification, and analysis. However, this task is
challenging due to the diverse characteristics of lesions, such as appearance,
shape, size, color, texture, and location, as well as image quality issues like
noise, artifacts, and occlusions. Deep learning models have recently been
applied to skin lesion segmentation, but they have high parameter counts and
computational demands, making them unsuitable for mobile health applications.
To address this challenge, we propose UCM-Net, a novel, efficient, and
lightweight solution that integrates Multi-Layer Perceptions (MLP) and
Convolutional Neural Networks (CNN). Unlike conventional UNet architectures,
our UCMNet-Block reduces parameter overhead and enhances UCM-Net's learning
capabilities, leading to robust segmentation performance. We validate UCM-Net's
competitiveness through extensive experiments on PH2, isic2017 and isic2018
datasets. Remarkably, UCM-Net has less than 50KB parameters and less than 0.05
Giga-Operations Per Second (GLOPs), setting a new possible standard for
efficiency in skin lesion segmentation. The source code will be publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CEIMVEN: An Approach of Cutting Edge Implementation of Modified Versions
  of EfficientNet (V1-V2) Architecture for Breast Cancer Detection and
  Classification from Ultrasound Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheekar Banerjee, Md. Kamrul Hasan Monir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Undoubtedly breast cancer identifies itself as one of the most widespread and
terrifying cancers across the globe. Millions of women are getting affected
each year from it. Breast cancer remains the major one for being the reason of
largest number of demise of women. In the recent time of research, Medical
Image Computing and Processing has been playing a significant role for
detecting and classifying breast cancers from ultrasound images and mammograms,
along with the celestial touch of deep neural networks. In this research, we
focused mostly on our rigorous implementations and iterative result analysis of
different cutting-edge modified versions of EfficientNet architectures namely
EfficientNet-V1 (b0-b7) and EfficientNet-V2 (b0-b3) with ultrasound image,
named as CEIMVEN. We utilized transfer learning approach here for using the
pre-trained models of EfficientNet versions. We activated the hyper-parameter
tuning procedures, added fully connected layers, discarded the unprecedented
outliers and recorded the accuracy results from our custom modified
EfficientNet architectures. Our deep learning model training approach was
related to both identifying the cancer affected areas with region of interest
(ROI) techniques and multiple classifications (benign, malignant and normal).
The approximate testing accuracies we got from the modified versions of
EfficientNet-V1 (b0- 99.15%, b1- 98.58%, b2- 98.43%, b3- 98.01%, b4- 98.86%,
b5- 97.72%, b6- 97.72%, b7- 98.72%) and EfficientNet-V2 (b0- 99.29%, b1-
99.01%, b2- 98.72%, b3- 99.43%) are showing very bright future and strong
potentials of deep learning approach for the successful detection and
classification of breast cancers from the ultrasound images at a very early
stage. The code for this research is available here:
https://github.com/ac005sheekar/CEIMVEN-Cutting-Edge-Implementation-of-Modified-EfficientNet-V1-V2-for-BreastCancer-Detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CARZero: Cross-Attention Alignment for Radiology Zero-Shot
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lai, Qingsong Yao, Zihang Jiang, Rongsheng Wang, Zhiyang He, Xiaodong Tao, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Zero-Shot Learning in the medical domain has been driven
forward by using pre-trained models on large-scale image-text pairs, focusing
on image-text alignment. However, existing methods primarily rely on cosine
similarity for alignment, which may not fully capture the complex relationship
between medical images and reports. To address this gap, we introduce a novel
approach called Cross-Attention Alignment for Radiology Zero-Shot
Classification (CARZero). Our approach innovatively leverages cross-attention
mechanisms to process image and report features, creating a Similarity
Representation that more accurately reflects the intricate relationships in
medical semantics. This representation is then linearly projected to form an
image-text similarity matrix for cross-modality alignment. Additionally,
recognizing the pivotal role of prompt selection in zero-shot learning, CARZero
incorporates a Large Language Model-based prompt alignment strategy. This
strategy standardizes diverse diagnostic expressions into a unified format for
both training and inference phases, overcoming the challenges of manual prompt
design. Our approach is simple yet effective, demonstrating state-of-the-art
performance in zero-shot classification on five official chest radiograph
diagnostic test sets, including remarkable results on datasets with long-tail
distributions of rare diseases. This achievement is attributed to our new
image-text alignment strategy, which effectively addresses the complex
relationship between medical images and reports. Code and models are available
at https://github.com/laihaoran/CARZero.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DGL-GAN: Discriminator Guided Learning for GAN Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuesong Tian, Li Shen, Xiang Tian, Dacheng Tao, Zhifeng Li, Wei Liu, Yaowu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) with high computation costs, e.g.,
BigGAN and StyleGAN2, have achieved remarkable results in synthesizing
high-resolution images from random noise. Reducing the computation cost of GANs
while keeping generating photo-realistic images is a challenging field. In this
work, we propose a novel yet simple {\bf D}iscriminator {\bf G}uided {\bf
L}earning approach for compressing vanilla {\bf GAN}, dubbed {\bf DGL-GAN}.
Motivated by the phenomenon that the teacher discriminator may contain some
meaningful information about both real images and fake images, we merely
transfer the knowledge from the teacher discriminator via the adversarial
interaction between the teacher discriminator and the student generator. We
apply DGL-GAN to compress the two most representative large-scale vanilla GANs,
i.e., StyleGAN2 and BigGAN. Experiments show that DGL-GAN achieves
state-of-the-art (SOTA) results on both StyleGAN2 and BigGAN. Moreover, DGL-GAN
is also effective in boosting the performance of original uncompressed GANs.
Original uncompressed StyleGAN2 boosted with DGL-GAN achieves FID 2.65 on FFHQ,
which achieves a new state-of-the-art performance. Code and models are
available at \url{https://github.com/yuesongtian/DGL-GAN}
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Lu, Stephen Cranefield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The online community has increasingly been inundated by a toxic wave of
harmful comments. In response to this growing challenge, we introduce a
two-stage ultra-low-cost multimodal harmful behavior detection method designed
to identify harmful comments and images with high precision and recall rates.
We first utilize the CLIP-ViT model to transform tweets and images into
embeddings, effectively capturing the intricate interplay of semantic meaning
and subtle contextual clues within texts and images. Then in the second stage,
the system feeds these embeddings into a conventional machine learning
classifier like SVM or logistic regression, enabling the system to be trained
rapidly and to perform inference at an ultra-low cost. By converting tweets
into rich multimodal embeddings through the CLIP-ViT model and utilizing them
to train conventional machine learning classifiers, our system is not only
capable of detecting harmful textual information with near-perfect performance,
achieving precision and recall rates above 99\% but also demonstrates the
ability to zero-shot harmful images without additional training, thanks to its
multimodal embedding input. This capability empowers our system to identify
unseen harmful images without requiring extensive and costly image datasets.
Additionally, our system quickly adapts to new harmful content; if a new
harmful content pattern is identified, we can fine-tune the classifier with the
corresponding tweets' embeddings to promptly update the system. This makes it
well suited to addressing the ever-evolving nature of online harmfulness,
providing online communities with a robust, generalizable, and cost-effective
tool to safeguard their communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be appear in International Workshop on Coordination,
  Organizations, Institutions, Norms and Ethics for Governance of Multi-Agent
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complementary Recommendation in E-commerce: Definition, Approaches, and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyue Li, Zhijuan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, complementary recommendation has received extensive
attention in the e-commerce domain. In this paper, we comprehensively summarize
and compare 34 representative studies conducted between 2009 and 2024. Firstly,
we compare the data and methods used for modeling complementary relationships
between products, including simple complementarity and more complex scenarios
such as asymmetric complementarity, the coexistence of substitution and
complementarity relationships between products, and varying degrees of
complementarity between different pairs of products. Next, we classify and
compare the models based on the research problems of complementary
recommendation, such as diversity, personalization, and cold-start.
Furthermore, we provide a comparative analysis of experimental results from
different studies conducted on the same dataset, which helps identify the
strengths and weaknesses of the research. Compared to previous surveys, this
paper provides a more updated and comprehensive summary of the research,
discusses future research directions, and contributes to the advancement of
this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankingSHAP -- Listwise Feature Attribution Explanations for Ranking
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Heuss, Maarten de Rijke, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature attributions are a commonly used explanation type, when we want to
posthoc explain the prediction of a trained model. Yet, they are not very well
explored in IR. Importantly, feature attribution has rarely been rigorously
defined, beyond attributing the most important feature the highest value. What
it means for a feature to be more important than others is often left vague.
Consequently, most approaches focus on just selecting the most important
features and under utilize or even ignore the relative importance within
features. In this work, we rigorously define the notion of feature attribution
for ranking models, and list essential properties that a valid attribution
should have. We then propose RankingSHAP as a concrete instantiation of a
list-wise ranking attribution method. Contrary to current explanation
evaluation schemes that focus on selections, we propose two novel evaluation
paradigms for evaluating attributions over learning-to-rank models. We evaluate
RankingSHAP for commonly used learning-to-rank datasets to showcase the more
nuanced use of an attribution method while highlighting the limitations of
selection-based explanations. In a simulated experiment we design an
interpretable model to demonstrate how list-wise ranking attributes can be used
to investigate model decisions and evaluate the explanations qualitatively.
Because of the contrastive nature of the ranking task, our understanding of
ranking model decisions can substantially benefit from feature attribution
explanations like RankingSHAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-aware Dual-side Attribute-enhanced Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taotian Pang, Xingyu Lou, Fei Zhao, Zhen Wu, Kuiyao Dong, Qiuying Peng, Yue Qi, Xinyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \textit{Knowledge-aware} recommendation methods (KGR) based on \textit{graph
neural networks} (GNNs) and \textit{contrastive learning} (CL) have achieved
promising performance. However, they fall short in modeling fine-grained user
preferences and further fail to leverage the \textit{preference-attribute
connection} to make predictions, leading to sub-optimal performance. To address
the issue, we propose a method named \textit{\textbf{K}nowledge-aware
\textbf{D}ual-side \textbf{A}ttribute-enhanced \textbf{R}ecommendation} (KDAR).
Specifically, we build \textit{user preference representations} and
\textit{attribute fusion representations} upon the attribute information in
knowledge graphs, which are utilized to enhance \textit{collaborative
filtering} (CF) based user and item representations, respectively. To
discriminate the contribution of each attribute in these two types of
attribute-based representations, a \textit{multi-level collaborative alignment
contrasting} mechanism is proposed to align the importance of attributes with
CF signals. Experimental results on four benchmark datasets demonstrate the
superiority of KDAR over several state-of-the-art baselines. Further analyses
verify the effectiveness of our method. The code of KDAR is released at:
\href{https://github.com/TJTP/KDAR}{https://github.com/TJTP/KDAR}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Our Model Achieves Excellent Performance on MovieLens: What Does it
  Mean? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09985v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09985v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-chen Fan, Yitong Ji, Jie Zhang, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A typical benchmark dataset for recommender system (RecSys) evaluation
consists of user-item interactions generated on a platform within a time
period. The interaction generation mechanism partially explains why a user
interacts with (e.g., like, purchase, rate) an item, and the context of when a
particular interaction happened. In this study, we conduct a meticulous
analysis of the MovieLens dataset and explain the potential impact of using the
dataset for evaluating recommendation algorithms. We make a few main findings
from our analysis. First, there are significant differences in user
interactions at the different stages when a user interacts with the MovieLens
platform. The early interactions largely define the user portrait which affects
the subsequent interactions. Second, user interactions are highly affected by
the candidate movies that are recommended by the platform's internal
recommendation algorithm(s). Third, changing the order of user interactions
makes it more difficult for sequential algorithms to capture the progressive
interaction process. We further discuss the discrepancy between the interaction
generation mechanism that is employed by the MovieLens system and that of
typical real-world recommendation scenarios. In summary, the MovieLens platform
demonstrates an efficient and effective way of collecting user preferences to
address cold-starts. However, models that achieve excellent recommendation
accuracy on the MovieLens dataset may not demonstrate superior performance in
practice, for at least two kinds of differences: (i) the differences in the
contexts of user-item interaction generation, and (ii) the differences in user
knowledge about the item collections. While results on MovieLens can be useful
as a reference, they should not be solely relied upon as the primary
justification for the effectiveness of a recommendation system model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Spacerini, a tool that integrates the Pyserini toolkit for
reproducible information retrieval research with Hugging Face to enable the
seamless construction and deployment of interactive search engines. Spacerini
makes state-of-the-art sparse and dense retrieval models more accessible to
non-IR practitioners while minimizing deployment effort. This is useful for NLP
researchers who want to better understand and validate their research by
performing qualitative analyses of training corpora, for IR researchers who
want to demonstrate new retrieval models integrated into the growing Pyserini
ecosystem, and for third parties reproducing the work of other researchers.
Spacerini is open source and includes utilities for loading, preprocessing,
indexing, and deploying search engines locally and remotely. We demonstrate a
portfolio of 13 search engines created with Spacerini for different use cases.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Analog Dynamic Range Compressors using Deep Learning and
  State-space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhi Yin, Gang Cheng, Christian J. Steinmetz, Ruibin Yuan, Richard M. Stern, Roger B. Dannenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a novel approach for developing realistic digital models of
dynamic range compressors for digital audio production by analyzing their
analog prototypes. While realistic digital dynamic compressors are potentially
useful for many applications, the design process is challenging because the
compressors operate nonlinearly over long time scales. Our approach is based on
the structured state space sequence model (S4), as implementing the state-space
model (SSM) has proven to be efficient at learning long-range dependencies and
is promising for modeling dynamic range compressors. We present in this paper a
deep learning model with S4 layers to model the Teletronix LA-2A analog dynamic
range compressor. The model is causal, executes efficiently in real time, and
achieves roughly the same quality as previous deep-learning models but with
fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Neural Microcircuits as Building Blocks: Concept and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Walter, Shimeng Wu, Andy M. Tyrrell, Liam McDaid, Malachy McElholm, Nidhin Thandassery Sumithran, Jim Harkin, Martin A. Trefzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Neural Networks (ANNs) are one of the most widely employed forms
of bio-inspired computation. However the current trend is for ANNs to be
structurally homogeneous. Furthermore, this structural homogeneity requires the
application of complex training and learning tools that produce application
specific ANNs, susceptible to pitfalls such as overfitting. In this paper, an
new approach is explored, inspired by the role played in biology by Neural
Microcircuits, the so called ``fundamental processing elements'' of organic
nervous systems. How large neural networks, particularly Spiking Neural
Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs),
intended as off-the-shelf components, is articulated; the results of initial
work to produce a catalogue of such Microcircuits though the use of Novelty
Search is shown; followed by efforts to expand upon this initial work,
including a discussion of challenges uncovered during these efforts and
explorations of methods by which they might be overcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 31 figures, 3 tables, submitted to A-Life Journal for
  review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization on a Finer Scale: Bounded Local Subgradient Variation
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jelena Diakonikolas, Cristóbal Guzmán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate the study of nonsmooth optimization problems under bounded local
subgradient variation, which postulates bounded difference between
(sub)gradients in small local regions around points, in either average or
maximum sense. The resulting class of objective functions encapsulates the
classes of objective functions traditionally studied in optimization, which are
defined based on either Lipschitz continuity of the objective or
H\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class
contains functions that are neither Lipschitz continuous nor have a H\"{o}lder
continuous gradient. When restricted to the traditional classes of optimization
problems, the parameters defining the studied classes lead to more fine-grained
complexity bounds, recovering traditional oracle complexity bounds in the worst
case but generally leading to lower oracle complexity for functions that are
not ``worst case.'' Some highlights of our results are that: (i) it is possible
to obtain complexity results for both convex and nonconvex problems with the
(local or global) Lipschitz constant being replaced by a constant of local
subgradient variation and (ii) mean width of the subdifferential set around the
optima plays a role in the complexity of nonsmooth optimization, particularly
in parallel settings. A consequence of (ii) is that for any error parameter
$\epsilon > 0$, parallel oracle complexity of nonsmooth Lipschitz convex
optimization is lower than its sequential oracle complexity by a factor
$\tilde{\Omega}\big(\frac{1}{\epsilon}\big)$ whenever the objective function is
piecewise linear with polynomially many pieces in the input size. This is
particularly surprising as existing parallel complexity lower bounds are based
on such classes of functions. The seeming contradiction is resolved by
considering the region in which the algorithm is allowed to query the
objective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Li, Zhiling Lan, Michael E. Papka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of high-performance computing (HPC), there has been recent
exploration into the use of deep reinforcement learning for cluster scheduling
(DRL scheduling), which has demonstrated promising outcomes. However, a
significant challenge arises from the lack of interpretability in deep neural
networks (DNN), rendering them as black-box models to system managers. This
lack of model interpretability hinders the practical deployment of DRL
scheduling. In this work, we present a framework called IRL (Interpretable
Reinforcement Learning) to address the issue of interpretability of DRL
scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a
decision tree by utilizing imitation learning. Unlike DNN, decision tree models
are non-parametric and easily comprehensible to humans. To extract an effective
and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger)
algorithm and introduces the notion of critical state to prune the derived
decision tree. Through trace-based experiments, we demonstrate that IRL is
capable of converting a black-box DNN policy into an interpretable rulebased
decision tree while maintaining comparable scheduling performance.
Additionally, IRL can contribute to the setting of rewards in DRL scheduling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Evolution of Football Betting- A Machine Learning Approach to Match
  Outcome Forecasting and Bookmaker Odds Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Purnachandra Mandadapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the significant history of professional football and the
betting industry, tracing its evolution from clandestine beginnings to a
lucrative multi-million-pound enterprise. Initiated by the legalization of
gambling in 1960 and complemented by advancements in football data gathering
pioneered by Thorold Charles Reep, the symbiotic relationship between these
sectors has propelled rapid growth and innovation. Over the past six decades,
both industries have undergone radical transformations, with data collection
methods evolving from rudimentary notetaking to sophisticated technologies such
as high-definition cameras and Artificial Intelligence (AI)-driven analytics.
Therefore, the primary aim of this study is to utilize Machine Learning (ML)
algorithms to forecast premier league football match outcomes. By analyzing
historical data and investigating the significance of various features, the
study seeks to identify the most effective predictive models and discern key
factors influencing match results. Additionally, the study aims to utilize
these forecasting to inform the establishment of bookmaker odds, providing
insights into the impact of different variables on match outcomes. By
highlighting the potential for informed decision-making in sports forecasting
and betting, this study opens up new avenues for research and practical
applications in the domain of sports analytics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhui Xu, Fuxun Yu, Zirui Xu, Nathan Inkawhich, Xiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research underscores the pivotal role of the Out-of-Distribution (OOD)
feature representation field scale in determining the efficacy of models in OOD
detection. Consequently, the adoption of model ensembles has emerged as a
prominent strategy to augment this feature representation field, capitalizing
on anticipated model diversity.
  However, our introduction of novel qualitative and quantitative model
ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and
the Self-Coupling Index, reveals a critical drawback in existing ensemble
methods. We find that these methods incorporate weights that are
affine-transformable, exhibiting limited variability and thus failing to
achieve the desired diversity in feature representation.
  To address this limitation, we elevate the dimensions of traditional model
ensembles, incorporating various factors such as different weight
initializations, data holdout, etc., into distinct supervision tasks. This
innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages
diverse training tasks to generate distinct comprehensions of the data and
labels, thereby extending the feature representation field.
  Our experimental results demonstrate the superior performance of the MC
Ensemble strategy in OOD detection compared to both the naive Deep Ensemble
method and a standalone model of comparable size. This underscores the
effectiveness of our proposed approach in enhancing the model's capability to
detect instances outside its training distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated
  Synthesis <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atefeh Khoshkhahtinat, Ali Zafari, Piyush M. Mehta, Nasser M. Nasrabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While replacing Gaussian decoders with a conditional diffusion model enhances
the perceptual quality of reconstructions in neural image compression, their
lack of inductive bias for image data restricts their ability to achieve
state-of-the-art perceptual levels. To address this limitation, we adopt a
non-isotropic diffusion model at the decoder side. This model imposes an
inductive bias aimed at distinguishing between frequency contents, thereby
facilitating the generation of high-quality images. Moreover, our framework is
equipped with a novel entropy model that accurately models the probability
distribution of latent representation by exploiting spatio-channel correlations
in latent space, while accelerating the entropy decoding step. This
channel-wise entropy model leverages both local and global spatial contexts
within each channel chunk. The global spatial context is built upon the
Transformer, which is specifically designed for image compression tasks. The
designed Transformer employs a Laplacian-shaped positional encoding, the
learnable parameters of which are adaptively adjusted for each channel cluster.
Our experiments demonstrate that our proposed framework yields better
perceptual quality compared to cutting-edge generative-based codecs, and the
proposed entropy model contributes to notable bitrate savings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Sequence-to-Sequence Models for Abstractive Text Summarization
  Using Meta Heuristic Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Saxena, Ashutosh Ranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As human society transitions into the information age, reduction in our
attention span is a contingency, and people who spend time reading lengthy news
articles are decreasing rapidly and the need for succinct information is higher
than ever before. Therefore, it is essential to provide a quick overview of
important news by concisely summarizing the top news article and the most
intuitive headline. When humans try to make summaries, they extract the
essential information from the source and add useful phrases and grammatical
annotations from the original extract. Humans have a unique ability to create
abstractions. However, automatic summarization is a complicated problem to
solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive
text summarization has been ascending as far as prevalence. Numerous innovative
strategies have been proposed to develop the current seq2seq models further,
permitting them to handle different issues like saliency, familiarity, and
human lucidness and create excellent synopses. In this article, we aimed toward
enhancing the present architectures and models for abstractive text
summarization. The modifications have been aimed at fine-tuning
hyper-parameters, attempting specific encoder-decoder combinations. We examined
many experiments on an extensively used CNN/DailyMail dataset to check the
effectiveness of various models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partially Blinded Unlearning: Class Unlearning for Deep Networks a
  Bayesian Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhodip Panda, Shashwat Sourav, Prathosh A. P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to adhere to regulatory standards governing individual data privacy
and safety, machine learning models must systematically eliminate information
derived from specific subsets of a user's training data that can no longer be
utilized. The emerging discipline of Machine Unlearning has arisen as a pivotal
area of research, facilitating the process of selectively discarding
information designated to specific sets or classes of data from a pre-trained
model, thereby eliminating the necessity for extensive retraining from scratch.
The principal aim of this study is to formulate a methodology tailored for the
purposeful elimination of information linked to a specific class of data from a
pre-trained classification network. This intentional removal is crafted to
degrade the model's performance specifically concerning the unlearned data
class while concurrently minimizing any detrimental impacts on the model's
performance in other classes. To achieve this goal, we frame the class
unlearning problem from a Bayesian perspective, which yields a loss function
that minimizes the log-likelihood associated with the unlearned data with a
stability regularization in parameter space. This stability regularization
incorporates Mohalanobis distance with respect to the Fisher Information matrix
and $l_2$ distance from the pre-trained model parameters. Our novel approach,
termed \textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing
state-of-the-art class unlearning methods, demonstrating superior
effectiveness. Notably, PBU achieves this efficacy without requiring awareness
of the entire training dataset but only to the unlearned data points, marking a
distinctive feature of its performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Equivalency, Substitutability, and Flexibility of Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che-Jui Chang, Danrui Li, Seonghyeon Moon, Mubbasir Kapadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study, from an empirical standpoint, the efficacy of synthetic data in
real-world scenarios. Leveraging synthetic data for training perception models
has become a key strategy embraced by the community due to its efficiency,
scalability, perfect annotations, and low costs. Despite proven advantages, few
studies put their stress on how to efficiently generate synthetic datasets to
solve real-world problems and to what extent synthetic data can reduce the
effort for real-world data collection. To answer the questions, we
systematically investigate several interesting properties of synthetic data --
the equivalency of synthetic data to real-world data, the substitutability of
synthetic data for real data, and the flexibility of synthetic data generators
to close up domain gaps. Leveraging the M3Act synthetic data generator, we
conduct experiments on DanceTrack and MOT17. Our results suggest that synthetic
data not only enhances model performance but also demonstrates substitutability
for real data, with 60% to 80% replacement without performance loss. In
addition, our study of the impact of synthetic data distributions on downstream
performance reveals the importance of flexible data generators in narrowing
domain gaps for improved model adaptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An early warning indicator trained on stochastic disease-spreading
  models with different noises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit K. Chakraborty, Shan Gao, Reza Miry, Pouria Ramazi, Russell Greiner, Mark A. Lewis, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The timely detection of disease outbreaks through reliable early warning
signals (EWSs) is indispensable for effective public health mitigation
strategies. Nevertheless, the intricate dynamics of real-world disease spread,
often influenced by diverse sources of noise and limited data in the early
stages of outbreaks, pose a significant challenge in developing reliable EWSs,
as the performance of existing indicators varies with extrinsic and intrinsic
noises. Here, we address the challenge of modeling disease when the
measurements are corrupted by additive white noise, multiplicative
environmental noise, and demographic noise into a standard epidemic
mathematical model. To navigate the complexities introduced by these noise
sources, we employ a deep learning algorithm that provides EWS in infectious
disease outbreak by training on noise-induced disease-spreading models. The
indicator's effectiveness is demonstrated through its application to real-world
COVID-19 cases in Edmonton and simulated time series derived from diverse
disease spread models affected by noise. Notably, the indicator captures an
impending transition in a time series of disease outbreaks and outperforms
existing indicators. This study contributes to advancing early warning
capabilities by addressing the intricate dynamics inherent in real-world
disease spread, presenting a promising avenue for enhancing public health
preparedness and response efforts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoverUp: Coverage-Guided LLM-Based Test Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Altmayer Pizzorno, Emery D. Berger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents CoverUp, a novel system that drives the generation of
high-coverage Python regression tests via a combination of coverage analysis
and large-language models (LLMs). CoverUp iteratively improves coverage,
interleaving coverage analysis with dialogs with the LLM to focus its attention
on as yet uncovered lines and branches. The resulting test suites significantly
improve coverage over the current state of the art: compared to CodaMosa, a
hybrid LLM / search-based software testing system, CoverUp substantially
improves coverage across the board. On a per-module basis, CoverUp achieves
median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and
line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative,
coverage-guided approach is crucial to its effectiveness, contributing to
nearly half of its successes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic construction of continuous-time neural networks for linear
  dynamical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Datar, Adwait Datar, Felix Dietrich, Wil Schilders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering a suitable neural network architecture for modeling complex
dynamical systems poses a formidable challenge, often involving extensive trial
and error and navigation through a high-dimensional hyper-parameter space. In
this paper, we discuss a systematic approach to constructing neural
architectures for modeling a subclass of dynamical systems, namely, Linear
Time-Invariant (LTI) systems. We use a variant of continuous-time neural
networks in which the output of each neuron evolves continuously as a solution
of a first-order or second-order Ordinary Differential Equation (ODE). Instead
of deriving the network architecture and parameters from data, we propose a
gradient-free algorithm to compute sparse architecture and network parameters
directly from the given LTI system, leveraging its properties. We bring forth a
novel neural architecture paradigm featuring horizontal hidden layers and
provide insights into why employing conventional neural architectures with
vertical hidden layers may not be favorable. We also provide an upper bound on
the numerical errors of our neural networks. Finally, we demonstrate the high
accuracy of our constructed networks on three numerical examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 25 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI
  Classification in Alzheimer Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaojie Li, Haichen Qu, Xinqi Dong, Bo Dang, Hengyi Zang, Yulu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring the application of deep learning technologies in the field of
medical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique
perspective for observing and diagnosing complex neurodegenerative diseases
such as Alzheimer Disease (AD). With advancements in deep learning,
particularly in Convolutional Neural Networks (CNNs) and the Xception network
architecture, we are now able to analyze and classify vast amounts of MRI data
with unprecedented accuracy. The progress of this technology not only enhances
our understanding of brain structural changes but also opens up new avenues for
monitoring disease progression through non-invasive means and potentially
allows for precise diagnosis in the early stages of the disease.
  This study aims to classify MRI images using deep learning models to identify
different stages of Alzheimer Disease through a series of innovative data
processing and model construction steps. Our experimental results show that the
deep learning framework based on the Xception model achieved a 99.6% accuracy
rate in the multi-class MRI image classification task, demonstrating its
potential application value in assistive diagnosis. Future research will focus
on expanding the dataset, improving model interpretability, and clinical
validation to further promote the application of deep learning technology in
the medical field, with the hope of bringing earlier diagnosis and more
personalized treatment plans to Alzheimer Disease patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence analysis of OT-Flow for sample generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Jing, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models aim to learn the underlying distribution of data and
generate new ones. Despite the diversity of generative models and their
high-quality generation performance in practice, most of them lack rigorous
theoretical convergence proofs. In this work, we aim to establish some
convergence results for OT-Flow, one of the deep generative models. First, by
reformulating the framework of OT-Flow model, we establish the
$\Gamma$-convergence of the formulation of OT-flow to the corresponding optimal
transport (OT) problem as the regularization term parameter $\alpha$ goes to
infinity. Second, since the loss function will be approximated by Monte Carlo
method in training, we established the convergence between the discrete loss
function and the continuous one when the sample number $N$ goes to infinity as
well. Meanwhile, the approximation capability of the neural network provides an
upper bound for the discrete loss function of the minimizers. The proofs in
both aspects provide convincing assurances for OT-Flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Discrete to Continuous: Deep Fair Clustering With Transferable
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of deep fair clustering, which partitions data into
clusters via the representations extracted by deep neural networks while hiding
sensitive data attributes. To achieve fairness, existing methods present a
variety of fairness-related objective functions based on the group fairness
criterion. However, these works typically assume that the sensitive attributes
are discrete and do not work for continuous sensitive variables, such as the
proportion of the female population in an area. Besides, the potential of the
representations learned from clustering tasks to improve performance on other
tasks is ignored by existing works. In light of these limitations, we propose a
flexible deep fair clustering method that can handle discrete and continuous
sensitive attributes simultaneously. Specifically, we design an information
bottleneck style objective function to learn fair and clustering-friendly
representations. Furthermore, we explore for the first time the transferability
of the extracted representations to other downstream tasks. Unlike existing
works, we impose fairness at the representation level, which could guarantee
fairness for the transferred task regardless of clustering results. To verify
the effectiveness of the proposed method, we perform extensive experiments on
datasets with discrete and continuous sensitive attributes, demonstrating the
advantage of our method in comparison with state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logic-based Explanations for Linear Support Vector Classifiers with
  Reject Option 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mateus Rocha Filho, Thiago Alves Rocha, Reginaldo Pereira Fernandes Ribeiro, Ajalmar Rêgo da Rocha Neto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model
for linear classification problems. It can be used in conjunction with a reject
option strategy to reject instances that are hard to correctly classify and
delegate them to a specialist. This further increases the confidence of the
model. Given this, obtaining an explanation of the cause of rejection is
important to not blindly trust the obtained results. While most of the related
work has developed means to give such explanations for machine learning models,
to the best of our knowledge none have done so for when reject option is
present. We propose a logic-based approach with formal guarantees on the
correctness and minimality of explanations for linear SVCs with reject option.
We evaluate our approach by comparing it to Anchors, which is a heuristic
algorithm for generating explanations. Obtained results show that our proposed
method gives shorter explanations with reduced time cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, submitted to BRACIS 2023 (Brazilian Conference on
  Intelligent Systems), accepted version published in Intelligent Systems,
  LNCS, vol 14195</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subspace Defense: Discarding Adversarial Perturbations by Learning a
  Subspace for Clean Signals <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zheng, Yuhao Zhou, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks
that place carefully crafted perturbations on normal examples to fool DNNs. To
better understand such attacks, a characterization of the features carried by
adversarial examples is needed. In this paper, we tackle this challenge by
inspecting the subspaces of sample features through spectral analysis. We first
empirically show that the features of either clean signals or adversarial
perturbations are redundant and span in low-dimensional linear subspaces
respectively with minimal overlap, and the classical low-dimensional subspace
projection can suppress perturbation features out of the subspace of clean
signals. This makes it possible for DNNs to learn a subspace where only
features of clean signals exist while those of perturbations are discarded,
which can facilitate the distinction of adversarial examples. To prevent the
residual perturbations that is inevitable in subspace learning, we propose an
independence criterion to disentangle clean signals from perturbations.
Experimental results show that the proposed strategy enables the model to
inherently suppress adversaries, which not only boosts model robustness but
also motivates new directions of effective adversarial defense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analytic Solution to Covariance Propagation in Neural Networks <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oren Wright, Yorie Nakahira, José M. F. Moura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification of neural networks is critical to measuring the
reliability and robustness of deep learning systems. However, this often
involves costly or inaccurate sampling methods and approximations. This paper
presents a sample-free moment propagation technique that propagates mean
vectors and covariance matrices across a network to accurately characterize the
input-output distributions of neural networks. A key enabler of our technique
is an analytic solution for the covariance of random variables passed through
nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide
applicability and merits of the proposed technique are shown in experiments
analyzing the input-output distributions of trained neural networks and
training Bayesian neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Masked Model is All You Need for Sensor Fault Detection, Isolation
  and Accommodation <span class="chip">IJCNN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Fu, Weizhong Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and reliable sensor measurements are critical for ensuring the
safety and longevity of complex engineering systems such as wind turbines. In
this paper, we propose a novel framework for sensor fault detection, isolation,
and accommodation (FDIA) using masked models and self-supervised learning. Our
proposed approach is a general time series modeling approach that can be
applied to any neural network (NN) model capable of sequence modeling, and
captures the complex spatio-temporal relationships among different sensors.
During training, the proposed masked approach creates a random mask, which acts
like a fault, for one or more sensors, making the training and inference task
unified: finding the faulty sensors and correcting them. We validate our
proposed technique on both a public dataset and a real-world dataset from GE
offshore wind turbines, and demonstrate its effectiveness in detecting,
diagnosing and correcting sensor faults. The masked model not only simplifies
the overall FDIA pipeline, but also outperforms existing approaches. Our
proposed technique has the potential to significantly improve the accuracy and
reliability of sensor measurements in complex engineering systems in real-time,
and could be applied to other types of sensors and engineering systems in the
future. We believe that our proposed framework can contribute to the
development of more efficient and effective FDIA techniques for a wide range of
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 2024 International Joint Conference on Neural
  Networks (IJCNN 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Consumer IoT Traffic: Security and Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Jia, Yuxin Song, Zihou Liu, Qingyin Tan, Fangming Wang, Yu Zhang, Zheli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the past few years, the Consumer Internet of Things (CIoT) has entered
public lives. While CIoT has improved the convenience of people's daily lives,
it has also brought new security and privacy concerns. In this survey, we try
to figure out what researchers can learn about the security and privacy of CIoT
by traffic analysis, a popular method in the security community. From the
security and privacy perspective, this survey seeks out the new characteristics
in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic
analysis, and the challenges yet to be solved. We collected 310 papers from
January 2018 to December 2023 related to CIoT traffic analysis from the
security and privacy perspective and summarized the process of CIoT traffic
analysis in which the new characteristics of CIoT are identified. Then, we
detail existing works based on five application goals: device fingerprinting,
user activity inference, malicious traffic analysis, security analysis, and
measurement. At last, we discuss the new challenges and future research
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural
  Network Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego A. de Aguiar, Hugo L. França, Cassio M. Oishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks in fluid mechanics offer an efficient approach for exploring
complex flows, including multiphase and free surface flows. The recurrent
neural network, particularly the Long Short-Term Memory (LSTM) model, proves
attractive for learning mappings from transient inputs to dynamic outputs. This
study applies LSTM to predict transient and static outputs for fluid flows
under surface tension effects. Specifically, we explore two distinct droplet
dynamic scenarios: droplets with diverse initial shapes impacting with solid
surfaces, as well as the coalescence of two droplets following collision. Using
only dimensionless numbers and geometric time series data from numerical
simulations, LSTM predicts the energy budget. The marker-and-cell
front-tracking methodology combined with a marker-and-cell finite-difference
strategy is adopted for simulating the droplet dynamics. Using a recurrent
neural network (RNN) architecture fed with time series data derived from
geometrical parameters, as for example droplet diameter variation, our study
shows the accuracy of our approach in predicting energy budgets, as for
instance the kinetic, dissipation, and surface energy trends, across a range of
Reynolds and Weber numbers in droplet dynamic problems. Finally, a two-phase
sequential neural network using only geometric data, which is readily available
in experimental settings, is employed to predict the energies and then use them
to estimate static parameters, such as the Reynolds and Weber numbers. While
our methodology has been primarily validated with simulation data, its
adaptability to experimental datasets is a promising avenue for future
exploration. We hope that our strategy can be useful for diverse applications,
spanning from inkjet printing to combustion engines, where the prediction of
energy budgets or dissipation energies is crucial.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CFAT: Unleashing TriangularWindows for Image Super-resolution <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhisek Ray, Gaurav Kumar, Maheshkumar H. Kolekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have revolutionized the field of image
super-resolution (SR) by harnessing their inherent ability to capture complex
contextual features. The overlapping rectangular shifted window technique used
in transformer architecture nowadays is a common practice in super-resolution
models to improve the quality and robustness of image upscaling. However, it
suffers from distortion at the boundaries and has limited unique shifting
modes. To overcome these weaknesses, we propose a non-overlapping triangular
window technique that synchronously works with the rectangular one to mitigate
boundary-level distortion and allows the model to access more unique sifting
modes. In this paper, we propose a Composite Fusion Attention Transformer
(CFAT) that incorporates triangular-rectangular window-based local attention
with a channel-based global attention technique in image super-resolution. As a
result, CFAT enables attention mechanisms to be activated on more image pixels
and captures long-range, multi-scale features to improve SR performance. The
extensive experimental results and ablation study demonstrate the effectiveness
of CFAT in the SR domain. Our proposed model shows a significant 0.7 dB
performance improvement over other state-of-the-art SR architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing of Graph Foundation Models: A
  Knowledge-Based Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwen Zhao, Yuhua Li, Yixiong Zou, Ruixuan Li, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph self-supervised learning is now a go-to method for pre-training graph
foundation models, including graph neural networks, graph transformers, and
more recent large language model (LLM)-based graph models. There is a wide
variety of knowledge patterns embedded in the structure and properties of
graphs which may be used for pre-training, but we lack a systematic overview of
self-supervised pre-training tasks from the perspective of graph knowledge. In
this paper, we comprehensively survey and analyze the pre-training tasks of
graph foundation models from a knowledge-based perspective, consisting of
microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global
structure, etc). It covers a total of 9 knowledge categories and 25
pre-training tasks, as well as various downstream task adaptation strategies.
Furthermore, an extensive list of the related papers with detailed metadata is
provided at https://github.com/Newiz430/Pretext.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complementary Recommendation in E-commerce: Definition, Approaches, and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyue Li, Zhijuan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, complementary recommendation has received extensive
attention in the e-commerce domain. In this paper, we comprehensively summarize
and compare 34 representative studies conducted between 2009 and 2024. Firstly,
we compare the data and methods used for modeling complementary relationships
between products, including simple complementarity and more complex scenarios
such as asymmetric complementarity, the coexistence of substitution and
complementarity relationships between products, and varying degrees of
complementarity between different pairs of products. Next, we classify and
compare the models based on the research problems of complementary
recommendation, such as diversity, personalization, and cold-start.
Furthermore, we provide a comparative analysis of experimental results from
different studies conducted on the same dataset, which helps identify the
strengths and weaknesses of the research. Compared to previous surveys, this
paper provides a more updated and comprehensive summary of the research,
discusses future research directions, and contributes to the advancement of
this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSHPool: The Separated Subgraph-based Hierarchical Pooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Xu, Lixin Cui, Yue Wang, Hangyuan Du, Lu Bai, Edwin R. Hancock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a novel local graph pooling method, namely the
Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph
classification. To this end, we commence by assigning the nodes of a sample
graph into different clusters, resulting in a family of separated subgraphs. We
individually employ a local graph convolution units as the local structure to
further compress each subgraph into a coarsened node, transforming the original
graph into a coarsened graph. Since these subgraphs are separated by different
clusters and the structural information cannot be propagated between them, the
local convolution operation can significantly avoid the over-smoothing problem
arising in most existing Graph Neural Networks (GNNs). By hierarchically
performing the proposed procedures on the resulting coarsened graph, the
proposed SSHPool can effectively extract the hierarchical global feature of the
original graph structure, encapsulating rich intrinsic structural
characteristics. Furthermore, we develop an end-to-end GNN framework associated
with the proposed SSHPool module for graph classification. Experimental results
demonstrate the superior performance of the proposed model on real-world
datasets, significantly outperforming state-of-the-art GNN methods in terms of
the classification accuracies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Runtime Monitoring and Fault Detection for Neural Network-Controlled
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianglin Lan, Siyuan Zhan, Ron Patton, Xianxian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an emerging trend in applying deep learning methods to control
complex nonlinear systems. This paper considers enhancing the runtime safety of
nonlinear systems controlled by neural networks in the presence of disturbance
and measurement noise. A robustly stable interval observer is designed to
generate sound and precise lower and upper bounds for the neural network,
nonlinear function, and system state. The obtained interval is utilised to
monitor the real-time system safety and detect faults in the system outputs or
actuators. An adaptive cruise control vehicular system is simulated to
demonstrate effectiveness of the proposed design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SAFEPROCESS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AKBR: Learning Adaptive Kernel-based Representations for Graph
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feifei Qian, Lixin Cui, Yue Wang, Hangyuan Du, Lu Bai, Edwin R. Hancock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new model to learn Adaptive Kernel-based
Representations (AKBR) for graph classification. Unlike state-of-the-art
R-convolution graph kernels that are defined by merely counting any pair of
isomorphic substructures between graphs and cannot provide an end-to-end
learning mechanism for the classifier, the proposed AKBR approach aims to
define an end-to-end representation learning model to construct an adaptive
kernel matrix for graphs. To this end, we commence by leveraging a novel
feature-channel attention mechanism to capture the interdependencies between
different substructure invariants of original graphs. The proposed AKBR model
can thus effectively identify the structural importance of different
substructures, and compute the R-convolution kernel between pairwise graphs
associated with the more significant substructures specified by their
structural attentions. Since each row of the resulting kernel matrix can be
theoretically seen as the embedding vector of a sample graph, the proposed AKBR
model is able to directly employ the resulting kernel matrix as the graph
feature matrix and input it into the classifier for classification (i.e., the
SoftMax layer), naturally providing an end-to-end learning architecture between
the kernel computation as well as the classifier. Experimental results show
that the proposed AKBR model outperforms existing state-of-the-art graph
kernels and deep learning methods on standard graph benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Codesign of Scheduling and Parallelization for Large Model Training in
  Heterogeneous Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Xue, Weihao Cui, Han Zhao, Quan Chen, Shulai Zhang, Pengyu Yang, Jing Yang, Shaobo Li, Minyi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint consideration of scheduling and adaptive parallelism offers great
opportunities for improving the training efficiency of large models on
heterogeneous GPU clusters. However, integrating adaptive parallelism into a
cluster scheduler expands the cluster scheduling space. The new space is the
product of the original scheduling space and the parallelism exploration space
of adaptive parallelism (also a product of pipeline, data, and tensor
parallelism). The exponentially enlarged scheduling space and ever-changing
optimal parallelism plan from adaptive parallelism together result in the
contradiction between low-overhead and accurate performance data acquisition
for efficient cluster scheduling. This paper presents Crius, a training system
for efficiently scheduling multiple large models with adaptive parallelism in a
heterogeneous cluster. Crius proposes a novel scheduling granularity called
Cell. It represents a job with deterministic resources and pipeline stages. The
exploration space of Cell is shrunk to the product of only data and tensor
parallelism, thus exposing the potential for accurate and low-overhead
performance estimation. Crius then accurately estimates Cells and efficiently
schedules training jobs. When a Cell is selected as a scheduling choice, its
represented job runs with the optimal parallelism plan explored. Experimental
results show that Crius reduces job completion time by up to 48.9% and
schedules large models with up to 1.49x cluster throughput improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Multi-Frame Neural Scene Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongrui Liu, Daqi Liu, Xueqian Li, Sihao Lin, Hongwei xie, Bing Wang, Xiaojun Chang, Lei Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown
remarkable adaptability in the context of large out-of-distribution autonomous
driving. Despite their success, the underlying reasons for their astonishing
generalization capabilities remain unclear. Our research addresses this gap by
examining the generalization capabilities of NSFP through the lens of uniform
stability, revealing that its performance is inversely proportional to the
number of input point clouds. This finding sheds light on NSFP's effectiveness
in handling large-scale point cloud scene flow estimation tasks. Motivated by
such theoretical insights, we further explore the improvement of scene flow
estimation by leveraging historical point clouds across multiple frames, which
inherently increases the number of point clouds. Consequently, we propose a
simple and effective method for multi-frame point cloud scene flow estimation,
along with a theoretical evaluation of its generalization abilities. Our
analysis confirms that the proposed method maintains a limited generalization
error, suggesting that adding multiple frames to the scene flow optimization
process does not detract from its generalizability. Extensive experimental
results on large-scale autonomous driving Waymo Open and Argoverse lidar
datasets demonstrate that the proposed method achieves state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opportunities and challenges in the application of large artificial
  intelligence models in radiology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangrui Pan, Zhenyu Zhao, Ying Lu, Kewei Tang, Liyong Fu, Qingchun Liang, Shaoliang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influenced by ChatGPT, artificial intelligence (AI) large models have
witnessed a global upsurge in large model research and development. As people
enjoy the convenience by this AI large model, more and more large models in
subdivided fields are gradually being proposed, especially large models in
radiology imaging field. This article first introduces the development history
of large models, technical details, workflow, working principles of multimodal
large models and working principles of video generation large models. Secondly,
we summarize the latest research progress of AI large models in radiology
education, radiology report generation, applications of unimodal and multimodal
radiology. Finally, this paper also summarizes some of the challenges of large
AI models in radiology, with the aim of better promoting the rapid revolution
in the field of radiography.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Transformer</span> approach for Electricity Price Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Llorente Gonzalez, Jose Portela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to electricity price forecasting (EPF)
using a pure Transformer model. As opposed to other alternatives, no other
recurrent network is used in combination to the attention mechanism. Hence,
showing that the attention layer is enough for capturing the temporal patterns.
The paper also provides fair comparison of the models using the open-source EPF
toolbox and provide the code to enhance reproducibility and transparency in EPF
research. The results show that the Transformer model outperforms traditional
methods, offering a promising solution for reliable and sustainable power
system operation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Label <span class="highlight-title">Dataset</span> of French Fake News: Human and Machine Insights <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Icard, François Maine, Morgane Casanova, Géraud Faye, Julien Chanson, Guillaume Gadek, Ghislain Atemezing, François Bancilhon, Paul Égré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of
French press considered unreliable by expert agencies, annotated using 11
labels by 8 annotators. By collecting more labels than usual, by more
annotators than is typically done, we can identify features that humans
consider as characteristic of fake news, and compare them to the predictions of
automated classifiers. We present a topic and genre analysis using Gate Cloud,
indicative of the prevalence of satire-like text in the corpus. We then use the
subjectivity analyzer VAGO, and a neural version of it, to clarify the link
between ascriptions of the label Subjective and ascriptions of the label Fake
News. The annotated dataset is available online at the following url:
https://github.com/obs-info/obsinfox
  Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion,
Exaggeration, French Press
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper to appear in the Proceedings of the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs as Compiler for Arabic Programming Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serry Sibaee, Omar Najar, Lahouri Ghouti, Anis Koubaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce APL (Arabic Programming Language) that uses Large
language models (LLM) as semi-compiler to covert Arabic text code to python
code then run the code. Designing a full pipeline from the structure of the APL
text then a prompt (using prompt engineering) then running the prodcued python
code using PyRunner. This project has a three parts first python library, a
playground with simple interface and this research paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral
  Evolution History 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xu, Weiran Shen, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional imitation learning focuses on modeling the behavioral mechanisms
of experts, which requires a large amount of interaction history generated by
some fixed expert. However, in many streaming applications, such as streaming
recommender systems, online decision-makers typically engage in online learning
during the decision-making process, meaning that the interaction history
generated by online decision-makers includes their behavioral evolution from
novice expert to experienced expert. This poses a new challenge for existing
imitation learning approaches that can only utilize data from experienced
experts. To address this issue, this paper proposes an inverse batched
contextual bandit (IBCB) framework that can efficiently perform estimations of
environment reward parameters and learned policy based on the expert's
behavioral evolution history. Specifically, IBCB formulates the inverse problem
into a simple quadratic programming problem by utilizing the behavioral
evolution history of the batched contextual bandit with inaccessible rewards.
We demonstrate that IBCB is a unified framework for both deterministic and
randomized bandit policies. The experimental results indicate that IBCB
outperforms several existing imitation learning algorithms on synthetic and
real-world data and significantly reduces running time. Additionally, empirical
analyses reveal that IBCB exhibits better out-of-distribution generalization
and is highly effective in learning the bandit policy from the interaction
history of novice experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifold Regularization Classification Model Based On Improved Diffusion
  Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfu Guo, Wencheng Zou, Zeyu Zhang, Shuishan Zhang, Ruitong Wang, Jintao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manifold regularization model is a semi-supervised learning model that
leverages the geometric structure of a dataset, comprising a small number of
labeled samples and a large number of unlabeled samples, to generate
classifiers. However, the original manifold norm limits the performance of
models to local regions. To address this limitation, this paper proposes an
approach to improve manifold regularization based on a label propagation model.
We initially enhance the probability transition matrix of the diffusion map
algorithm, which can be used to estimate the Neumann heat kernel, enabling it
to accurately depict the label propagation process on the manifold. Using this
matrix, we establish a label propagation function on the dataset to describe
the distribution of labels at different time steps. Subsequently, we extend the
label propagation function to the entire data manifold. We prove that the
extended label propagation function converges to a stable distribution after a
sufficiently long time and can be considered as a classifier. Building upon
this concept, we propose a viable improvement to the manifold regularization
model and validate its superiority through experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 24figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangjoon Park, Yongsung Kwon, Hyungjoon Soh, Mi Jin Lee, Seung-Woo Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting temporal patterns across various domains poses significant
challenges due to their nuanced and often nonlinear trajectories. To address
this challenge, prediction frameworks have been continuously refined, employing
data-driven statistical methods, mathematical models, and machine learning.
Recently, as one of the challenging systems, shared transport systems such as
public bicycles have gained prominence due to urban constraints and
environmental concerns. Predicting rental and return patterns at bicycle
stations remains a formidable task due to the system's openness and imbalanced
usage patterns across stations. In this study, we propose a deep learning
framework to predict rental and return patterns by leveraging cartogram
approaches. The cartogram approach facilitates the prediction of demand for
newly installed stations with no training data as well as long-period
prediction, which has not been achieved before. We apply this method to public
bicycle rental-and-return data in Seoul, South Korea, employing a
spatial-temporal convolutional graph attention network. Our improved
architecture incorporates batch attention and modified node feature updates for
better prediction accuracy across different time scales. We demonstrate the
effectiveness of our framework in predicting temporal patterns and its
potential applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Node Classification via Semantic-Structural Attention-Enhanced Graph
  Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph data, also known as complex network data, is omnipresent across various
domains and applications. Prior graph neural network models primarily focused
on extracting task-specific structural features through supervised learning
objectives, but they fell short in capturing the inherent semantic and
structural features of the entire graph. In this paper, we introduce the
semantic-structural attention-enhanced graph convolutional network (SSA-GCN),
which not only models the graph structure but also extracts generalized
unsupervised features to enhance vertex classification performance. The
SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic
information through unsupervised feature extraction from a knowledge graph
perspective; secondly, it obtains structural information through unsupervised
feature extraction from a complex network perspective; and finally, it
integrates these features through a cross-attention mechanism. By leveraging
these features, we augment the graph convolutional network, thereby enhancing
the model's generalization capabilities. Our experiments on the Cora and
CiteSeer datasets demonstrate the performance improvements achieved by our
proposed method. Furthermore, our approach also exhibits excellent accuracy
under privacy settings, making it a robust and effective solution for graph
data analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ghost on the Shell: An Expressive Representation of General 3D Shapes <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15168v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15168v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of photorealistic virtual worlds requires the accurate modeling
of 3D surface geometry for a wide range of objects. For this, meshes are
appealing since they 1) enable fast physics-based rendering with realistic
material and lighting, 2) support physical simulation, and 3) are
memory-efficient for modern graphics pipelines. Recent work on reconstructing
and statistically modeling 3D shape, however, has critiqued meshes as being
topologically inflexible. To capture a wide range of object shapes, any 3D
representation must be able to model solid, watertight, shapes as well as thin,
open, surfaces. Recent work has focused on the former, and methods for
reconstructing open surfaces do not support fast reconstruction with material
and lighting or unconditional generative modelling. Inspired by the observation
that open surfaces can be seen as islands floating on watertight surfaces, we
parameterize open surfaces by defining a manifold signed distance field on
watertight templates. With this parameterization, we further develop a
grid-based and differentiable representation that parameterizes both watertight
and non-watertight meshes of arbitrary topology. Our new representation, called
Ghost-on-the-Shell (G-Shell), enables two important applications:
differentiable rasterization-based reconstruction from multiview images and
generative modelling of non-watertight meshes. We empirically demonstrate that
G-Shell achieves state-of-the-art performance on non-watertight mesh
reconstruction and generation tasks, while also performing effectively for
watertight meshes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Oral (v3: 30 pages, 19 figures, Project Page:
  https://gshell3d.github.io/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Development and Evaluation of a Learning-based Model for Real-time
  Haptic Texture Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.13332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.13332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negin Heravi, Heather Culbertson, Allison M. Okamura, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Virtual Reality (VR) environments lack the rich haptic signals that
humans experience during real-life interactions, such as the sensation of
texture during lateral movement on a surface. Adding realistic haptic textures
to VR environments requires a model that generalizes to variations of a user's
interaction and to the wide variety of existing textures in the world. Current
methodologies for haptic texture rendering exist, but they usually develop one
model per texture, resulting in low scalability. We present a deep
learning-based action-conditional model for haptic texture rendering and
evaluate its perceptual performance in rendering realistic texture vibrations
through a multi part human user study. This model is unified over all materials
and uses data from a vision-based tactile sensor (GelSight) to render the
appropriate surface conditioned on the user's action in real time. For
rendering texture, we use a high-bandwidth vibrotactile transducer attached to
a 3D Systems Touch device. The result of our user study shows that our
learning-based method creates high-frequency texture renderings with comparable
or better quality than state-of-the-art methods without the need for learning a
separate model per texture. Furthermore, we show that the method is capable of
rendering previously unseen textures using a single GelSight image of their
surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Haptics 2024. 12
  pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VQPy: An Object-Oriented Approach to Modern Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Yu, Zhenting Zhu, Yu Chen, Hanchen Xu, Pengzhan Zhao, Yang Wang, Arthi Padmanabhan, Hugo Latapie, Harry Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video analytics is widely used in contemporary systems and services. At the
forefront of video analytics are video queries that users develop to find
objects of particular interest. Building upon the insight that video objects
(e.g., human, animals, cars, etc.), the center of video analytics, are similar
in spirit to objects modeled by traditional object-oriented languages, we
propose to develop an object-oriented approach to video analytics. This
approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant
with constructs that make it easy for users to express video objects and their
interactions$\unicode{x2015}$as well as an extensible backend that can
automatically construct and optimize pipelines based on video objects. We have
implemented and open-sourced VQPy, which has been productized in Cisco as part
of its DeepVision framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MLSys'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing cognitive function among older adults using machine learning
  and wearable device data: a feasibility study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Collin Sakal, Tingyou Li, Juan Li, Xinyue Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timely implementation of interventions to slow cognitive decline among older
adults requires accurate monitoring to detect changes in cognitive function.
Data gathered using wearable devices that can continuously monitor factors
known to be associated with cognition could be used to train machine learning
models and develop wearable-based cognitive monitoring systems. Using data from
over 2,400 older adults in the National Health and Nutrition Examination Survey
(NHANES) we developed prediction models to differentiate older adults with
normal cognition from those with poor cognition based on outcomes from three
cognitive tests measuring different domains of cognitive function. During
repeated cross-validation, CatBoost, XGBoost, and Random Forest models
performed best when predicting cognition based on processing speed, working
memory, and attention (median AUCs >0.82) compared to immediate and delayed
recall (median AUCs >0.72) and categorical verbal fluency (median AUC >0.68).
Activity and sleep parameters were also more strongly associated with
processing speed, working memory, and attention compared to other cognitive
subdomains. Our work provides proof of concept that wearable-based cognitive
monitoring systems may be a viable alternative to traditional methods for
monitoring processing speeds, working memory, and attention. We further
identified novel metrics that could be targets in future causal studies seeking
to better understand how sleep and activity parameters influence cognitive
function among older adults.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking and Improving Data Credibility: A Study with <span class="highlight-title">Dataset</span>s for
  Training Harmless Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowei Zhu, Jialu Wang, Hao Cheng, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have shown promise in various tasks but can be affected by
undesired data during training, fine-tuning, or alignment. For example, if some
unsafe conversations are wrongly annotated as safe ones, the model fine-tuned
on these samples may be harmful. Therefore, the correctness of annotations,
i.e., the credibility of the dataset, is important. This study focuses on the
credibility of real-world datasets, including the popular benchmarks Jigsaw
Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that
can be used for training a harmless language model. Given the cost and
difficulty of cleaning these datasets by humans, we introduce a systematic
framework for evaluating the credibility of datasets, identifying label errors,
and evaluating the influence of noisy labels in the curated language data,
specifically focusing on unsafe comments and conversation classification. With
the framework, we find and fix an average of 6.16% label errors in 11 datasets
constructed from the above benchmarks. The data credibility and downstream
learning performance can be remarkably improved by directly fixing label
errors, indicating the significance of cleaning existing real-world datasets.
We provide an open-source tool, Docta, for data cleaning at
https://github.com/Docta-ai/docta.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Principled Federated Domain Adaptation: Gradient Projection and
  Auto-Weighting <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05049v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05049v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enyi Jiang, Yibo Jacky Zhang, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Domain Adaptation (FDA) describes the federated learning (FL)
setting where source clients and a server work collaboratively to improve the
performance of a target client where limited data is available. The domain
shift between the source and target domains, coupled with limited data of the
target client, makes FDA a challenging problem, e.g., common techniques such as
federated averaging and fine-tuning fail due to domain shift and data scarcity.
To theoretically understand the problem, we introduce new metrics that
characterize the FDA setting and a theoretical framework with novel theorems
for analyzing the performance of server aggregation rules. Further, we propose
a novel lightweight aggregation rule, Federated Gradient Projection
($\texttt{FedGP}$), which significantly improves the target performance with
domain shift and data scarcity. Moreover, our theory suggests an
$\textit{auto-weighting scheme}$ that finds the optimal combinations of the
source and target gradients. This scheme improves both $\texttt{FedGP}$ and a
simpler heuristic aggregation rule. Extensive experiments verify the
theoretical insights and illustrate the effectiveness of the proposed methods
in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusionNAG: Predictor-guided Neural Architecture Generation with
  Diffusion Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16943v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16943v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohyun An, Hayeon Lee, Jaehyeong Jo, Seanie Lee, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing NAS methods suffer from either an excessive amount of time for
repetitive sampling and training of many task-irrelevant architectures. To
tackle such limitations of existing NAS methods, we propose a paradigm shift
from NAS to a novel conditional Neural Architecture Generation (NAG) framework
based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the
neural architectures as directed graphs and propose a graph diffusion model for
generating them. Moreover, with the guidance of parameterized predictors,
DiffusionNAG can flexibly generate task-optimal architectures with the desired
properties for diverse tasks, by sampling from a region that is more likely to
satisfy the properties. This conditional NAG scheme is significantly more
efficient than previous NAS schemes which sample the architectures and filter
them using the property predictors. We validate the effectiveness of
DiffusionNAG through extensive experiments in two predictor-based NAS
scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS.
DiffusionNAG achieves superior performance with speedups of up to 35 times when
compared to the baselines on Transferable NAS benchmarks. Furthermore, when
integrated into a BO-based algorithm, DiffusionNAG outperforms existing
BO-based NAS approaches, particularly in the large MobileNetV3 search space on
the ImageNet 1K dataset. Code is available at
https://github.com/CownowAn/DiffusionNAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent <span class="highlight-title">Dataset</span> Distillation with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficacy of machine learning has traditionally relied on the availability
of increasingly larger datasets. However, large datasets pose storage
challenges and contain non-influential samples, which could be ignored during
training without impacting the final accuracy of the model. In response to
these limitations, the concept of distilling the information on a dataset into
a condensed set of (synthetic) samples, namely a distilled dataset, emerged.
One crucial aspect is the selected architecture (usually ConvNet) for linking
the original and synthetic datasets. However, the final accuracy is lower if
the employed model architecture differs from the model used during
distillation. Another challenge is the generation of high-resolution images,
e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation
with Diffusion Models (LD3M) that combine diffusion in latent space with
dataset distillation to tackle both challenges. LD3M incorporates a novel
diffusion process tailored for dataset distillation, which improves the
gradient norms for learning synthetic images. By adjusting the number of
diffusion steps, LD3M also offers a straightforward way of controlling the
trade-off between speed and accuracy. We evaluate our approach in several
ImageNet subsets and for high-resolution images (128x128 and 256x256). As a
result, LD3M consistently outperforms state-of-the-art distillation techniques
by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Hessian Fittings on Lie Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi-Lin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the fitting of Hessian or its inverse for stochastic
optimizations using a Hessian fitting criterion from the preconditioned
stochastic gradient descent (PSGD) method, which is intimately related to many
commonly used second order and adaptive gradient optimizers, e.g., BFGS,
Gaussian-Newton and natural gradient descent, AdaGrad, etc. Our analyses reveal
the efficiency and reliability differences among a wide range of preconditioner
fitting methods, from closed-form to iterative solutions, using Hessian-vector
products or stochastic gradients only, with Hessian fittings in the Euclidean
space, the manifold of symmetric positive definite (SPL) matrices, or a variety
of Lie groups. The most intriguing discovery is that the Hessian fitting itself
as an optimization problem is strongly convex under mild conditions on a
specific yet general enough Lie group. This discovery turns Hessian fitting
into a well behaved optimization problem, and facilitates the designs of highly
efficient and elegant Lie group sparse preconditioner fitting methods for large
scale stochastic optimizations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedNMUT -- Federated Noisy Model Update Tracking Convergence Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, Stanislaw H. Żak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel Decentralized Noisy Model Update Tracking Federated Learning
algorithm (FedNMUT) is proposed that is tailored to function efficiently in the
presence of noisy communication channels that reflect imperfect information
exchange. This algorithm uses gradient tracking to minimize the impact of data
heterogeneity while minimizing communication overhead. The proposed algorithm
incorporates noise into its parameters to mimic the conditions of noisy
communication channels, thereby enabling consensus among clients through a
communication graph topology in such challenging environments. FedNMUT
prioritizes parameter sharing and noise incorporation to increase the
resilience of decentralized learning systems against noisy communications.
Theoretical results for the smooth non-convex objective function are provided
by us, and it is shown that the $\epsilon-$stationary solution is achieved by
our algorithm at the rate of $\mathcal{O}\left(\frac{1}{\sqrt{T}}\right)$,
where $T$ is the total number of communication rounds. Additionally, via
empirical validation, we demonstrated that the performance of FedNMUT is
superior to the existing state-of-the-art methods and conventional
parameter-mixing approaches in dealing with imperfect information sharing. This
proves the capability of the proposed algorithm to counteract the negative
effects of communication noise in a decentralized learning framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2303.10695</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight
  Quantization of Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently demonstrated remarkable success
across various tasks. However, efficiently serving LLMs has been a challenge
due to the large memory bottleneck, specifically in small batch inference
settings (e.g. mobile devices). Weight-only quantization can be a promising
approach, but sub-4 bit quantization remains a challenge due to large-magnitude
activation outliers. To mitigate the undesirable outlier effect, we first
propose per-IC quantization, a simple yet effective method that creates
quantization groups within each input channel (IC) rather than the conventional
per-output-channel (per-OC). Our method is motivated by the observation that
activation outliers affect the input dimension of the weight matrix, so
similarly grouping the weights in the IC direction can isolate outliers within
a group. We also find that activation outliers do not dictate quantization
difficulty, and inherent weight sensitivities also exist. With per-IC
quantization as a new outlier-friendly scheme, we propose Adaptive Dimensions
(AdaDim), a versatile quantization framework that can adapt to various weight
sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting
prior methods such as Round-To-Nearest and GPTQ, showing significant
improvements across various language modeling benchmarks for both base (up to
+4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is
available at https://github.com/johnheo/adadim-llm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. 19 pages, 11 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection of diabetic retinopathy using longitudinal <span class="highlight-title">self-supervised</span>
  learning <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Gwenolé Quellec, Mathieu Lamard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Longitudinal imaging is able to capture both static anatomical structures and
dynamic changes in disease progression towards earlier and better
patient-specific pathology management. However, conventional approaches for
detecting diabetic retinopathy (DR) rarely take advantage of longitudinal
information to improve DR analysis. In this work, we investigate the benefit of
exploiting self-supervised learning with a longitudinal nature for DR diagnosis
purposes. We compare different longitudinal self-supervised learning (LSSL)
methods to model the disease progression from longitudinal retinal color fundus
photographs (CFP) to detect early DR severity changes using a pair of
consecutive exams. The experiments were conducted on a longitudinal DR
screening dataset with or without those trained encoders (LSSL) acting as a
longitudinal pretext task. Results achieve an AUC of 0.875 for the baseline
(model trained from scratch) and an AUC of 0.96 (95% CI: 0.9593-0.9655 DeLong
test) with a p-value < 2.2e-16 on early fusion using a simple ResNet alike
architecture with frozen LSSL weights, suggesting that the LSSL latent space
enables to encode the dynamic of DR progression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted preprint for presentation at MICCAI-OMIA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concurrent Learning of Policy and Unknown Safety Constraints in
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15893v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15893v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lunet Yifru, Ali Baheri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has revolutionized decision-making across a wide
range of domains over the past few decades. Yet, deploying RL policies in
real-world scenarios presents the crucial challenge of ensuring safety.
Traditional safe RL approaches have predominantly focused on incorporating
predefined safety constraints into the policy learning process. However, this
reliance on predefined safety constraints poses limitations in dynamic and
unpredictable real-world settings where such constraints may not be available
or sufficiently adaptable. Bridging this gap, we propose a novel approach that
concurrently learns a safe RL control policy and identifies the unknown safety
constraint parameters of a given environment. Initializing with a parametric
signal temporal logic (pSTL) safety specification and a small initial labeled
dataset, we frame the problem as a bilevel optimization task, intricately
integrating constrained policy optimization, using a Lagrangian-variant of the
twin delayed deep deterministic policy gradient (TD3) algorithm, with Bayesian
optimization for optimizing parameters for the given pSTL safety specification.
Through experimentation in comprehensive case studies, we validate the efficacy
of this approach across varying forms of environmental constraints,
consistently yielding safe RL policies with high returns. Furthermore, our
findings indicate successful learning of STL safety constraint parameters,
exhibiting a high degree of conformity with true environmental safety
constraints. The performance of our model closely mirrors that of an ideal
scenario that possesses complete prior knowledge of safety constraints,
demonstrating its proficiency in accurately identifying environmental safety
constraints and learning safe policies that adhere to those constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Quantification of MLE for Entity Ranking with Covariates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqing Fan, Jikai Hou, Mengxin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper concerns with statistical estimation and inference for the ranking
problems based on pairwise comparisons with additional covariate information
such as the attributes of the compared items. Despite extensive studies, few
prior literatures investigate this problem under the more realistic setting
where covariate information exists. To tackle this issue, we propose a novel
model, Covariate-Assisted Ranking Estimation (CARE) model, that extends the
well-known Bradley-Terry-Luce (BTL) model, by incorporating the covariate
information. Specifically, instead of assuming every compared item has a fixed
latent score $\{\theta_i^*\}_{i=1}^n$, we assume the underlying scores are
given by $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$, where $\alpha_i^*$ and
${x}_i^\top\beta^*$ represent latent baseline and covariate score of the $i$-th
item, respectively. We impose natural identifiability conditions and derive the
$\ell_{\infty}$- and $\ell_2$-optimal rates for the maximum likelihood
estimator of $\{\alpha_i^*\}_{i=1}^{n}$ and $\beta^*$ under a sparse comparison
graph, using a novel `leave-one-out' technique (Chen et al., 2019) . To conduct
statistical inferences, we further derive asymptotic distributions for the MLE
of $\{\alpha_i^*\}_{i=1}^n$ and $\beta^*$ with minimal sample complexity. This
allows us to answer the question whether some covariates have any explanation
power for latent scores and to threshold some sparse parameters to improve the
ranking performance. We improve the approximation method used in (Gao et al.,
2021) for the BLT model and generalize it to the CARE model. Moreover, we
validate our theoretical results through large-scale numerical studies and an
application to the mutual fund stock holding dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>81 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lemur: Integrating Large Language Models in Automated Program
  Verification <span class="chip">ICLR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04870v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04870v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoze Wu, Clark Barrett, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The demonstrated code-understanding capability of LLMs raises the question of
whether they can be used for automated program verification, a task that
demands high-level abstract reasoning about program properties that is
challenging for verification tools. We propose a general methodology to combine
the power of LLMs and automated reasoners for automated program verification.
We formally describe this methodology as a set of derivation rules and prove
its soundness. We instantiate the calculus as a sound automated verification
procedure, which led to practical improvements on a set of synthetic and
competition benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Limit Order Book Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Briola, Silvia Bartolucci, Tomaso Aste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We exploit cutting-edge deep learning methodologies to explore the
predictability of high-frequency Limit Order Book mid-price changes for a
heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we
release `LOBFrame', an open-source code base to efficiently process large-scale
Limit Order Book data and quantitatively assess state-of-the-art deep learning
models' forecasting capabilities. Our results are twofold. We demonstrate that
the stocks' microstructural characteristics influence the efficacy of deep
learning methods and that their high forecasting power does not necessarily
correspond to actionable trading signals. We argue that traditional machine
learning metrics fail to adequately assess the quality of forecasts in the
Limit Order Book context. As an alternative, we propose an innovative
operational framework that evaluates predictions' practicality by focusing on
the probability of accurately forecasting complete transactions. This work
offers academics and practitioners an avenue to make informed and robust
decisions on the application of deep learning techniques, their scope and
limitations, effectively exploiting emergent statistical properties of the
Limit Order Book.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 14 figures, 12 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-TPT: Calibrated Test-Time <span class="highlight-title">Prompt</span> Tuning for Vision-Language Models via
  Text Feature Dispersion <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep learning, test-time adaptation has gained attention as a method for
model fine-tuning without the need for labeled data. A prime exemplification is
the recently proposed test-time prompt tuning for large-scale vision-language
models such as CLIP. Unfortunately, these prompts have been mainly developed to
improve accuracy, overlooking the importance of calibration, which is a crucial
aspect for quantifying prediction uncertainty. However, traditional calibration
methods rely on substantial amounts of labeled data, making them impractical
for test-time scenarios. To this end, this paper explores calibration during
test-time prompt tuning by leveraging the inherent properties of CLIP. Through
a series of observations, we find that the prompt choice significantly affects
the calibration in CLIP, where the prompts leading to higher text feature
dispersion result in better-calibrated predictions. Introducing the Average
Text Feature Dispersion (ATFD), we establish its relationship with calibration
error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),
for optimizing prompts during test-time with enhanced calibration. Through
extensive experiments on different CLIP architectures and datasets, we show
that C-TPT can effectively improve the calibration of test-time prompt tuning
without needing labeled data. The code is publicly accessible at
https://github.com/hee-suk-yoon/C-TPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse joint shift in multinomial classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16971v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16971v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Tasche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse joint shift (SJS) was recently proposed as a tractable model for
general dataset shift which may cause changes to the marginal distributions of
features and labels as well as the posterior probabilities and the
class-conditional feature distributions. Fitting SJS for a target dataset
without label observations may produce valid predictions of labels and
estimates of class prior probabilities. We present new results on the
transmission of SJS from sets of features to larger sets of features, a
conditional correction formula for the class posterior probabilities under the
target distribution, identifiability of SJS, and the relationship between SJS
and covariate shift. In addition, we point out inconsistencies in the
algorithms which were proposed for estimating the characteristics of SJS, as
they could hamper the search for optimal solutions, and suggest potential
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FreshGNN: Reducing Memory Access via Stable Historical Embeddings for
  Graph Neural Network Training <span class="chip">VLDB 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07482v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07482v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kezhao Huang, Haitian Jiang, Minjie Wang, Guangxuan Xiao, David Wipf, Xiang Song, Quan Gan, Zengfeng Huang, Jidong Zhai, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key performance bottleneck when training graph neural network (GNN) models
on large, real-world graphs is loading node features onto a GPU. Due to limited
GPU memory, expensive data movement is necessary to facilitate the storage of
these features on alternative devices with slower access (e.g. CPU memory).
Moreover, the irregularity of graph structures contributes to poor data
locality which further exacerbates the problem. Consequently, existing
frameworks capable of efficiently training large GNN models usually incur a
significant accuracy degradation because of the currently-available shortcuts
involved. To address these limitations, we instead propose FreshGNN, a
general-purpose GNN mini-batch training framework that leverages a historical
cache for storing and reusing GNN node embeddings instead of re-computing them
through fetching raw features at every iteration. Critical to its success, the
corresponding cache policy is designed, using a combination of gradient-based
and staleness criteria, to selectively screen those embeddings which are
relatively stable and can be cached, from those that need to be re-computed to
reduce estimation errors and subsequent downstream accuracy loss. When paired
with complementary system enhancements to support this selective historical
cache, FreshGNN is able to accelerate the training speed on large graph
datasets such as ogbn-papers100M and MAG240M by 3.4x up to 20.5x and reduce the
memory access by 59%, with less than 1% influence on test accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by VLDB 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic
  Under Real-World Perturbations Via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bibek Poudel, Weizi Li, Kevin Heaslip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-driven vehicles (HVs) amplify naturally occurring perturbations in
traffic, leading to congestion--a major contributor to increased fuel
consumption, higher collision risks, and reduced road capacity utilization.
While previous research demonstrates that Robot Vehicles (RVs) can be leveraged
to mitigate these issues, most such studies rely on simulations with simplistic
models of human car-following behaviors. In this work, we analyze real-world
driving trajectories and extract a wide range of acceleration profiles. We then
incorporates these profiles into simulations for training RVs to mitigate
congestion. We evaluate the safety, efficiency, and stability of mixed traffic
via comprehensive experiments conducted in two mixed traffic environments (Ring
and Bottleneck) at various traffic densities, configurations, and RV
penetration rates. The results show that under real-world perturbations, prior
RV controllers experience performance degradation on all three objectives
(sometimes even lower than 100% HVs). To address this, we introduce a
reinforcement learning based RV that employs a congestion stage classifier to
optimize the safety, efficiency, and stability of mixed traffic. Our RVs
demonstrate significant improvements: safety by up to 66%, efficiency by up to
54%, and stability by up to 97%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Channel Dependence for Multivariate Time Series Forecasting:
  Learning from Leading Indicators <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifan Zhao, Yanyan Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, channel-independent methods have achieved state-of-the-art
performance in multivariate time series (MTS) forecasting. Despite reducing
overfitting risks, these methods miss potential opportunities in utilizing
channel dependence for accurate predictions. We argue that there exist locally
stationary lead-lag relationships between variates, i.e., some lagged variates
may follow the leading indicators within a short time period. Exploiting such
channel dependence is beneficial since leading indicators offer advance
information that can be used to reduce the forecasting difficulty of the lagged
variates. In this paper, we propose a new method named LIFT that first
efficiently estimates leading indicators and their leading steps at each time
step and then judiciously allows the lagged variates to utilize the advance
information from leading indicators. LIFT plays as a plugin that can be
seamlessly collaborated with arbitrary time series forecasting methods.
Extensive experiments on six real-world datasets demonstrate that LIFT improves
the state-of-the-art methods by 5.5% in average forecasting performance. Our
code is available at https://github.com/SJTU-Quant/LIFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Copyright be Reduced to Privacy? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niva Elkin-Koren, Uri Hacohen, Roi Livni, Shay Moran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing concern that generative AI models will generate outputs
closely resembling the copyrighted materials for which they are trained. This
worry has intensified as the quality and complexity of generative models have
immensely improved, and the availability of extensive datasets containing
copyrighted material has expanded. Researchers are actively exploring
strategies to mitigate the risk of generating infringing samples, with a recent
line of work suggesting to employ techniques such as differential privacy and
other forms of algorithmic stability to provide guarantees on the lack of
infringing copying. In this work, we examine whether such algorithmic stability
techniques are suitable to ensure the responsible use of generative models
without inadvertently violating copyright laws. We argue that while these
techniques aim to verify the presence of identifiable information in datasets,
thus being privacy-oriented, copyright law aims to promote the use of original
works for the benefit of society as a whole, provided that no unlicensed use of
protected expression occurred. These fundamental differences between privacy
and copyright must not be overlooked. In particular, we demonstrate that while
algorithmic stability may be perceived as a practical tool to detect copying,
such copying does not necessarily constitute copyright infringement. Therefore,
if adopted as a standard for detecting an establishing copyright infringement,
algorithmic stability may undermine the intended objectives of copyright law.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyMNet: a Multimodal Deep Learning System for Hypertension
  Classification using Fundus Photographs and Cardiometabolic Risk Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Baharoon, Hessa Almatar, Reema Alduhayan, Tariq Aldebasi, Badr Alahmadi, Yahya Bokhari, Mohammed Alawad, Ahmed Almazroa, Abdulrhman Aljouie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep learning has shown promise in predicting hypertension
(HTN) from fundus images. However, most prior research has primarily focused on
analyzing a single type of data, which may not capture the full complexity of
HTN risk. To address this limitation, this study introduces a multimodal deep
learning (MMDL) system, dubbed HyMNet, which combines fundus images and
cardiometabolic risk factors, specifically age and gender, to improve
hypertension detection capabilities. Our MMDL system uses RETFound, a
foundation model pre-trained on 1.6 million retinal images, for the fundus path
and a fully connected neural network for the age and gender path. The two paths
are jointly trained by concatenating the feature vectors from each path that
are then fed into a fusion network. The system was trained on 5,016 retinal
images from 1,243 individuals collected from the Saudi Ministry of National
Guard Health Affairs. The results show that the multimodal model that
integrates fundus images along with age and gender outperforms the unimodal
system trained solely on fundus photographs, with an F1 score of 0.771 [0.747,
0.796], and 0.745 [0.719, 0.772] for hypertension detection, respectively.
Additionally, we studied the effect underlying diabetes mellitus has on the
model's predictive ability, concluding that diabetes is used as a confounding
variable for distinguishing hypertensive cases. Our code and model weights are
publicly available at https://github.com/MohammedSB/HyMNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Neighbor Explainability for Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Llorente Gonzalez, Rana Fawzy, Jared Keown, Michal Horemuz, Péter Vaderna, Sándor Laki, Roland Kotroczó, Rita Csoma, János Márk Szalai-Gindl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability in Graph Neural Networks (GNNs) is a new field growing in the
last few years. In this publication we address the problem of determining how
important is each neighbor for the GNN when classifying a node and how to
measure the performance for this specific task. To do this, various known
explainability methods are reformulated to get the neighbor importance and four
new metrics are presented. Our results show that there is almost no difference
between the explanations provided by gradient-based techniques in the GNN
domain. In addition, many explainability techniques failed to identify
important neighbors when GNNs without self-loops are used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamical softassign and adaptive parameter tuning for graph matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08233v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08233v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binrui Shen, Qiang Niu, Shengxin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies a unified framework for graph matching problems called the
constrained gradient method. Popular algorithms within this framework include
graduated assignment (GA), integer projected fixed-point method (IPFP), and
doubly stochastic projected fixed-point method (DSPFP). These algorithms differ
from the step size parameter and constrained operator. Our contributed adaptive
step size parameter can guarantee the underlying algorithms' convergence and
enhance their efficiency and accuracy. A preliminary analysis suggests that the
optimal step size parameter has a high probability of being 1 in fully
connected graph matching. Secondly, we propose a dynamic strategy for
softassign, a popular constrained operator, to address its sensitivity
concerning nodes' cardinality and risk of overflow. Combining the adaptive step
size parameter and the dynamical softassign, we propose a novel graph matching
algorithm: the softassign constrained gradient method. Various experiments
demonstrate that it is significantly faster than other state-of-the-art
algorithms based on the constrained gradient method with improved accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Anil, Víctor Gutiérrez-Basulto, Yazmín Ibañéz-García, Steven Schockaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of inductive knowledge graph completion requires models to learn
inference patterns from a training graph, which can then be used to make
predictions on a disjoint test graph. Rule-based methods seem like a natural
fit for this task, but in practice they significantly underperform
state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet.
We hypothesise that the underperformance of rule-based methods is due to two
factors: (i) implausible entities are not ranked at all and (ii) only the most
informative path is taken into account when determining the confidence in a
given link prediction answer. To analyse the impact of these factors, we study
a number of variants of a rule-based approach, which are specifically aimed at
addressing the aforementioned issues. We find that the resulting models can
achieve a performance which is close to that of NBFNet. Crucially, the
considered variants only use a small fraction of the evidence that NBFNet
relies on, which means that they largely keep the interpretability advantage of
rule-based methods. Moreover, we show that a further variant, which does look
at the full KG, consistently outperforms NBFNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis <span class="chip">VLDB 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhan Zhong, Sizhe Song, Weipeng Zhuo, Guanyao Li, Yang Liu, S. -H. Gary Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series data, including univariate and multivariate ones, are
characterized by unique composition and complex multi-scale temporal
variations. They often require special consideration of decomposition and
multi-scale modeling to analyze. Existing deep learning methods on this best
fit to univariate time series only, and have not sufficiently considered
sub-series modeling and decomposition completeness. To address these
challenges, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer, which
learns to explicitly decompose and represent the input time series in its
different layers. To handle the multi-scale temporal patterns and multivariate
dependencies, we propose a novel temporal patching approach to model the time
series as multi-scale patches, and employ MLPs to capture intra- and
inter-patch variations and channel-wise correlations. In addition, we propose a
novel loss function to constrain both the mean and the autocorrelation of the
decomposition residual for better decomposition completeness. Through extensive
experiments on various real-world datasets for five common time series analysis
tasks, we demonstrate that MSD-Mixer consistently and significantly outperforms
other state-of-the-art algorithms with better efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for VLDB 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point-PEFT: Parameter-Efficient Fine-Tuning for 3D <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03059v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03059v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Tang, Ray Zhang, Zoey Guo, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code is released at
https://github.com/Ivan-Tang-3D/Point-PEFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The specialized PEFT framework for 3D pre-trained models, which
  achieves competitive performance to full fine-tuning, and significantly
  reduces the computational resources. Project page:
  https://github.com/Ivan-Tang-3D/Point-PEFT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A tutorial on learning from preferences and choices with Gaussian
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Benavoli, Dario Azzimonti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference modelling lies at the intersection of economics, decision theory,
machine learning and statistics. By understanding individuals' preferences and
how they make choices, we can build products that closely match their
expectations, paving the way for more efficient and personalised applications
across a wide range of domains. The objective of this tutorial is to present a
cohesive and comprehensive framework for preference learning with Gaussian
Processes (GPs), demonstrating how to seamlessly incorporate rationality
principles (from economics and decision theory) into the learning process. By
suitably tailoring the likelihood function, this framework enables the
construction of preference learning models that encompass random utility
models, limits of discernment, and scenarios with multiple conflicting
utilities for both object- and label-preference. This tutorial builds upon
established research while simultaneously introducing some novel GP-based
models to address specific gaps in the existing literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALI-DPFL: Differentially Private Federated Learning with Adaptive Local
  Iterations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10457v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10457v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpeng Ling, Jie Fu, Kuncan Wang, Haitao Liu, Zhili Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed machine learning technique that
allows model training among multiple devices or organizations by sharing
training parameters instead of raw data. However, adversaries can still infer
individual information through inference attacks (e.g. differential attacks) on
these training parameters. As a result, Differential Privacy (DP) has been
widely used in FL to prevent such attacks.
  We consider differentially private federated learning in a
resource-constrained scenario, where both privacy budget and communication
rounds are constrained. By theoretically analyzing the convergence, we can find
the optimal number of local DPSGD iterations for clients between any two
sequential global updates. Based on this, we design an algorithm of
Differentially Private Federated Learning with Adaptive Local Iterations
(ALI-DPFL). We experiment our algorithm on the MNIST, FashionMNIST and Cifar10
datasets, and demonstrate significantly better performances than previous work
in the resource-constraint scenario. Code is available at
https://github.com/KnightWan/ALI-DPFL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient-less Federated Gradient Boosting Trees with Learnable Learning
  Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Ma, Xinchi Qiu, Daniel J. Beutel, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The privacy-sensitive nature of decentralized datasets and the robustness of
eXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train
XGBoost in the context of federated learning (FL). Existing works on federated
XGBoost in the horizontal setting rely on the sharing of gradients, which
induce per-node level communication frequency and serious privacy concerns. To
alleviate these problems, we develop an innovative framework for horizontal
federated XGBoost which does not depend on the sharing of gradients and
simultaneously boosts privacy and communication efficiency by making the
learning rates of the aggregated tree ensembles learnable. We conduct extensive
evaluations on various classification and regression datasets, showing our
approach achieves performance comparable to the state-of-the-art method and
effectively improves communication efficiency by lowering both communication
rounds and communication overhead by factors ranging from 25x to 700x. Project
Page: https://flower.ai/blog/2023-04-19-xgboost-with-flower/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 3rd ACM Workshop on Machine Learning and Systems
  (EuroMLSys), May 8th 2023, Rome, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynerMix: Synergistic Mixup Solution for Enhanced Intra-Class Cohesion
  and Inter-Class Separability in Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Xu, Ya Gao, Xiaorong Qiu, Yang Chen, Ying Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the issues of MixUp and its variants (e.g., Manifold MixUp) in
image classification tasks-namely, their neglect of mixing within the same
class (intra-class mixup) and their inadequacy in enhancing intra-class
cohesion through their mixing operations-we propose a novel mixup method named
SynerMix-Intra and, building upon this, introduce a synergistic mixup solution
named SynerMix. SynerMix-Intra specifically targets intra-class mixup to
bolster intra-class cohesion, a feature not addressed by current mixup methods.
For each mini-batch, it leverages feature representations of unaugmented
original images from each class to generate a synthesized feature
representation through random linear interpolation. All synthesized
representations are then fed into the classification and loss layers to
calculate an average classification loss that significantly enhances
intra-class cohesion. Furthermore, SynerMix combines SynerMix-Intra with an
existing mixup approach (e.g., MixUp, Manifold MixUp), which primarily focuses
on inter-class mixup and has the benefit of enhancing inter-class separability.
In doing so, it integrates both inter- and intra-class mixup in a balanced way
while concurrently improving intra-class cohesion and inter-class separability.
Experimental results on six datasets show that SynerMix achieves a 0.1% to
3.43% higher accuracy than the best of either MixUp or SynerMix-Intra alone,
averaging a 1.16% gain. It also surpasses the top-performer of either Manifold
MixUp or SynerMix-Intra by 0.12% to 5.16%, with an average gain of 1.11%. Given
that SynerMix is model-agnostic, it holds significant potential for application
in other domains where mixup methods have shown promise, such as speech and
text classification. Our code is publicly available at:
https://github.com/wxitxy/synermix.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages,12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exponential Concentration in Stochastic Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kody Law, Neil Walton, Shangda Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the behavior of stochastic approximation algorithms where
iterates, in expectation, progress towards an objective at each step. When
progress is proportional to the step size of the algorithm, we prove
exponential concentration bounds. These tail-bounds contrast asymptotic
normality results, which are more frequently associated with stochastic
approximation. The methods that we develop rely on a geometric ergodicity
proof. This extends a result on Markov chains due to Hajek (1982) to the area
of stochastic approximation algorithms. We apply our results to several
different Stochastic Approximation algorithms, specifically Projected
Stochastic Gradient Descent, Kiefer-Wolfowitz and Stochastic Frank-Wolfe
algorithms. When applicable, our results prove faster $O(1/t)$ and linear
convergence rates for Projected Stochastic Gradient Descent with a
non-vanishing gradient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 11 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Energy-Based Models by Cooperative Diffusion Recovery
  Likelihood 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05153v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05153v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxuan Zhu, Jianwen Xie, Yingnian Wu, Ruiqi Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training energy-based models (EBMs) on high-dimensional data can be both
challenging and time-consuming, and there exists a noticeable gap in sample
quality between EBMs and other generative frameworks like GANs and diffusion
models. To close this gap, inspired by the recent efforts of learning EBMs by
maximizing diffusion recovery likelihood (DRL), we propose cooperative
diffusion recovery likelihood (CDRL), an effective approach to tractably learn
and sample from a series of EBMs defined on increasingly noisy versions of a
dataset, paired with an initializer model for each EBM. At each noise level,
the two models are jointly estimated within a cooperative training framework:
samples from the initializer serve as starting points that are refined by a few
MCMC sampling steps from the EBM. The EBM is then optimized by maximizing
recovery likelihood, while the initializer model is optimized by learning from
the difference between the refined samples and the initial samples. In
addition, we made several practical designs for EBM training to further improve
the sample quality. Combining these advances, our approach significantly boost
the generation performance compared to existing EBM methods on CIFAR-10 and
ImageNet datasets. We also demonstrate the effectiveness of our models for
several downstream tasks, including classifier-free guided generation,
compositional generation, image inpainting and out-of-distribution detection.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CFAT: Unleashing TriangularWindows for Image Super-resolution <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhisek Ray, Gaurav Kumar, Maheshkumar H. Kolekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have revolutionized the field of image
super-resolution (SR) by harnessing their inherent ability to capture complex
contextual features. The overlapping rectangular shifted window technique used
in transformer architecture nowadays is a common practice in super-resolution
models to improve the quality and robustness of image upscaling. However, it
suffers from distortion at the boundaries and has limited unique shifting
modes. To overcome these weaknesses, we propose a non-overlapping triangular
window technique that synchronously works with the rectangular one to mitigate
boundary-level distortion and allows the model to access more unique sifting
modes. In this paper, we propose a Composite Fusion Attention Transformer
(CFAT) that incorporates triangular-rectangular window-based local attention
with a channel-based global attention technique in image super-resolution. As a
result, CFAT enables attention mechanisms to be activated on more image pixels
and captures long-range, multi-scale features to improve SR performance. The
extensive experimental results and ablation study demonstrate the effectiveness
of CFAT in the SR domain. Our proposed model shows a significant 0.7 dB
performance improvement over other state-of-the-art SR architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Landmark-Guided Cross-Speaker Lip Reading with Mutual Information
  Regularization <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linzhi Wu, Xingyu Zhang, Yakun Zhang, Changyan Zheng, Tiejun Liu, Liang Xie, Ye Yan, Erwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lip reading, the process of interpreting silent speech from visual lip
movements, has gained rising attention for its wide range of realistic
applications. Deep learning approaches greatly improve current lip reading
systems. However, lip reading in cross-speaker scenarios where the speaker
identity changes, poses a challenging problem due to inter-speaker variability.
A well-trained lip reading system may perform poorly when handling a brand new
speaker. To learn a speaker-robust lip reading model, a key insight is to
reduce visual variations across speakers, avoiding the model overfitting to
specific speakers. In this work, in view of both input visual clues and latent
representations based on a hybrid CTC/attention architecture, we propose to
exploit the lip landmark-guided fine-grained visual clues instead of
frequently-used mouth-cropped images as input features, diminishing
speaker-specific appearance characteristics. Furthermore, a max-min mutual
information regularization approach is proposed to capture speaker-insensitive
latent representations. Experimental evaluations on public lip reading datasets
demonstrate the effectiveness of the proposed approach under the intra-speaker
and inter-speaker conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture-of-<span class="highlight-title">Prompt</span>-Experts for Multi-modal Semantic Understanding <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu, Yunfang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep multimodal semantic understanding that goes beyond the mere superficial
content relation mining has received increasing attention in the realm of
artificial intelligence. The challenges of collecting and annotating
high-quality multi-modal data have underscored the significance of few-shot
learning. In this paper, we focus on two critical tasks under this context:
few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis
(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware
Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on
the unified vision-language model (VLM). Specifically, we design three experts
of soft prompts: a text prompt and an image prompt that extract
modality-specific features to enrich the single-modal representation, and a
unified prompt to assist multi-modal interaction. Additionally, we reorganize
Transformer layers into several blocks and introduce cross-modal prompt
attention between adjacent blocks, which smoothens the transition from
single-modal representation to multi-modal fusion. On both MSD and MSA datasets
in few-shot setting, our proposed model not only surpasses the 8.2B model
InstructBLIP with merely 2% parameters (150M), but also significantly
outperforms other widely-used prompt methods on VLMs or task-specific methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024, Long Paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T05:23:56.907179451Z">
            2024-03-28 05:23:56 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
